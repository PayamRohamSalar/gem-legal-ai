# src/retrieval/embedding_manager.py

import os
import json
import time
import numpy as np
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
import torch
from datetime import datetime
from tqdm import tqdm

class EmbeddingModelManager:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ embedding Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆÙ† Ø­Ù‚ÙˆÙ‚ÛŒ ÙØ§Ø±Ø³ÛŒ
    
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦ÙˆÙ„ Ø§Ù†ØªØ®Ø§Ø¨ØŒ ØªØ³Øª Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù embedding Ø§Ø³Øª
    """
    
    def __init__(self, cache_dir: str = "models/embeddings"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        self.supported_models = {
            # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø§ÙˆÙ„)
            'multilingual-mpnet': {
                'model_name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
                'description': 'Ù…Ø¯Ù„ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ - Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ',
                'dimension': 768,
                'max_length': 512,
                'languages': ['fa', 'en', 'ar', 'many others'],
                'recommended': True
            },
            'multilingual-minilm': {
                'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                'description': 'Ù…Ø¯Ù„ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø³Ø±ÛŒØ¹ - Ù…ØªØ¹Ø§Ø¯Ù„ Ø³Ø±Ø¹Øª Ùˆ Ú©ÛŒÙÛŒØª',
                'dimension': 384,
                'max_length': 512,
                'languages': ['fa', 'en', 'ar', 'many others'],
                'recommended': True
            },
            
            # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ ØªØ®ØµØµÛŒ
            'bert-fa-base': {
                'model_name': 'HooshvareLab/bert-fa-base-uncased',
                'description': 'BERT ÙØ§Ø±Ø³ÛŒ - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆÙ† ØªØ®ØµØµÛŒ',
                'dimension': 768,
                'max_length': 512,
                'languages': ['fa'],
                'recommended': False  # Ù†ÛŒØ§Ø² Ø¨Ù‡ fine-tuning
            },
            
            # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø¨Ú©
            'all-minilm': {
                'model_name': 'sentence-transformers/all-MiniLM-L6-v2',
                'description': 'Ù…Ø¯Ù„ Ø³Ø¨Ú© Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ - Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±Ø¹Øª',
                'dimension': 384,
                'max_length': 512,
                'languages': ['en'],
                'recommended': False  # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
            }
        }
        
        self.current_model = None
        self.current_model_name = None
        
    def list_available_models(self) -> Dict:
        """Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡"""
        return self.supported_models
    
    def load_model(self, model_key: str, force_reload: bool = False) -> bool:
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ embedding
        
        Args:
            model_key: Ú©Ù„ÛŒØ¯ Ù…Ø¯Ù„ Ø§Ø² Ù„ÛŒØ³Øª supported_models
            force_reload: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¬Ø¯Ø¯ Ø­ØªÛŒ Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡
            
        Returns:
            bool: Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
        """
        if model_key not in self.supported_models:
            print(f"âŒ Ù…Ø¯Ù„ '{model_key}' Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯!")
            print(f"Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {list(self.supported_models.keys())}")
            return False
        
        # Ø§Ú¯Ø± Ù‡Ù…ÛŒÙ† Ù…Ø¯Ù„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡
        if (not force_reload and 
            self.current_model_name == model_key and 
            self.current_model is not None):
            print(f"âœ… Ù…Ø¯Ù„ '{model_key}' Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª")
            return True
        
        model_info = self.supported_models[model_key]
        model_name = model_info['model_name']
        
        print(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {model_key}")
        print(f"   ğŸ“ ØªÙˆØ¶ÛŒØ­Ø§Øª: {model_info['description']}")
        print(f"   ğŸ”¢ Ø¨Ø¹Ø¯ Ø¨Ø±Ø¯Ø§Ø±: {model_info['dimension']}")
        
        try:
            start_time = time.time()
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
            self.current_model = SentenceTransformer(
                model_name,
                cache_folder=self.cache_dir
            )
            
            # ØªÙ†Ø¸ÛŒÙ… Ø¯Ø³ØªÚ¯Ø§Ù‡ (GPU Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.current_model.to(device)
            
            loading_time = time.time() - start_time
            self.current_model_name = model_key
            
            print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ Ø¯Ø± {loading_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
            print(f"   ğŸ–¥ï¸ Ø¯Ø³ØªÚ¯Ø§Ù‡: {device}")
            print(f"   ğŸ’¾ Ø­Ø§ÙØ¸Ù‡ GPU: {self._get_gpu_memory()}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {str(e)}")
            self.current_model = None
            self.current_model_name = None
            return False
    
    def _get_gpu_memory(self) -> str:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø§ÙØ¸Ù‡ GPU"""
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_total = torch.cuda.memory_reserved() / 1024**3  # GB
            return f"{memory_used:.1f}/{memory_total:.1f} GB"
        return "CPU mode"
    
    def encode_text(self, text: str) -> Optional[np.ndarray]:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ embedding
        
        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            
        Returns:
            numpy array: Ø¨Ø±Ø¯Ø§Ø± embedding ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        """
        if self.current_model is None:
            print("âŒ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡! Ø§Ø¨ØªØ¯Ø§ load_model() Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
            return None
        
        try:
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
            text = text.strip()
            if not text:
                return np.zeros(self.get_embedding_dimension())
            
            # ØªÙˆÙ„ÛŒØ¯ embedding
            embedding = self.current_model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=True  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ cosine similarity
            )
            
            return embedding
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± encoding: {str(e)}")
            return None
    
    def encode_batch(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> List[np.ndarray]:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù…ØªÙˆÙ† Ø¨Ù‡ embedding
        
        Args:
            texts: Ù„ÛŒØ³Øª Ù…ØªÙˆÙ†
            batch_size: Ø§Ù†Ø¯Ø§Ø²Ù‡ batch Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
            show_progress: Ù†Ù…Ø§ÛŒØ´ progress bar
            
        Returns:
            list: Ù„ÛŒØ³Øª embeddings
        """
        if self.current_model is None:
            print("âŒ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡!")
            return []
        
        try:
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙˆÙ†
            clean_texts = [text.strip() if text else "" for text in texts]
            
            print(f"ğŸ”„ ØªÙˆÙ„ÛŒØ¯ embedding Ø¨Ø±Ø§ÛŒ {len(texts)} Ù…ØªÙ†...")
            start_time = time.time()
            
            # ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø§ batch processing
            embeddings = self.current_model.encode(
                clean_texts,
                batch_size=batch_size,
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=show_progress
            )
            
            encoding_time = time.time() - start_time
            speed = len(texts) / encoding_time
            
            print(f"âœ… {len(embeddings)} embedding ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ Ø¯Ø± {encoding_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
            print(f"   âš¡ Ø³Ø±Ø¹Øª: {speed:.1f} Ù…ØªÙ†/Ø«Ø§Ù†ÛŒÙ‡")
            
            return embeddings.tolist()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± batch encoding: {str(e)}")
            return []
    
    def get_embedding_dimension(self) -> int:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø¹Ø¯ embedding Ù…Ø¯Ù„ ÙØ¹Ù„ÛŒ"""
        if self.current_model_name:
            return self.supported_models[self.current_model_name]['dimension']
        return 0
    
    def test_model_quality(self, test_texts: List[str] = None) -> Dict:
        """
        ØªØ³Øª Ú©ÛŒÙÛŒØª Ù…Ø¯Ù„ Ø¨Ø§ Ù…ØªÙˆÙ† Ù†Ù…ÙˆÙ†Ù‡
        
        Args:
            test_texts: Ù…ØªÙˆÙ† ØªØ³Øª (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            
        Returns:
            dict: Ù†ØªØ§ÛŒØ¬ ØªØ³Øª
        """
        if self.current_model is None:
            return {"error": "Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡"}
        
        # Ù…ØªÙˆÙ† ØªØ³Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (Ø­Ù‚ÙˆÙ‚ÛŒ ÙØ§Ø±Ø³ÛŒ)
        if test_texts is None:
            test_texts = [
                "Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒÙ‡Ø§",
                "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ù…Ø§Øª Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ",
                "Ù…Ø¬Ø§Ø²Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ",
                "ØªØ®Ù„ÙØ§Øª Ø§Ù†Ø¶Ø¨Ø§Ø·ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡",
                "ÙˆØ²Ø§Ø±Øª Ø¹Ù„ÙˆÙ… ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ",
                "Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø¯Ø± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±",
                "Ù…Ø§Ø¯Ù‡ ÛŒÚ© Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Ø§Ø³ÛŒ Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ",
                "Ø­Ù‚ÙˆÙ‚ Ù…Ø§Ù„ÛŒ Ú©Ø§Ø±Ú©Ù†Ø§Ù† Ø¯ÙˆÙ„Øª"
            ]
        
        print(f"ğŸ§ª ØªØ³Øª Ú©ÛŒÙÛŒØª Ù…Ø¯Ù„ Ø¨Ø§ {len(test_texts)} Ù…ØªÙ†...")
        
        try:
            start_time = time.time()
            
            # ØªÙˆÙ„ÛŒØ¯ embeddings
            embeddings = self.encode_batch(test_texts, show_progress=False)
            
            if not embeddings:
                return {"error": "Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ embedding"}
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ similarity matrix
            embeddings_np = np.array(embeddings)
            similarity_matrix = np.dot(embeddings_np, embeddings_np.T)
            
            # Ø¢Ù†Ø§Ù„ÛŒØ² Ú©ÛŒÙÛŒØª
            avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
            max_similarity = np.max(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
            min_similarity = np.min(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
            
            # ØªØ³Øª Ø³Ø±Ø¹Øª
            test_time = time.time() - start_time
            speed = len(test_texts) / test_time
            
            # Ù†ØªØ§ÛŒØ¬
            results = {
                "model_name": self.current_model_name,
                "test_texts_count": len(test_texts),
                "embedding_dimension": len(embeddings[0]),
                "quality_metrics": {
                    "average_similarity": float(avg_similarity),
                    "max_similarity": float(max_similarity),
                    "min_similarity": float(min_similarity),
                    "similarity_range": float(max_similarity - min_similarity)
                },
                "performance_metrics": {
                    "encoding_time": test_time,
                    "speed_texts_per_second": speed,
                    "avg_time_per_text": test_time / len(test_texts)
                },
                "device_info": {
                    "device": 'cuda' if torch.cuda.is_available() else 'cpu',
                    "gpu_memory": self._get_gpu_memory()
                }
            }
            
            # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
            print(f"âœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯ Ø¯Ø± {test_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            print(f"ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ú©ÛŒÙÛŒØª:")
            print(f"   â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø´Ø¨Ø§Ù‡Øª: {avg_similarity:.3f}")
            print(f"   â€¢ Ø¯Ø§Ù…Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª: {min_similarity:.3f} - {max_similarity:.3f}")
            print(f"   â€¢ Ø³Ø±Ø¹Øª: {speed:.1f} Ù…ØªÙ†/Ø«Ø§Ù†ÛŒÙ‡")
            
            return results
            
        except Exception as e:
            return {"error": f"Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª: {str(e)}"}
    
    def compare_models(self, model_keys: List[str], test_texts: List[str] = None) -> Dict:
        """
        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ÛŒÙ† Ù…Ø¯Ù„ embedding
        
        Args:
            model_keys: Ù„ÛŒØ³Øª Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
            test_texts: Ù…ØªÙˆÙ† ØªØ³Øª
            
        Returns:
            dict: Ù†ØªØ§ÛŒØ¬ Ù…Ù‚Ø§ÛŒØ³Ù‡
        """
        print(f"ğŸ”¬ Ù…Ù‚Ø§ÛŒØ³Ù‡ {len(model_keys)} Ù…Ø¯Ù„ embedding...")
        
        comparison_results = {}
        
        for model_key in model_keys:
            print(f"\nğŸ”„ ØªØ³Øª Ù…Ø¯Ù„: {model_key}")
            
            if self.load_model(model_key):
                results = self.test_model_quality(test_texts)
                comparison_results[model_key] = results
            else:
                comparison_results[model_key] = {"error": "Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯"}
        
        # Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡
        print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù…Ù‚Ø§ÛŒØ³Ù‡:")
        print("-" * 50)
        
        for model_key, results in comparison_results.items():
            if "error" not in results:
                quality = results["quality_metrics"]["average_similarity"]
                speed = results["performance_metrics"]["speed_texts_per_second"]
                print(f"{model_key:20} | Ú©ÛŒÙÛŒØª: {quality:.3f} | Ø³Ø±Ø¹Øª: {speed:.1f}")
            else:
                print(f"{model_key:20} | âŒ {results['error']}")
        
        return comparison_results
    
    def save_model_benchmark(self, results: Dict, output_file: str = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ benchmark"""
        if output_file is None:
            output_file = f"data/metadata/embedding_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Ù†ØªØ§ÛŒØ¬ benchmark Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_file}")
    
    def get_recommended_model(self) -> str:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ"""
        for key, info in self.supported_models.items():
            if info.get('recommended', False):
                return key
        return list(self.supported_models.keys())[0]


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    print("ğŸ”§ Legal AI Assistant - Embedding Model Manager")
    print("=" * 50)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ± embedding
    embedding_manager = EmbeddingModelManager()
    
    # Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    print("ğŸ“‹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:")
    for key, info in embedding_manager.list_available_models().items():
        status = "â­ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ" if info.get('recommended') else ""
        print(f"  {key}: {info['description']} {status}")
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
    recommended_model = embedding_manager.get_recommended_model()
    print(f"\nğŸš€ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ: {recommended_model}")
    
    if embedding_manager.load_model(recommended_model):
        # ØªØ³Øª Ú©ÛŒÙÛŒØª
        test_results = embedding_manager.test_model_quality()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        embedding_manager.save_model_benchmark({
            "single_model_test": test_results,
            "timestamp": datetime.now().isoformat()
        })
        
        print(f"\nâœ… ØªØ³Øª Ù…Ø¯Ù„ {recommended_model} ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
    else:
        print(f"\nâŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {recommended_model}")


if __name__ == "__main__":
    main()