# src/retrieval/vector_database.py

import os
import json
import uuid
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import numpy as np
from tqdm import tqdm

import chromadb
from chromadb.config import Settings
import chromadb.utils.embedding_functions as embedding_functions

class LegalVectorDatabase:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Vector Database Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù†Ø§Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ
    
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦ÙˆÙ„ Ø°Ø®ÛŒØ±Ù‡ØŒ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª embeddings Ø¯Ø± ChromaDB Ø§Ø³Øª
    """
    
    def __init__(self, 
                 db_path: str = "data/vector_db",
                 collection_name: str = "legal_documents",
                 embedding_model_name: str = None):
        
        self.db_path = db_path
        self.collection_name = collection_name
        
        # ØªØ¹ÛŒÛŒÙ† Ù†Ø§Ù… ØµØ­ÛŒØ­ Ù…Ø¯Ù„ embedding
        if embedding_model_name is None:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ú©Ø§Ù…Ù„ ØµØ­ÛŒØ­ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            self.embedding_model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
        else:
            # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ú©Ù„ÛŒØ¯ Ø¨Ù‡ Ù†Ø§Ù… Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„
            model_mapping = {
                'multilingual-mpnet': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
                'multilingual-minilm': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                'bert-fa-base': 'HooshvareLab/bert-fa-base-uncased',
                'all-minilm': 'sentence-transformers/all-MiniLM-L6-v2'
            }
            
            # Ø§Ú¯Ø± Ù†Ø§Ù… Ú©Ù„ÛŒØ¯ Ø§Ø³ØªØŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ØŒ ÙˆÚ¯Ø±Ù†Ù‡ Ù‡Ù…Ø§Ù† Ù†Ø§Ù… Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            self.embedding_model_name = model_mapping.get(embedding_model_name, embedding_model_name)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        os.makedirs(db_path, exist_ok=True)
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ChromaDB
        self.client = None
        self.collection = None
        self.embedding_function = None
        
        self._initialize_database()
    
    def _initialize_database(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        try:
            print(f"ğŸ”„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ChromaDB Ø¯Ø±: {self.db_path}")
            print(f"ğŸ“‹ Ù…Ø¯Ù„ embedding: {self.embedding_model_name}")
            
            # Ø§ÛŒØ¬Ø§Ø¯ client
            self.client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # ØªÙ†Ø¸ÛŒÙ… embedding function
            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=self.embedding_model_name
            )
            
            # Ø§ÛŒØ¬Ø§Ø¯ ÛŒØ§ Ø¯Ø±ÛŒØ§ÙØª collection
            try:
                self.collection = self.client.get_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function
                )
                print(f"âœ… Collection Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {self.collection_name}")
                
            except Exception:
                # Ø§Ú¯Ø± collection ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"description": "Legal documents collection for RAG system"}
                )
                print(f"âœ… Collection Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {self.collection_name}")
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
            count = self.collection.count()
            print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ documents Ø¯Ø± collection: {count}")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}")
            raise
    
    def reset_database(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        try:
            print("âš ï¸ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
            
            if self.client and self.collection:
                self.client.delete_collection(self.collection_name)
                print(f"âœ… Collection {self.collection_name} Ù¾Ø§Ú© Ø´Ø¯")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¬Ø¯Ø¯
            self._initialize_database()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± reset Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}")
    
    def add_chunks_from_files(self, chunks_dir: str) -> bool:
        """
        Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† chunks Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON
        
        Args:
            chunks_dir: Ù…Ø³ÛŒØ± ÙÙˆÙ„Ø¯Ø± Ø­Ø§ÙˆÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ chunk
            
        Returns:
            bool: Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†
        """
        if not os.path.exists(chunks_dir):
            print(f"âŒ ÙÙˆÙ„Ø¯Ø± {chunks_dir} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
            return False
        
        print(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† chunks Ø§Ø²: {chunks_dir}")
        
        all_chunks = []
        processed_files = 0
        
        # Ù¾ÛŒÙ…Ø§ÛŒØ´ ØªÙ…Ø§Ù… ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ chunks
        for item in os.listdir(chunks_dir):
            item_path = os.path.join(chunks_dir, item)
            
            if os.path.isdir(item_path) and item.endswith('_chunks'):
                print(f"ğŸ“ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙÙˆÙ„Ø¯Ø±: {item}")
                
                # Ø®ÙˆØ§Ù†Ø¯Ù† ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON Ø¯Ø± ÙÙˆÙ„Ø¯Ø±
                chunk_files = [f for f in os.listdir(item_path) if f.endswith('.json')]
                
                for chunk_file in tqdm(chunk_files, desc=f"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {item}"):
                    chunk_path = os.path.join(item_path, chunk_file)
                    
                    try:
                        with open(chunk_path, 'r', encoding='utf-8') as f:
                            chunk_data = json.load(f)
                            all_chunks.append(chunk_data)
                    
                    except Exception as e:
                        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {chunk_file}: {str(e)}")
                        continue
                
                processed_files += 1
        
        if not all_chunks:
            print("âŒ Ù‡ÛŒÚ† chunk Ù…Ø¹ØªØ¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return False
        
        print(f"ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ chunks ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(all_chunks)} Ø§Ø² {processed_files} ÙØ§ÛŒÙ„")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        return self.add_chunks(all_chunks)
    
    def add_chunks(self, chunks: List[Dict]) -> bool:
        """
        Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒØ³ØªÛŒ Ø§Ø² chunks Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        
        Args:
            chunks: Ù„ÛŒØ³Øª chunks Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†
            
        Returns:
            bool: Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†
        """
        if not chunks:
            print("âŒ Ù„ÛŒØ³Øª chunks Ø®Ø§Ù„ÛŒ Ø§Ø³Øª!")
            return False
        
        try:
            print(f"ğŸ”„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† {len(chunks)} chunk Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ChromaDB
            documents = []
            metadatas = []
            ids = []
            
            for chunk in tqdm(chunks, desc="Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ chunks"):
                # ØªÙˆÙ„ÛŒØ¯ ID ÛŒÚ©ØªØ§
                chunk_id = chunk.get('chunk_id', str(uuid.uuid4()))
                
                # Ù…ØªÙ† chunk
                document_text = chunk.get('text', '')
                if not document_text.strip():
                    continue
                
                # metadata
                metadata = {
                    'chunk_index': chunk.get('chunk_index', 0),
                    'word_count': chunk.get('word_count', 0),
                    'char_count': chunk.get('char_count', 0),
                    'quality_score': chunk.get('quality_score', 0.0),
                    'document_title': chunk.get('document_title', ''),
                    'document_type': chunk.get('document_type', ''),
                    'authority': chunk.get('authority', ''),
                    'approval_date': chunk.get('approval_date', ''),
                    'structures': json.dumps(chunk.get('structures', []), ensure_ascii=False),
                    'keywords': json.dumps(chunk.get('keywords', []), ensure_ascii=False),
                    'legal_entities': json.dumps(chunk.get('legal_entities', []), ensure_ascii=False)
                }
                
                documents.append(document_text)
                metadatas.append(metadata)
                ids.append(chunk_id)
            
            if not documents:
                print("âŒ Ù‡ÛŒÚ† document Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒØ§ÙØª Ù†Ø´Ø¯!")
                return False
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ ChromaDB (Ø¯Ø± batches)
            batch_size = 100
            total_added = 0
            
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i+batch_size]
                batch_metas = metadatas[i:i+batch_size]
                batch_ids = ids[i:i+batch_size]
                
                try:
                    self.collection.add(
                        documents=batch_docs,
                        metadatas=batch_metas,
                        ids=batch_ids
                    )
                    total_added += len(batch_docs)
                    
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† batch {i//batch_size + 1}: {str(e)}")
                    continue
            
            print(f"âœ… {total_added} chunk Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
            print(f"ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ documents Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {self.collection.count()}")
            
            return total_added > 0
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† chunks: {str(e)}")
            return False
    
    def search_semantic(self, 
                       query: str, 
                       n_results: int = 10,
                       filters: Dict = None) -> List[Dict]:
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        
        Args:
            query: Ù…ØªÙ† Ø¬Ø³ØªØ¬Ùˆ
            n_results: ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬
            filters: ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ metadata
            
        Returns:
            list: Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ
        """
        if not query.strip():
            return []
        
        try:
            print(f"ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ: '{query[:50]}...'")
            
            # ØªØ¨Ø¯ÛŒÙ„ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª ChromaDB
            where_clause = None
            if filters:
                where_clause = self._build_where_clause(filters)
            
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ChromaDB
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_clause,
                include=['documents', 'metadatas', 'distances']
            )
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªØ§ÛŒØ¬
            processed_results = []
            
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    similarity = 1 - distance  # ØªØ¨Ø¯ÛŒÙ„ distance Ø¨Ù‡ similarity
                    
                    # Parse JSON fields
                    try:
                        metadata['structures'] = json.loads(metadata.get('structures', '[]'))
                        metadata['keywords'] = json.loads(metadata.get('keywords', '[]'))
                        metadata['legal_entities'] = json.loads(metadata.get('legal_entities', '[]'))
                    except:
                        pass
                    
                    result_item = {
                        'text': doc,
                        'metadata': metadata,
                        'similarity_score': float(similarity),
                        'distance': float(distance),
                        'rank': i + 1
                    }
                    
                    processed_results.append(result_item)
            
            print(f"âœ… {len(processed_results)} Ù†ØªÛŒØ¬Ù‡ ÛŒØ§ÙØª Ø´Ø¯")
            
            return processed_results
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ: {str(e)}")
            return []
    
    def _build_where_clause(self, filters: Dict) -> Dict:
        """ØªØ¨Ø¯ÛŒÙ„ ÙÛŒÙ„ØªØ±Ù‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª ChromaDB"""
        where_clause = {}
        
        for key, value in filters.items():
            if key == 'document_type' and value:
                where_clause['document_type'] = {"$eq": value}
            elif key == 'authority' and value:
                where_clause['authority'] = {"$eq": value}
            elif key == 'min_quality_score' and value:
                where_clause['quality_score'] = {"$gte": float(value)}
            elif key == 'min_word_count' and value:
                where_clause['word_count'] = {"$gte": int(value)}
        
        return where_clause
    
    def get_statistics(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        try:
            total_count = self.collection.count()
            
            if total_count == 0:
                return {
                    'total_documents': 0,
                    'message': 'Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª'
                }
            
            # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…Ø§Ø±Ú¯ÛŒØ±ÛŒ
            sample_size = min(100, total_count)
            sample_results = self.collection.get(limit=sample_size, include=['metadatas'])
            
            if not sample_results['metadatas']:
                return {'total_documents': total_count}
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
            doc_types = {}
            authorities = {}
            quality_scores = []
            word_counts = []
            
            for metadata in sample_results['metadatas']:
                # Ù†ÙˆØ¹ Ø³Ù†Ø¯
                doc_type = metadata.get('document_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
                
                # Ù…Ø±Ø¬Ø¹
                authority = metadata.get('authority', 'Ù†Ø§Ù…Ø´Ø®Øµ')
                authorities[authority] = authorities.get(authority, 0) + 1
                
                # Ú©ÛŒÙÛŒØª
                quality = metadata.get('quality_score', 0)
                if isinstance(quality, (int, float)):
                    quality_scores.append(float(quality))
                
                # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª
                word_count = metadata.get('word_count', 0)
                if isinstance(word_count, int):
                    word_counts.append(word_count)
            
            stats = {
                'total_documents': total_count,
                'sample_size': len(sample_results['metadatas']),
                'document_types': doc_types,
                'authorities': authorities,
                'quality_stats': {
                    'average': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
                    'min': min(quality_scores) if quality_scores else 0,
                    'max': max(quality_scores) if quality_scores else 0
                },
                'word_count_stats': {
                    'average': sum(word_counts) / len(word_counts) if word_counts else 0,
                    'min': min(word_counts) if word_counts else 0,
                    'max': max(word_counts) if word_counts else 0
                }
            }
            
            return stats
            
        except Exception as e:
            return {'error': f'Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±: {str(e)}'}
    
    def search_by_filters(self, filters: Dict, limit: int = 50) -> List[Dict]:
        """Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ metadata"""
        try:
            where_clause = self._build_where_clause(filters)
            
            results = self.collection.get(
                where=where_clause,
                limit=limit,
                include=['documents', 'metadatas']
            )
            
            processed_results = []
            
            if results['documents']:
                for i, doc in enumerate(results['documents']):
                    metadata = results['metadatas'][i]
                    
                    # Parse JSON fields
                    try:
                        metadata['structures'] = json.loads(metadata.get('structures', '[]'))
                        metadata['keywords'] = json.loads(metadata.get('keywords', '[]'))
                        metadata['legal_entities'] = json.loads(metadata.get('legal_entities', '[]'))
                    except:
                        pass
                    
                    result_item = {
                        'text': doc,
                        'metadata': metadata,
                        'rank': i + 1
                    }
                    
                    processed_results.append(result_item)
            
            return processed_results
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ÙÛŒÙ„ØªØ±ÛŒ: {str(e)}")
            return []
    
    def delete_collection(self):
        """Ø­Ø°Ù collection"""
        try:
            self.client.delete_collection(self.collection_name)
            print(f"âœ… Collection {self.collection_name} Ø­Ø°Ù Ø´Ø¯")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù collection: {str(e)}")
    
    def backup_database(self, backup_path: str):
        """Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        try:
            import shutil
            shutil.copytree(self.db_path, backup_path)
            print(f"âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ø± {backup_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {str(e)}")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    print("ğŸ”§ Legal AI Assistant - Vector Database Manager")
    print("=" * 50)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    db = LegalVectorDatabase()
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± ÙØ¹Ù„ÛŒ
    stats = db.get_statistics()
    print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # Ø§Ú¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø§Ù„ÛŒ Ø§Ø³ØªØŒ chunks Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†
    if stats.get('total_documents', 0) == 0:
        print(f"\nğŸ”„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ chunks...")
        chunks_dir = "data/chunks"
        
        if os.path.exists(chunks_dir):
            success = db.add_chunks_from_files(chunks_dir)
            if success:
                print(f"âœ… chunks Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
                
                # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ø¬Ø¯ÛŒØ¯
                new_stats = db.get_statistics()
                print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ø¬Ø¯ÛŒØ¯:")
                for key, value in new_stats.items():
                    print(f"   {key}: {value}")
            else:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ chunks")
        else:
            print(f"âŒ ÙÙˆÙ„Ø¯Ø± chunks ÛŒØ§ÙØª Ù†Ø´Ø¯: {chunks_dir}")
            print(f"Ø§Ø¨ØªØ¯Ø§ ÙØ§Ø² 1 Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯: python run_phase1.py")
    
    # ØªØ³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡
    if stats.get('total_documents', 0) > 0:
        print(f"\nğŸ” ØªØ³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡...")
        test_query = "Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ"
        results = db.search_semantic(test_query, n_results=3)
        
        print(f"Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ '{test_query}':")
        for i, result in enumerate(results, 1):
            print(f"{i}. Ø§Ù…ØªÛŒØ§Ø²: {result['similarity_score']:.3f}")
            print(f"   Ù…ØªÙ†: {result['text'][:100]}...")
            print(f"   Ù†ÙˆØ¹: {result['metadata'].get('document_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
            print()


if __name__ == "__main__":
    main()