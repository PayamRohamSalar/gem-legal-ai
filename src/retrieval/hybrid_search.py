# src/retrieval/hybrid_search.py

import os
import json
import re
from typing import List, Dict, Tuple, Optional, Any
from collections import defaultdict
import numpy as np
from datetime import datetime

from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import hazm

# Ø§Ú¯Ø± relative import Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ØŒ Ø§Ø² absolute import Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
try:
    from .vector_database import LegalVectorDatabase
    from .embedding_manager import EmbeddingModelManager
except ImportError:
    from src.retrieval.vector_database import LegalVectorDatabase
    from src.retrieval.embedding_manager import EmbeddingModelManager

class PersianTextProcessor:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒÚ¯Ø± Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ"""
    
    def __init__(self):
        self.normalizer = hazm.Normalizer()
        self.stemmer = hazm.Stemmer()
        self.lemmatizer = hazm.Lemmatizer()
        
        # Ú©Ù„Ù…Ø§Øª stop ÙØ§Ø±Ø³ÛŒ
        self.stop_words = {
            'Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ø¨Ø§', 'Ø¨Ø±Ø§ÛŒ', 'ØªØ§', 'Ø¨Ø±', 'Ø±Ø§',
            'Ø§Ø³Øª', 'Ø¨ÙˆØ¯', 'Ù…ÛŒ', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ø¨Ø§ÛŒØ¯', 'Ø´Ø¯Ù‡', 'Ø´ÙˆØ¯', 'Ú©Ø±Ø¯', 'Ù†Ù…ÙˆØ¯', 'Ú¯Ø±ÙØª',
            'Ù‡Ø§ÛŒ', 'Ù‡Ø§', 'Ø§Ù†', 'ÛŒØ§', 'Ø§Ú¯Ø±', 'Ú†ÙˆÙ†', 'Ú†Ø±Ø§', 'Ú©Ø¬Ø§', 'Ú©ÛŒ', 'Ú†Ù‡', 'Ù‡Ø±',
            'Ù‡Ù…Ù‡', 'Ù‡Ù…Ú†Ù†ÛŒÙ†', 'ÙˆÙ„ÛŒ', 'Ø§Ù…Ø§', 'ÛŒØ¹Ù†ÛŒ', 'Ù…Ø«Ù„', 'Ù…Ø§Ù†Ù†Ø¯', 'Ù†ÛŒØ²', 'Ù‡Ù…'
        }
        
        # Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù… Ø­Ù‚ÙˆÙ‚ÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
        self.legal_keywords = {
            'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡', 'ÙØµÙ„', 'Ù…Ù‚Ø±Ø±', 'Ù…ÙˆØ¶ÙˆØ¹', 'Ù…ØµÙˆØ¨',
            'Ù…Ø¬Ù„Ø³', 'Ù‡ÛŒØ¦Øª', 'ÙˆØ²ÛŒØ±Ø§Ù†', 'Ø´ÙˆØ±Ø§ÛŒ', 'Ø¹Ø§Ù„ÛŒ', 'ÙˆØ²Ø§Ø±Øª', 'Ù…Ø¤Ø³Ø³Ù‡',
            'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', 'Ù¾Ú˜ÙˆÙ‡Ø´', 'ÙÙ†Ø§ÙˆØ±ÛŒ', 'ØªØ­Ù‚ÛŒÙ‚Ø§Øª', 'Ø¹Ù„ÙˆÙ…', 'Ø¢Ù…ÙˆØ²Ø´',
            'Ø§Ù†ØªØ¸Ø§Ù…ÛŒ', 'ØªØ®Ù„Ù', 'Ù…Ø¬Ø§Ø²Ø§Øª', 'ØªØ¹Ù‡Ø¯', 'Ù…Ø³Ø¦ÙˆÙ„ÛŒØª', 'Ø­Ù‚', 'ÙˆØ¸ÛŒÙÙ‡'
        }
    
    def process_text(self, text: str, for_search: bool = True) -> List[str]:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
        
        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
            for_search: Ø¢ÛŒØ§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³Øª ÛŒØ§ Ù†Ù‡
            
        Returns:
            list: Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
        """
        if not text:
            return []
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        text = self.normalizer.normalize(text)
        
        # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ (Ø¬Ø² Ù†Ù‚Ø·Ù‡ Ùˆ Ú©Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø±)
        text = re.sub(r'[^\w\s\.\,]', ' ', text)
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
        words = text.split()
        
        processed_words = []
        
        for word in words:
            word = word.strip()
            if not word:
                continue
            
            # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ù„Øµ (Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¬Ø²Ø¡ Ø¹Ø¨Ø§Ø±Øª Ø­Ù‚ÙˆÙ‚ÛŒ Ø¨Ø§Ø´Ù†Ø¯)
            if word.isdigit() and len(word) > 4:
                continue
            
            # Ú©Ù„Ù…Ø§Øª Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡
            if len(word) < 2:
                continue
            
            # Ø­Ø°Ù stop words (Ù…Ú¯Ø± Ú©Ù„Ù…Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ Ù…Ù‡Ù…)
            if for_search and word in self.stop_words and word not in self.legal_keywords:
                continue
            
            # stemming Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
            if for_search and word not in self.legal_keywords:
                try:
                    word = self.stemmer.stem(word)
                except:
                    pass  # Ø§Ú¯Ø± stemming Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ Ú©Ù„Ù…Ù‡ Ø§ØµÙ„ÛŒ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
            
            processed_words.append(word)
        
        return processed_words
    
    def extract_key_phrases(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ù…ØªÙ†"""
        phrases = []
        
        # Ù¾ØªØ±Ù†â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        legal_patterns = [
            r'Ù…Ø§Ø¯Ù‡\s+\d+',
            r'Ø¨Ù†Ø¯\s+[Ø§Ù„Ù-ÛŒ]',
            r'ØªØ¨ØµØ±Ù‡\s+\d*',
            r'ÙØµÙ„\s+\d+',
            r'Ù‚Ø§Ù†ÙˆÙ†\s+[\w\s]{5,50}',
            r'Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡\s+[\w\s]{5,50}',
            r'Ù…ØµÙˆØ¨\s+\d{2,4}/\d{1,2}/\d{1,2}'
        ]
        
        for pattern in legal_patterns:
            matches = re.findall(pattern, text)
            phrases.extend(matches)
        
        return phrases


class HybridSearchEngine:
    """
    Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ (Semantic + Keyword)
    
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ùˆ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒØ§ÛŒ Ø±Ø§ ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """
    
    def __init__(self, 
                 vector_db: LegalVectorDatabase,
                 embedding_manager: EmbeddingModelManager):
        
        self.vector_db = vector_db
        self.embedding_manager = embedding_manager
        self.text_processor = PersianTextProcessor()
        
        # BM25 index
        self.bm25_index = None
        self.bm25_documents = []
        self.bm25_metadata = []
        
        # TF-IDF vectorizer
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
        self._build_keyword_indices()
    
    
    # Ø¯Ø± ÙØ§ÛŒÙ„ src/retrieval/hybrid_search.py

    def _build_keyword_indices(self):
        """Ø³Ø§Ø®Øª indices Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒØ§ÛŒ"""
        print("ğŸ”„ Ø³Ø§Ø®Øª indices Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒØ§ÛŒ...")
        
        try:
            # Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… documents Ø§Ø² vector database - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
            # Ø¨Ù‡ Ø¬Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÛŒÙ„ØªØ±ØŒ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² collection.get Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if self.vector_db.collection.count() == 0:
                print("âŒ Ù‡ÛŒÚ† document Ø¯Ø± vector database ÛŒØ§ÙØª Ù†Ø´Ø¯!")
                return
            
            # Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø¯ÙˆÙ† ÙÛŒÙ„ØªØ±
            all_results_raw = self.vector_db.collection.get(
                limit=self.vector_db.collection.count(), 
                include=['documents', 'metadatas']
            )

            print(f"ğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ {len(all_results_raw['ids'])} document...")
            
            # Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯...
            processed_texts = []
            raw_texts = []
            metadatas = []

            for i in range(len(all_results_raw['ids'])):
                text = all_results_raw['documents'][i]
                metadata = all_results_raw['metadatas'][i]
                
                processed_words = self.text_processor.process_text(text, for_search=True)
                processed_texts.append(processed_words)
                
                raw_texts.append(text)
                metadatas.append(metadata)

            # Ø¨Ù‚ÛŒÙ‡ Ù…ØªØ¯ Ù†ÛŒØ² Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø³Øª
            if processed_texts:
                self.bm25_index = BM25Okapi(processed_texts)
                self.bm25_documents = raw_texts
                self.bm25_metadata = metadatas
                print(f"âœ… BM25 index Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ø¨Ø§ {len(processed_texts)} document")
            
            if raw_texts:
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=5000,
                    stop_words=list(self.text_processor.stop_words),
                    ngram_range=(1, 2),
                    min_df=2
                )
                self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(raw_texts)
                print(f"âœ… TF-IDF index Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ø¨Ø§ {self.tfidf_matrix.shape[1]} feature")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª keyword indices: {str(e)}")
    
    
    def search_bm25(self, query: str, top_k: int = 20) -> List[Dict]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ BM25"""
        if not self.bm25_index:
            return []
        
        try:
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ query
            processed_query = self.text_processor.process_text(query, for_search=True)
            
            if not processed_query:
                return []
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª BM25
            scores = self.bm25_index.get_scores(processed_query)
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ top results
            top_indices = np.argsort(scores)[::-1][:top_k]
            
            results = []
            for rank, idx in enumerate(top_indices):
                if scores[idx] > 0:  # ÙÙ‚Ø· Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ù…Ø«Ø¨Øª
                    results.append({
                        'text': self.bm25_documents[idx],
                        'metadata': self.bm25_metadata[idx],
                        'bm25_score': float(scores[idx]),
                        'rank': rank + 1,
                        'search_type': 'bm25'
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ BM25: {str(e)}")
            return []
    
    def search_tfidf(self, query: str, top_k: int = 20) -> List[Dict]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ TF-IDF"""
        if not self.tfidf_vectorizer or self.tfidf_matrix is None:
            return []
        
        try:
            # ØªØ¨Ø¯ÛŒÙ„ query Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± TF-IDF
            query_vector = self.tfidf_vectorizer.transform([query])
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª cosine
            similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ top results
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for rank, idx in enumerate(top_indices):
                if similarities[idx] > 0.01:  # threshold Ú©Ù…ÛŒÙ†Ù‡
                    results.append({
                        'text': self.bm25_documents[idx],
                        'metadata': self.bm25_metadata[idx],
                        'tfidf_score': float(similarities[idx]),
                        'rank': rank + 1,
                        'search_type': 'tfidf'
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ TF-IDF: {str(e)}")
            return []
    
    def search_semantic(self, query: str, top_k: int = 20, filters: Dict = None) -> List[Dict]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ"""
        try:
            results = self.vector_db.search_semantic(query, n_results=top_k, filters=filters)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ
            for result in results:
                result['search_type'] = 'semantic'
            
            return results
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ: {str(e)}")
            return []
    
    def hybrid_search(self, 
                     query: str,
                     top_k: int = 10,
                     semantic_weight: float = 0.6,
                     bm25_weight: float = 0.3,
                     tfidf_weight: float = 0.1,
                     filters: Dict = None) -> List[Dict]:
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ (Ù‡ÛŒØ¨Ø±ÛŒØ¯)
        
        Args:
            query: Ù…ØªÙ† Ø¬Ø³ØªØ¬Ùˆ
            top_k: ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ
            semantic_weight: ÙˆØ²Ù† Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
            bm25_weight: ÙˆØ²Ù† BM25
            tfidf_weight: ÙˆØ²Ù† TF-IDF
            filters: ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ metadata
            
        Returns:
            list: Ù†ØªØ§ÛŒØ¬ ØªØ±Ú©ÛŒØ¨ Ø´Ø¯Ù‡ Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
        """
        print(f"ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÛŒØ¨Ø±ÛŒØ¯ Ø¨Ø±Ø§ÛŒ: '{query[:50]}...'")
        
        # Ø§Ù†Ø¬Ø§Ù… Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        semantic_results = self.search_semantic(query, top_k=top_k*2, filters=filters)
        bm25_results = self.search_bm25(query, top_k=top_k*2)
        tfidf_results = self.search_tfidf(query, top_k=top_k*2)
        
        print(f"ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø®Ø§Ù…: {len(semantic_results)} semantic, {len(bm25_results)} BM25, {len(tfidf_results)} TF-IDF")
        
        # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
        combined_results = self._combine_results(
            semantic_results, bm25_results, tfidf_results,
            semantic_weight, bm25_weight, tfidf_weight
        )
        
        # Re-ranking Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†
        final_results = self._rerank_results(combined_results, query)[:top_k]
        
        print(f"âœ… {len(final_results)} Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ")
        
        return final_results
    
    def _combine_results(self, 
                        semantic_results: List[Dict],
                        bm25_results: List[Dict], 
                        tfidf_results: List[Dict],
                        semantic_weight: float,
                        bm25_weight: float,
                        tfidf_weight: float) -> List[Dict]:
        """ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù"""
        
        # Ø§ÛŒØ¬Ø§Ø¯ dictionary Ø¨Ø±Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª
        combined_scores = defaultdict(lambda: {
            'text': '', 'metadata': {}, 'scores': {}, 'total_score': 0.0
        })
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª semantic
        if semantic_results:
            max_semantic = max(r['similarity_score'] for r in semantic_results)
            for result in semantic_results:
                text = result['text']
                normalized_score = result['similarity_score'] / max_semantic if max_semantic > 0 else 0
                
                combined_scores[text]['text'] = text
                combined_scores[text]['metadata'] = result['metadata']
                combined_scores[text]['scores']['semantic'] = normalized_score
                combined_scores[text]['total_score'] += normalized_score * semantic_weight
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª BM25
        if bm25_results:
            max_bm25 = max(r['bm25_score'] for r in bm25_results)
            for result in bm25_results:
                text = result['text']
                normalized_score = result['bm25_score'] / max_bm25 if max_bm25 > 0 else 0
                
                if text not in combined_scores:
                    combined_scores[text]['text'] = text
                    combined_scores[text]['metadata'] = result['metadata']
                
                combined_scores[text]['scores']['bm25'] = normalized_score
                combined_scores[text]['total_score'] += normalized_score * bm25_weight
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª TF-IDF
        if tfidf_results:
            max_tfidf = max(r['tfidf_score'] for r in tfidf_results)
            for result in tfidf_results:
                text = result['text']
                normalized_score = result['tfidf_score'] / max_tfidf if max_tfidf > 0 else 0
                
                if text not in combined_scores:
                    combined_scores[text]['text'] = text
                    combined_scores[text]['metadata'] = result['metadata']
                
                combined_scores[text]['scores']['tfidf'] = normalized_score
                combined_scores[text]['total_score'] += normalized_score * tfidf_weight
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù„ÛŒØ³Øª Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ
        final_results = []
        for text, data in combined_scores.items():
            final_results.append({
                'text': data['text'],
                'metadata': data['metadata'],
                'hybrid_score': data['total_score'],
                'component_scores': data['scores'],
                'search_type': 'hybrid'
            })
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„
        final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        return final_results
    
    def _rerank_results(self, results: List[Dict], query: str) -> List[Dict]:
        """re-ranking Ù†ØªØ§ÛŒØ¬ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² query
        query_phrases = self.text_processor.extract_key_phrases(query)
        query_words = set(self.text_processor.process_text(query, for_search=True))
        
        for i, result in enumerate(results):
            text = result['text']
            metadata = result['metadata']
            
            # Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¹Ø¨Ø§Ø±Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ
            phrase_bonus = 0.0
            for phrase in query_phrases:
                if phrase.lower() in text.lower():
                    phrase_bonus += 0.1
            
            # Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª chunk
            quality_bonus = metadata.get('quality_score', 0) / 10.0 * 0.05
            
            # Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ·Ø§Ø¨Ù‚ Ù†ÙˆØ¹ Ø³Ù†Ø¯
            doc_type_bonus = 0.0
            if any(word in ['Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯'] for word in query_words):
                if metadata.get('document_type') == 'Ù‚Ø§Ù†ÙˆÙ†':
                    doc_type_bonus = 0.02
            
            # Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ Ø¬Ø¯ÛŒØ¯
            total_bonus = phrase_bonus + quality_bonus + doc_type_bonus
            result['final_score'] = result['hybrid_score'] + total_bonus
            result['rerank_bonus'] = total_bonus
            result['rank'] = i + 1
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯
        results.sort(key=lambda x: x['final_score'], reverse=True)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ rank
        for i, result in enumerate(results):
            result['rank'] = i + 1
        
        return results
    
    def get_search_stats(self) -> Dict:
        """Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ… Ø¬Ø³ØªØ¬Ùˆ"""
        stats = {
            'vector_db_stats': self.vector_db.get_statistics(),
            'bm25_ready': self.bm25_index is not None,
            'tfidf_ready': self.tfidf_vectorizer is not None,
            'total_indexed_documents': len(self.bm25_documents),
            'embedding_model': self.embedding_manager.current_model_name
        }
        
        if self.tfidf_vectorizer:
            stats['tfidf_features'] = self.tfidf_matrix.shape[1]
        
        return stats


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    print("ğŸ”§ Legal AI Assistant - Hybrid Search Engine")
    print("=" * 50)
    
    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ components
    print("ğŸ”„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§Ø¬Ø²Ø§ÛŒ Ø³ÛŒØ³ØªÙ…...")
    
    # Vector Database
    vector_db = LegalVectorDatabase()
    
    # Embedding Manager
    embedding_manager = EmbeddingModelManager()
    recommended_model = embedding_manager.get_recommended_model()
    
    if not embedding_manager.load_model(recommended_model):
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ embedding model!")
        return
    
    # Hybrid Search Engine
    search_engine = HybridSearchEngine(vector_db, embedding_manager)
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
    stats = search_engine.get_search_stats()
    print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ… Ø¬Ø³ØªØ¬Ùˆ:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # ØªØ³Øª Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
    test_queries = [
        "Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ",
        "Ù…Ø¬Ø§Ø²Ø§Øª ØªØ®Ù„ÙØ§Øª Ø§Ù†Ø¶Ø¨Ø§Ø·ÛŒ",
        "ÙˆØ²Ø§Ø±Øª Ø¹Ù„ÙˆÙ… ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ",
        "Ù…Ø§Ø¯Ù‡ 7 ØªØ®Ù„ÙØ§Øª"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ØªØ³Øª Ø¬Ø³ØªØ¬Ùˆ: '{query}'")
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÛŒØ¨Ø±ÛŒØ¯
        results = search_engine.hybrid_search(query, top_k=3)
        
        if results:
            for i, result in enumerate(results, 1):
                print(f"{i}. Ø§Ù…ØªÛŒØ§Ø²: {result['final_score']:.3f}")
                print(f"   Ù…ØªÙ†: {result['text'][:80]}...")
                print(f"   Ù†ÙˆØ¹: {result['metadata'].get('document_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')}")
        else:
            print("   Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")


if __name__ == "__main__":
    main()