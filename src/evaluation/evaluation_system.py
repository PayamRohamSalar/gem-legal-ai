"""
src/evaluation/phase4_evaluation_system.py - Ø³ÛŒØ³ØªÙ… ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ø² 4

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§ÛŒ ÙØ§Ø² 4 Ø±Ø§ Ø§Ø¯ØºØ§Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
"""

import json
import logging
import asyncio
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import statistics
import sys

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src
current_dir = Path(__file__).parent.parent.parent
src_path = current_dir / 'src'
sys.path.insert(0, str(src_path))

# Import Ø§Ø¬Ø²Ø§ÛŒ ÙØ§Ø² 4
from evaluation.dataset_generator import LegalDatasetGenerator, TestQuestion, QuestionDifficulty, QuestionCategory
from evaluation.metrics_calculator import LegalMetricsCalculator, OverallMetrics
from evaluation.optimization_manager import LegalSystemOptimizer, OptimizationConfig, OptimizationResult

# Import Ø§Ø¬Ø²Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„ÛŒ
from generation.integrated_response_system import LegalResponseSystem, ResponseRequest
from generation.llm_manager import LLMManager, create_model_configs

logger = logging.getLogger(__name__)

@dataclass
class EvaluationConfig:
    """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""
    dataset_size: int = 100
    test_subset_size: int = 20  # Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹
    use_ollama: bool = True
    preferred_model: str = 'qwen_7b'
    timeout_per_question: int = 30  # Ø«Ø§Ù†ÛŒÙ‡
    parallel_evaluations: int = 1
    save_detailed_results: bool = True
    
@dataclass
class QuestionEvaluation:
    """Ù†ØªÛŒØ¬Ù‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÛŒÚ© Ø³ÙˆØ§Ù„"""
    question_id: str
    question: str
    expected_answer: str
    generated_answer: str
    overall_metrics: OverallMetrics
    processing_time: float
    success: bool
    error_message: Optional[str] = None

@dataclass
class SystemEvaluation:
    """Ù†ØªÛŒØ¬Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÛŒØ³ØªÙ…"""
    config: EvaluationConfig
    total_questions: int
    successful_evaluations: int
    failed_evaluations: int
    overall_score: float
    detailed_metrics: Dict[str, float]
    question_results: List[QuestionEvaluation]
    evaluation_time: float
    timestamp: str

class Phase4EvaluationSystem:
    """Ø³ÛŒØ³ØªÙ… ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ø² 4"""
    
    def __init__(self, output_dir: str = "data/phase4_evaluation"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ…
        self.dataset_generator = LegalDatasetGenerator()
        self.metrics_calculator = LegalMetricsCalculator()
        self.legal_system: Optional[LegalResponseSystem] = None
        self.optimizer: Optional[LegalSystemOptimizer] = None
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        self.current_config: Optional[EvaluationConfig] = None
        self.evaluation_history: List[SystemEvaluation] = []
        
        logger.info("Phase4EvaluationSystem Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    async def initialize_system(self, config: EvaluationConfig) -> bool:
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø­Ù‚ÙˆÙ‚ÛŒ"""
        
        self.current_config = config
        
        try:
            # Ø§ÛŒØ¬Ø§Ø¯ Ø³ÛŒØ³ØªÙ… Ø­Ù‚ÙˆÙ‚ÛŒ
            llm_configs = create_model_configs()
            self.legal_system = LegalResponseSystem(
                llm_configs=llm_configs,
                default_model=config.preferred_model
            )
            
            # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…
            success = await self.legal_system.initialize()
            if not success:
                logger.error("Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø­Ù‚ÙˆÙ‚ÛŒ")
                return False
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù…Ù†Ø§Ø³Ø¨
            if config.use_ollama:
                model_loaded = self.legal_system.llm_manager.load_model(
                    config.preferred_model, 
                    prefer_ollama=True
                )
            else:
                model_loaded = self.legal_system.llm_manager.load_model_mock(
                    config.preferred_model
                )
            
            if not model_loaded:
                logger.warning("Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Mock")
                self.legal_system.llm_manager.load_model_mock(config.preferred_model)
            
            logger.info("Ø³ÛŒØ³ØªÙ… Ø­Ù‚ÙˆÙ‚ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ: {e}")
            return False
    
    def generate_test_dataset(self, config: EvaluationConfig) -> List[TestQuestion]:
        """ØªÙˆÙ„ÛŒØ¯ dataset ØªØ³Øª"""
        
        logger.info(f"ØªÙˆÙ„ÛŒØ¯ {config.dataset_size} Ø³ÙˆØ§Ù„ ØªØ³Øª...")
        
        # ØªÙˆØ²ÛŒØ¹ Ù…ØªØ¹Ø§Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        difficulty_distribution = {
            QuestionDifficulty.BASIC: 0.5,        # 50% Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹
            QuestionDifficulty.INTERMEDIATE: 0.3,  # 30% Ù…ØªÙˆØ³Ø·
            QuestionDifficulty.ADVANCED: 0.2       # 20% Ù¾ÛŒØ´Ø±ÙØªÙ‡
        }
        
        category_distribution = {
            QuestionCategory.FACULTY_DUTIES: 0.4,
            QuestionCategory.KNOWLEDGE_BASED_COMPANIES: 0.3,
            QuestionCategory.TECHNOLOGY_TRANSFER: 0.2,
            QuestionCategory.RESEARCH_CONTRACTS: 0.1
        }
        
        questions = self.dataset_generator.generate_dataset(
            total_questions=config.dataset_size,
            difficulty_distribution=difficulty_distribution,
            category_distribution=category_distribution
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ dataset
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dataset_path = self.dataset_generator.save_dataset(
            questions, 
            f"phase4_dataset_{timestamp}.json"
        )
        
        logger.info(f"Dataset Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {dataset_path}")
        
        return questions
    
    async def evaluate_single_question(
        self, 
        question: TestQuestion
    ) -> QuestionEvaluation:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÛŒÚ© Ø³ÙˆØ§Ù„"""
        
        start_time = time.time()
        
        try:
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ context Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ·Ù‡
            contexts = self._prepare_question_contexts(question)
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
            request = ResponseRequest(
                question=question.question,
                contexts=contexts,
                include_citations=True,
                temperature=0.1
            )
            
            # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ timeout
            response_result = await asyncio.wait_for(
                self.legal_system.generate_response(request),
                timeout=self.current_config.timeout_per_question
            )
            
            processing_time = time.time() - start_time
            
            if not response_result.success:
                return QuestionEvaluation(
                    question_id=question.id,
                    question=question.question,
                    expected_answer=question.expected_answer,
                    generated_answer="",
                    overall_metrics=None,
                    processing_time=processing_time,
                    success=False,
                    error_message=response_result.error_message
                )
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            overall_metrics = self.metrics_calculator.calculate_overall_metrics(
                generated_text=response_result.enhanced_response,
                reference_text=question.expected_answer,
                retrieved_docs=contexts,
                relevant_docs=question.context_needed,
                expected_citations=question.relevant_articles,
                source_contexts=[ctx['content'] for ctx in contexts],
                expected_elements=question.keywords
            )
            
            return QuestionEvaluation(
                question_id=question.id,
                question=question.question,
                expected_answer=question.expected_answer,
                generated_answer=response_result.enhanced_response,
                overall_metrics=overall_metrics,
                processing_time=processing_time,
                success=True
            )
            
        except asyncio.TimeoutError:
            return QuestionEvaluation(
                question_id=question.id,
                question=question.question,
                expected_answer=question.expected_answer,
                generated_answer="",
                overall_metrics=None,
                processing_time=time.time() - start_time,
                success=False,
                error_message="Timeout"
            )
            
        except Exception as e:
            return QuestionEvaluation(
                question_id=question.id,
                question=question.question,
                expected_answer=question.expected_answer,
                generated_answer="",
                overall_metrics=None,
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    def _prepare_question_contexts(self, question: TestQuestion) -> List[Dict[str, Any]]:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ context Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„"""
        
        # Context Ù‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡ Ø³ÙˆØ§Ù„
        context_templates = {
            QuestionCategory.FACULTY_DUTIES: [
                {
                    "content": "Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ù…ÙˆØ¸Ù Ø¨Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ ØªØ®ØµØµÛŒ Ø®ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯",
                    "source": "Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ - Ù…Ø§Ø¯Ù‡ 3",
                    "document_type": "Ù‚Ø§Ù†ÙˆÙ†",
                    "article_number": "3",
                    "relevance_score": 0.95
                }
            ],
            QuestionCategory.KNOWLEDGE_BASED_COMPANIES: [
                {
                    "content": "Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¨Ù†ÛŒØ§Ù† Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ Ø¯Ø§Ù†Ø´ ÙÙ†ÛŒ Ø¨Ø±ÙˆØ² ÙØ¹Ø§Ù„ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯",
                    "source": "Ù‚Ø§Ù†ÙˆÙ† Ø­Ù…Ø§ÛŒØª Ø§Ø² Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ø´â€ŒØ¨Ù†ÛŒØ§Ù† - Ù…Ø§Ø¯Ù‡ 1",
                    "document_type": "Ù‚Ø§Ù†ÙˆÙ†",
                    "article_number": "1",
                    "relevance_score": 1.0
                }
            ],
            QuestionCategory.TECHNOLOGY_TRANSFER: [
                {
                    "content": "Ø§Ù†ØªÙ‚Ø§Ù„ ÙÙ†Ø§ÙˆØ±ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø§Ù†Ø´ ÙÙ†ÛŒ Ø§Ø² Ù…Ø±Ø§Ú©Ø² ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ù‡ Ø¹Ù„Ù… Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø§Ø³Øª",
                    "source": "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø§Ù†ØªÙ‚Ø§Ù„ ÙÙ†Ø§ÙˆØ±ÛŒ - Ù…Ø§Ø¯Ù‡ 2",
                    "document_type": "Ø¢ÛŒÛŒÙ†_Ù†Ø§Ù…Ù‡",
                    "article_number": "2",
                    "relevance_score": 0.9
                }
            ]
        }
        
        # Ø§Ù†ØªØ®Ø§Ø¨ context Ù…Ù†Ø§Ø³Ø¨
        return context_templates.get(question.category, [
            {
                "content": "Ù…ØªÙ† Ø¹Ù…ÙˆÙ…ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø³ÙˆØ§Ù„",
                "source": "Ù…Ù†Ø¨Ø¹ Ø¹Ù…ÙˆÙ…ÛŒ",
                "document_type": "Ù‚Ø§Ù†ÙˆÙ†",
                "relevance_score": 0.7
            }
        ])
    
    async def run_full_evaluation(
        self, 
        config: EvaluationConfig,
        questions: List[TestQuestion] = None
    ) -> SystemEvaluation:
        """Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…"""
        
        logger.info("Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…")
        start_time = time.time()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…
        if not await self.initialize_system(config):
            raise Exception("Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…")
        
        # ØªÙˆÙ„ÛŒØ¯ Ø³ÙˆØ§Ù„Ø§Øª Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        if questions is None:
            questions = self.generate_test_dataset(config)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ subset Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        if config.test_subset_size < len(questions):
            import random
            questions = random.sample(questions, config.test_subset_size)
            logger.info(f"Ø§Ù†ØªØ®Ø§Ø¨ {len(questions)} Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª")
        
        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÙˆØ§Ù„Ø§Øª
        question_results = []
        successful_count = 0
        
        for i, question in enumerate(questions, 1):
            logger.info(f"Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÙˆØ§Ù„ {i}/{len(questions)}: {question.question[:50]}...")
            
            result = await self.evaluate_single_question(question)
            question_results.append(result)
            
            if result.success:
                successful_count += 1
                logger.info(f"âœ… Ù…ÙˆÙÙ‚ - Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {result.overall_metrics.overall_score:.3f}")
            else:
                logger.warning(f"âŒ Ù†Ø§Ù…ÙˆÙÙ‚ - Ø®Ø·Ø§: {result.error_message}")
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ
        evaluation_time = time.time() - start_time
        
        if successful_count > 0:
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª
            overall_score = statistics.mean([
                r.overall_metrics.overall_score for r in question_results if r.success
            ])
            
            # Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
            detailed_metrics = self._calculate_detailed_metrics(question_results)
        else:
            overall_score = 0.0
            detailed_metrics = {}
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        system_evaluation = SystemEvaluation(
            config=config,
            total_questions=len(questions),
            successful_evaluations=successful_count,
            failed_evaluations=len(questions) - successful_count,
            overall_score=overall_score,
            detailed_metrics=detailed_metrics,
            question_results=question_results,
            evaluation_time=evaluation_time,
            timestamp=datetime.now().isoformat()
        )
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        if config.save_detailed_results:
            self._save_evaluation_results(system_evaluation)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.evaluation_history.append(system_evaluation)
        
        logger.info(f"Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯ - Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {overall_score:.3f}")
        
        return system_evaluation
    
    def _calculate_detailed_metrics(
        self, 
        question_results: List[QuestionEvaluation]
    ) -> Dict[str, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙØµÛŒÙ„ÛŒ"""
        
        successful_results = [r for r in question_results if r.success and r.overall_metrics]
        
        if not successful_results:
            return {}
        
        metrics = {}
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
        metrics['avg_precision_at_5'] = statistics.mean([
            r.overall_metrics.retrieval_metrics.precision_at_5 for r in successful_results
        ])
        
        metrics['avg_recall_at_5'] = statistics.mean([
            r.overall_metrics.retrieval_metrics.recall_at_5 for r in successful_results
        ])
        
        metrics['avg_mrr'] = statistics.mean([
            r.overall_metrics.retrieval_metrics.mrr for r in successful_results
        ])
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯
        metrics['avg_rouge_l'] = statistics.mean([
            r.overall_metrics.generation_metrics.rouge_l_f for r in successful_results
        ])
        
        metrics['avg_bleu_4'] = statistics.mean([
            r.overall_metrics.generation_metrics.bleu_4 for r in successful_results
        ])
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        metrics['avg_citation_accuracy'] = statistics.mean([
            r.overall_metrics.legal_accuracy.citation_accuracy for r in successful_results
        ])
        
        metrics['avg_legal_compliance'] = statistics.mean([
            r.overall_metrics.legal_accuracy.law_compliance for r in successful_results
        ])
        
        metrics['avg_completeness'] = statistics.mean([
            r.overall_metrics.legal_accuracy.completeness_score for r in successful_results
        ])
        
        # Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
        metrics['avg_processing_time'] = statistics.mean([
            r.processing_time for r in successful_results
        ])
        
        metrics['success_rate'] = len(successful_results) / len(question_results)
        
        return metrics
    
    def _save_evaluation_results(self, evaluation: SystemEvaluation) -> str:
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"phase4_evaluation_{timestamp}.json"
        filepath = self.output_dir / filename
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ dictionary Ù‚Ø§Ø¨Ù„ Ø°Ø®ÛŒØ±Ù‡
        data = {
            "evaluation_summary": {
                "timestamp": evaluation.timestamp,
                "total_questions": evaluation.total_questions,
                "successful_evaluations": evaluation.successful_evaluations,
                "success_rate": evaluation.successful_evaluations / evaluation.total_questions,
                "overall_score": evaluation.overall_score,
                "evaluation_time": evaluation.evaluation_time
            },
            "config": asdict(evaluation.config),
            "detailed_metrics": evaluation.detailed_metrics,
            "question_results": [
                {
                    "question_id": r.question_id,
                    "question": r.question,
                    "success": r.success,
                    "processing_time": r.processing_time,
                    "overall_score": r.overall_metrics.overall_score if r.overall_metrics else 0.0,
                    "error_message": r.error_message
                }
                for r in evaluation.question_results
            ]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")
        
        return str(filepath)
    
    def setup_optimization(self) -> LegalSystemOptimizer:
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        
        def evaluation_function(parameters: Dict[str, Any]) -> Dict[str, float]:
            """ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
            
            # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ø³ÛŒØ³ØªÙ… Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†Ø¯
            # Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯
            
            # Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ØŒ mock evaluation
            import random
            base_score = 0.75
            
            # ØªØ£Ø«ÛŒØ± Ø¨Ø±Ø®ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
            if parameters.get('chunk_size', 500) in [400, 500, 600]:
                base_score += 0.1
            
            if parameters.get('temperature', 0.1) <= 0.2:
                base_score += 0.1
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ù†ÙˆÛŒØ²
            noise = random.uniform(-0.05, 0.05)
            
            return {
                'retrieval_precision': min(1.0, base_score + noise),
                'retrieval_recall': min(1.0, base_score + noise * 0.8),
                'generation_rouge': min(1.0, base_score + noise * 1.2),
                'legal_accuracy': min(1.0, base_score + noise * 0.9),
                'citation_quality': min(1.0, base_score + noise * 1.1),
                'response_time': random.uniform(3, 12)
            }
        
        self.optimizer = LegalSystemOptimizer(
            evaluation_function=evaluation_function,
            output_dir=str(self.output_dir / "optimization")
        )
        
        return self.optimizer
    
    def generate_phase4_report(self) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ ÙØ§Ø² 4"""
        
        if not self.evaluation_history:
            return {"message": "Ù‡ÛŒÚ† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯Ù‡"}
        
        latest_evaluation = self.evaluation_history[-1]
        
        report = {
            "phase4_summary": {
                "completion_date": datetime.now().isoformat(),
                "total_evaluations": len(self.evaluation_history),
                "latest_overall_score": latest_evaluation.overall_score,
                "system_status": "Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø² 5" if latest_evaluation.overall_score > 0.7 else "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯"
            },
            
            "dataset_statistics": {
                "total_questions_generated": latest_evaluation.total_questions,
                "success_rate": latest_evaluation.successful_evaluations / latest_evaluation.total_questions,
                "average_processing_time": latest_evaluation.detailed_metrics.get('avg_processing_time', 0)
            },
            
            "performance_metrics": latest_evaluation.detailed_metrics,
            
            "model_performance": {
                "ollama_status": "Active" if self.current_config and self.current_config.use_ollama else "Mock",
                "preferred_model": self.current_config.preferred_model if self.current_config else "unknown"
            },
            
            "optimization_results": self.optimizer.get_optimization_summary() if self.optimizer else {"message": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯Ù‡"},
            
            "recommendations": self._generate_recommendations(latest_evaluation)
        }
        
        return report
    
    def _generate_recommendations(self, evaluation: SystemEvaluation) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬"""
        
        recommendations = []
        
        if evaluation.overall_score < 0.6:
            recommendations.append("Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ø¨Ø±Ø±Ø³ÛŒ prompt templates Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        if evaluation.detailed_metrics.get('success_rate', 0) < 0.8:
            recommendations.append("Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ø¨Ø±Ø±Ø³ÛŒ timeout Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ù„Ø§Ø²Ù… Ø§Ø³Øª")
        
        if evaluation.detailed_metrics.get('avg_processing_time', 0) > 10:
            recommendations.append("Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§Ù„Ø§ Ø§Ø³Øª. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        if evaluation.detailed_metrics.get('avg_citation_accuracy', 0) < 0.7:
            recommendations.append("Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ø¨Ù‡Ø¨ÙˆØ¯ citation engine Ù„Ø§Ø²Ù… Ø§Ø³Øª")
        
        if evaluation.detailed_metrics.get('avg_legal_compliance', 0) < 0.8:
            recommendations.append("Ø§Ù†Ø·Ø¨Ø§Ù‚ Ø­Ù‚ÙˆÙ‚ÛŒ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ø¨Ø§Ø²Ù†Ú¯Ø±ÛŒ prompt templates Ø­Ù‚ÙˆÙ‚ÛŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª")
        
        if not recommendations:
            recommendations.append("Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø§Ø³Øª. Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø² 5")
        
        return recommendations

# Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ§Ø² 4
async def run_phase4_complete_evaluation():
    """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ø² 4"""
    
    print("ðŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ ÙØ§Ø² 4")
    print("=" * 60)
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    config = EvaluationConfig(
        dataset_size=30,      # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù… Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        test_subset_size=10,  # ÙÙ‚Ø· 10 Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹
        use_ollama=True,      # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ollama
        preferred_model='qwen_7b',
        timeout_per_question=15,
        save_detailed_results=True
    )
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    evaluation_system = Phase4EvaluationSystem()
    
    try:
        # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        result = await evaluation_system.run_full_evaluation(config)
        
        print(f"\nðŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
        print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª: {result.total_questions}")
        print(f"   â€¢ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒÙ‡Ø§: {result.successful_evaluations}")
        print(f"   â€¢ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {result.successful_evaluations/result.total_questions*100:.1f}%")
        print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {result.overall_score:.3f}")
        print(f"   â€¢ Ø²Ù…Ø§Ù† Ú©Ù„: {result.evaluation_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
        
        if result.detailed_metrics:
            print(f"\nðŸ“ˆ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙØµÛŒÙ„ÛŒ:")
            for metric, value in result.detailed_metrics.items():
                print(f"   â€¢ {metric}: {value:.3f}")
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„
        report = evaluation_system.generate_phase4_report()
        
        print(f"\nðŸŽ¯ ÙˆØ¶Ø¹ÛŒØª ÙØ§Ø² 4: {report['phase4_summary']['system_status']}")
        
        if report.get('recommendations'):
            print(f"\nðŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:")
            for rec in report['recommendations']:
                print(f"   â€¢ {rec}")
        
        return result, report
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ: {e}")
        return None, None

# ØªØ³Øª
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª
    asyncio.run(run_phase4_complete_evaluation())