"""
src/evaluation/optimization_manager.py - Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ù…Ø³Ø¦ÙˆÙ„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ hyperparameterÙ‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ… Ø§Ø³Øª:
- ØªÙ†Ø¸ÛŒÙ… hyperparameterÙ‡Ø§ (chunk size, overlap, top_k)
- Ø¨Ù‡Ø¨ÙˆØ¯ prompt templates
- Fine-tuning embedding model (Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²)
"""

import json
import logging
import itertools
import statistics
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from copy import deepcopy
import concurrent.futures
import time

logger = logging.getLogger(__name__)

@dataclass
class HyperParameter:
    """ØªØ¹Ø±ÛŒÙ ÛŒÚ© hyperparameter"""
    name: str
    current_value: Any
    min_value: Optional[Any] = None
    max_value: Optional[Any] = None
    step: Optional[Any] = None
    possible_values: Optional[List[Any]] = None
    description: str = ""

@dataclass
class OptimizationResult:
    """Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
    parameters: Dict[str, Any]
    score: float
    metrics: Dict[str, float]
    evaluation_time: float
    iteration: int

@dataclass
class OptimizationConfig:
    """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
    max_iterations: int = 50
    early_stopping_patience: int = 10
    early_stopping_threshold: float = 0.001
    n_random_starts: int = 5
    parallel_evaluations: int = 2
    save_intermediate_results: bool = True

class LegalSystemOptimizer:
    """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ø³ÛŒØ³ØªÙ… Ø­Ù‚ÙˆÙ‚ÛŒ"""
    
    def __init__(
        self,
        evaluation_function: Callable,
        output_dir: str = "data/optimization_results"
    ):
        self.evaluation_function = evaluation_function
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ØªØ¹Ø±ÛŒÙ hyperparameterÙ‡Ø§
        self.hyperparameters = self._define_hyperparameters()
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.optimization_history: List[OptimizationResult] = []
        self.best_result: Optional[OptimizationResult] = None
        
        logger.info("LegalSystemOptimizer Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def _define_hyperparameters(self) -> Dict[str, HyperParameter]:
        """ØªØ¹Ø±ÛŒÙ hyperparameterÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
        
        return {
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Chunking
            "chunk_size": HyperParameter(
                name="chunk_size",
                current_value=500,
                possible_values=[300, 400, 500, 600, 700, 800],
                description="Ø§Ù†Ø¯Ø§Ø²Ù‡ chunk Ù‡Ø§ÛŒ Ù…ØªÙ† (Ú©Ù„Ù…Ù‡)"
            ),
            
            "chunk_overlap": HyperParameter(
                name="chunk_overlap",
                current_value=50,
                possible_values=[25, 50, 75, 100, 125],
                description="Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø¨ÛŒÙ† chunk Ù‡Ø§ (Ú©Ù„Ù…Ù‡)"
            ),
            
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Retrieval
            "top_k_retrieval": HyperParameter(
                name="top_k_retrieval",
                current_value=5,
                possible_values=[3, 5, 7, 10, 15],
                description="ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù†Ø§Ø¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡"
            ),
            
            "similarity_threshold": HyperParameter(
                name="similarity_threshold",
                current_value=0.7,
                possible_values=[0.5, 0.6, 0.7, 0.8, 0.9],
                description="Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø§Ø³Ù†Ø§Ø¯"
            ),
            
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Generation
            "temperature": HyperParameter(
                name="temperature",
                current_value=0.1,
                possible_values=[0.0, 0.1, 0.2, 0.3, 0.5],
                description="Ø¯Ù…Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ LLM"
            ),
            
            "max_tokens": HyperParameter(
                name="max_tokens",
                current_value=2048,
                possible_values=[1024, 1536, 2048, 3072, 4096],
                description="Ø­Ø¯Ø§Ú©Ø«Ø± ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ÛŒ"
            ),
            
            "top_p": HyperParameter(
                name="top_p",
                current_value=0.9,
                possible_values=[0.7, 0.8, 0.9, 0.95, 1.0],
                description="Top-p sampling"
            ),
            
            # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Context
            "context_window_size": HyperParameter(
                name="context_window_size",
                current_value=3000,
                possible_values=[2000, 2500, 3000, 3500, 4000],
                description="Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ context (Ú©Ø§Ø±Ø§Ú©ØªØ±)"
            ),
            
            "context_diversity_weight": HyperParameter(
                name="context_diversity_weight",
                current_value=0.3,
                possible_values=[0.1, 0.2, 0.3, 0.4, 0.5],
                description="ÙˆØ²Ù† ØªÙ†ÙˆØ¹ Ø¯Ø± Ø§Ù†ØªØ®Ø§Ø¨ context"
            )
        }
    
    def _evaluate_parameters(self, parameters: Dict[str, Any]) -> OptimizationResult:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÛŒÚ© set Ø§Ø² parameters"""
        
        start_time = time.time()
        
        try:
            # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            metrics = self.evaluation_function(parameters)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ
            score = self._calculate_composite_score(metrics)
            
            evaluation_time = time.time() - start_time
            
            result = OptimizationResult(
                parameters=parameters.copy(),
                score=score,
                metrics=metrics,
                evaluation_time=evaluation_time,
                iteration=len(self.optimization_history) + 1
            )
            
            logger.info(f"Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ {result.iteration}: Ø§Ù…ØªÛŒØ§Ø²={score:.4f}, Ø²Ù…Ø§Ù†={evaluation_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ parameters: {e}")
            
            return OptimizationResult(
                parameters=parameters.copy(),
                score=0.0,
                metrics={},
                evaluation_time=time.time() - start_time,
                iteration=len(self.optimization_history) + 1
            )
    
    def _calculate_composite_score(self, metrics: Dict[str, float]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        weights = {
            'retrieval_precision': 0.20,
            'retrieval_recall': 0.15,
            'generation_rouge': 0.20,
            'legal_accuracy': 0.25,
            'citation_quality': 0.15,
            'response_time': 0.05  # Ú©Ù…ØªØ±ÛŒÙ† ÙˆØ²Ù† Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª
        }
        
        score = 0.0
        total_weight = 0.0
        
        for metric_name, weight in weights.items():
            if metric_name in metrics:
                # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ (Ú©Ù…ØªØ± Ø¨Ù‡ØªØ±)
                if 'time' in metric_name:
                    # ØªØ¨Ø¯ÛŒÙ„ Ø²Ù…Ø§Ù† Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² (Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ø«Ø§Ù†ÛŒÙ‡)
                    metric_score = max(0, 1 - (metrics[metric_name] / 10))
                else:
                    metric_score = metrics[metric_name]
                
                score += weight * metric_score
                total_weight += weight
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        return score / total_weight if total_weight > 0 else 0.0
    
    def grid_search(
        self,
        config: OptimizationConfig = None,
        param_subset: List[str] = None
    ) -> OptimizationResult:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ø¨Ú©Ù‡â€ŒØ§ÛŒ (Grid Search)"""
        
        if config is None:
            config = OptimizationConfig()
        
        logger.info("Ø´Ø±ÙˆØ¹ Grid Search")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ subset Ø§Ø² parameters
        if param_subset:
            params_to_optimize = {
                k: v for k, v in self.hyperparameters.items() 
                if k in param_subset
            }
        else:
            # Ø§Ù†ØªØ®Ø§Ø¨ parameters Ù…Ù‡Ù…â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ù†ÙØ¬Ø§Ø± ØªØ±Ú©ÛŒØ¨ÛŒ
            params_to_optimize = {
                k: v for k, v in self.hyperparameters.items()
                if k in ['chunk_size', 'top_k_retrieval', 'temperature', 'context_window_size']
            }
        
        # ØªÙˆÙ„ÛŒØ¯ ØªÙ…Ø§Ù… ØªØ±Ú©ÛŒØ¨Ø§Øª Ù…Ù…Ú©Ù†
        param_names = list(params_to_optimize.keys())
        param_values = [params_to_optimize[name].possible_values for name in param_names]
        
        total_combinations = 1
        for values in param_values:
            total_combinations *= len(values)
        
        logger.info(f"ØªØ¹Ø¯Ø§Ø¯ ØªØ±Ú©ÛŒØ¨Ø§Øª: {total_combinations}")
        
        if total_combinations > config.max_iterations:
            logger.warning(f"ØªØ¹Ø¯Ø§Ø¯ ØªØ±Ú©ÛŒØ¨Ø§Øª ({total_combinations}) Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² ({config.max_iterations})")
            # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ ØªØµØ§Ø¯ÙÛŒ
            all_combinations = list(itertools.product(*param_values))
            import random
            random.shuffle(all_combinations)
            selected_combinations = all_combinations[:config.max_iterations]
        else:
            selected_combinations = list(itertools.product(*param_values))
        
        best_result = None
        results = []
        
        for i, combination in enumerate(selected_combinations):
            # Ø³Ø§Ø®Øª dictionary parameters
            parameters = {
                param_names[j]: combination[j] 
                for j in range(len(param_names))
            }
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ± parameters Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± default
            for name, hp in self.hyperparameters.items():
                if name not in parameters:
                    parameters[name] = hp.current_value
            
            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            result = self._evaluate_parameters(parameters)
            results.append(result)
            self.optimization_history.append(result)
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
            if best_result is None or result.score > best_result.score:
                best_result = result
                self.best_result = result
                logger.info(f"ðŸŽ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¬Ø¯ÛŒØ¯: {result.score:.4f}")
            
            # Early stopping
            if self._should_stop_early(results, config):
                logger.info(f"Early stopping Ø¯Ø± iteration {i+1}")
                break
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù…ÛŒØ§Ù†ÛŒ
            if config.save_intermediate_results and (i + 1) % 10 == 0:
                self._save_intermediate_results(results)
        
        logger.info(f"Grid Search ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {best_result.score:.4f}")
        
        return best_result
    
    def random_search(
        self,
        config: OptimizationConfig = None,
        param_subset: List[str] = None
    ) -> OptimizationResult:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµØ§Ø¯ÙÛŒ (Random Search)"""
        
        if config is None:
            config = OptimizationConfig()
        
        logger.info("Ø´Ø±ÙˆØ¹ Random Search")
        
        import random
        
        best_result = None
        results = []
        no_improvement_count = 0
        
        for iteration in range(config.max_iterations):
            # Ø§Ù†ØªØ®Ø§Ø¨ ØªØµØ§Ø¯ÙÛŒ parameters
            parameters = {}
            
            for name, hp in self.hyperparameters.items():
                if param_subset is None or name in param_subset:
                    if hp.possible_values:
                        parameters[name] = random.choice(hp.possible_values)
                    else:
                        # Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ continuous
                        if hp.min_value is not None and hp.max_value is not None:
                            parameters[name] = random.uniform(hp.min_value, hp.max_value)
                        else:
                            parameters[name] = hp.current_value
                else:
                    parameters[name] = hp.current_value
            
            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            result = self._evaluate_parameters(parameters)
            results.append(result)
            self.optimization_history.append(result)
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
            if best_result is None or result.score > best_result.score:
                best_result = result
                self.best_result = result
                no_improvement_count = 0
                logger.info(f"ðŸŽ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¬Ø¯ÛŒØ¯: {result.score:.4f}")
            else:
                no_improvement_count += 1
            
            # Early stopping
            if no_improvement_count >= config.early_stopping_patience:
                logger.info(f"Early stopping: {no_improvement_count} iteration Ø¨Ø¯ÙˆÙ† Ø¨Ù‡Ø¨ÙˆØ¯")
                break
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù…ÛŒØ§Ù†ÛŒ
            if config.save_intermediate_results and (iteration + 1) % 10 == 0:
                self._save_intermediate_results(results)
        
        logger.info(f"Random Search ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {best_result.score:.4f}")
        
        return best_result
    
    def bayesian_optimization(
        self,
        config: OptimizationConfig = None,
        param_subset: List[str] = None
    ) -> OptimizationResult:
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨ÛŒØ²ÛŒ (Bayesian Optimization) - Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡"""
        
        if config is None:
            config = OptimizationConfig()
        
        logger.info("Ø´Ø±ÙˆØ¹ Bayesian Optimization (Simplified)")
        
        # Ø´Ø±ÙˆØ¹ Ø¨Ø§ Ú†Ù†Ø¯ Ù†Ù‚Ø·Ù‡ ØªØµØ§Ø¯ÙÛŒ
        results = []
        
        # Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„: Random exploration
        for i in range(config.n_random_starts):
            parameters = self._sample_random_parameters(param_subset)
            result = self._evaluate_parameters(parameters)
            results.append(result)
            self.optimization_history.append(result)
        
        best_result = max(results, key=lambda r: r.score)
        self.best_result = best_result
        
        # Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…: Exploitation around best points
        for iteration in range(config.n_random_starts, config.max_iterations):
            # Ø§Ù†ØªØ®Ø§Ø¨ parameters Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ù‚Ø¨Ù„ÛŒ
            if len(results) >= 3:
                # Ø§Ù†ØªØ®Ø§Ø¨ 3 Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
                top_results = sorted(results, key=lambda r: r.score, reverse=True)[:3]
                
                # ØªÙˆÙ„ÛŒØ¯ parameters Ø¬Ø¯ÛŒØ¯ Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
                parameters = self._sample_around_best(top_results, param_subset)
            else:
                parameters = self._sample_random_parameters(param_subset)
            
            result = self._evaluate_parameters(parameters)
            results.append(result)
            self.optimization_history.append(result)
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
            if result.score > best_result.score:
                best_result = result
                self.best_result = result
                logger.info(f"ðŸŽ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¬Ø¯ÛŒØ¯: {result.score:.4f}")
            
            # Early stopping
            if self._should_stop_early(results, config):
                logger.info(f"Early stopping Ø¯Ø± iteration {iteration+1}")
                break
        
        logger.info(f"Bayesian Optimization ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {best_result.score:.4f}")
        
        return best_result
    
    def _sample_random_parameters(self, param_subset: List[str] = None) -> Dict[str, Any]:
        """Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² parameters"""
        
        import random
        
        parameters = {}
        
        for name, hp in self.hyperparameters.items():
            if param_subset is None or name in param_subset:
                if hp.possible_values:
                    parameters[name] = random.choice(hp.possible_values)
                else:
                    parameters[name] = hp.current_value
            else:
                parameters[name] = hp.current_value
        
        return parameters
    
    def _sample_around_best(
        self,
        top_results: List[OptimizationResult],
        param_subset: List[str] = None
    ) -> Dict[str, Any]:
        """Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø¯Ø± Ø§Ø·Ø±Ø§Ù Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬"""
        
        import random
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ÛŒÚ© Ù†ØªÛŒØ¬Ù‡ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
        base_result = random.choice(top_results)
        base_params = base_result.parameters
        
        new_params = {}
        
        for name, hp in self.hyperparameters.items():
            if param_subset is None or name in param_subset:
                if hp.possible_values:
                    current_value = base_params.get(name, hp.current_value)
                    
                    # ÛŒØ§ÙØªÙ† index Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ
                    try:
                        current_idx = hp.possible_values.index(current_value)
                        
                        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù‚Ø¯Ø§Ø± Ù†Ø²Ø¯ÛŒÚ© (Â±1 ÛŒØ§ Â±2 Ù…ÙˆÙ‚Ø¹ÛŒØª)
                        max_shift = min(2, len(hp.possible_values) // 4)
                        shift = random.randint(-max_shift, max_shift)
                        new_idx = max(0, min(len(hp.possible_values) - 1, current_idx + shift))
                        
                        new_params[name] = hp.possible_values[new_idx]
                    except ValueError:
                        # Ø§Ú¯Ø± Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ Ø¯Ø± Ù„ÛŒØ³Øª Ù†ÛŒØ³Øª
                        new_params[name] = random.choice(hp.possible_values)
                else:
                    new_params[name] = base_params.get(name, hp.current_value)
            else:
                new_params[name] = hp.current_value
        
        return new_params
    
    def _should_stop_early(
        self,
        results: List[OptimizationResult],
        config: OptimizationConfig
    ) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø· early stopping"""
        
        if len(results) < config.early_stopping_patience:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø± iterations Ø§Ø®ÛŒØ±
        recent_scores = [r.score for r in results[-config.early_stopping_patience:]]
        max_recent = max(recent_scores)
        
        # Ø§Ú¯Ø± Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ø¯Ø± iterations Ø§Ø®ÛŒØ± Ú©Ù… Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡
        if len(results) > config.early_stopping_patience:
            best_before = max(r.score for r in results[:-config.early_stopping_patience])
            improvement = max_recent - best_before
            
            if improvement < config.early_stopping_threshold:
                return True
        
        return False
    
    def _save_intermediate_results(self, results: List[OptimizationResult]) -> None:
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù…ÛŒØ§Ù†ÛŒ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = self.output_dir / f"optimization_intermediate_{timestamp}.json"
        
        data = {
            "timestamp": timestamp,
            "total_results": len(results),
            "best_score": max(r.score for r in results),
            "results": [asdict(r) for r in results[-10:]]  # ÙÙ‚Ø· 10 Ù†ØªÛŒØ¬Ù‡ Ø¢Ø®Ø±
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def save_optimization_results(self, filename: str = None) -> str:
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"optimization_results_{timestamp}.json"
        
        filepath = self.output_dir / filename
        
        data = {
            "optimization_summary": {
                "total_evaluations": len(self.optimization_history),
                "best_score": self.best_result.score if self.best_result else 0.0,
                "best_parameters": self.best_result.parameters if self.best_result else {},
                "start_time": datetime.now().isoformat()
            },
            "hyperparameters_definition": {
                name: {
                    "current_value": hp.current_value,
                    "possible_values": hp.possible_values,
                    "description": hp.description
                }
                for name, hp in self.hyperparameters.items()
            },
            "optimization_history": [asdict(r) for r in self.optimization_history],
            "best_result": asdict(self.best_result) if self.best_result else None
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")
        
        return str(filepath)
    
    def load_optimization_results(self, filepath: str) -> None:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‚Ø¨Ù„ÛŒ"""
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ history
        self.optimization_history = []
        for result_data in data.get('optimization_history', []):
            result = OptimizationResult(**result_data)
            self.optimization_history.append(result)
        
        # Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
        if data.get('best_result'):
            self.best_result = OptimizationResult(**data['best_result'])
        
        logger.info(f"Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯: {len(self.optimization_history)} Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        
        if not self.optimization_history:
            return {"message": "Ù‡ÛŒÚ† Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯Ù‡"}
        
        scores = [r.score for r in self.optimization_history]
        times = [r.evaluation_time for r in self.optimization_history]
        
        return {
            "total_evaluations": len(self.optimization_history),
            "best_score": max(scores),
            "worst_score": min(scores),
            "mean_score": statistics.mean(scores),
            "score_std": statistics.stdev(scores) if len(scores) > 1 else 0,
            "total_time": sum(times),
            "average_evaluation_time": statistics.mean(times),
            "best_parameters": self.best_result.parameters if self.best_result else {},
            "improvement": max(scores) - scores[0] if len(scores) > 1 else 0
        }
    
    def recommend_parameters(self) -> Dict[str, Any]:
        """ØªÙˆØµÛŒÙ‡ Ø¨Ù‡ØªØ±ÛŒÙ† parameters"""
        
        if not self.best_result:
            return {name: hp.current_value for name, hp in self.hyperparameters.items()}
        
        recommended = self.best_result.parameters.copy()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù†Ø¨ÙˆØ¯ Ø¯Ø± Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
        for name, hp in self.hyperparameters.items():
            if name not in recommended:
                recommended[name] = hp.current_value
        
        return recommended

# ØªØ³Øª
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("âš™ï¸  ØªØ³Øª Optimization Manager")
    print("=" * 50)
    
    # ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Mock
    def mock_evaluation_function(parameters: Dict[str, Any]) -> Dict[str, float]:
        """ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Mock Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
        import random
        import time
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø²Ù…Ø§Ù† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        time.sleep(0.1)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ parameters (mock)
        score_base = 0.7
        
        # chunk_size: Ø¨Ù‡ØªØ± Ø¯Ø± Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…ØªÙˆØ³Ø·
        if parameters.get('chunk_size', 500) in [400, 500, 600]:
            score_base += 0.1
        
        # temperature: Ø¨Ù‡ØªØ± Ø¯Ø± Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾Ø§ÛŒÛŒÙ†
        if parameters.get('temperature', 0.1) <= 0.2:
            score_base += 0.1
        
        # Ù†ÙˆÛŒØ² ØªØµØ§Ø¯ÙÛŒ
        noise = random.uniform(-0.1, 0.1)
        
        return {
            'retrieval_precision': min(1.0, score_base + noise),
            'retrieval_recall': min(1.0, score_base + noise * 0.8),
            'generation_rouge': min(1.0, score_base + noise * 1.2),
            'legal_accuracy': min(1.0, score_base + noise * 0.9),
            'citation_quality': min(1.0, score_base + noise * 1.1),
            'response_time': random.uniform(2, 8)
        }
    
    # Ø§ÛŒØ¬Ø§Ø¯ optimizer
    optimizer = LegalSystemOptimizer(mock_evaluation_function)
    
    # ØªØ³Øª Random Search Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù… iteration
    config = OptimizationConfig(max_iterations=10, early_stopping_patience=5)
    
    print("ðŸ” Ø´Ø±ÙˆØ¹ Random Search...")
    best_result = optimizer.random_search(
        config=config,
        param_subset=['chunk_size', 'temperature', 'top_k_retrieval']
    )
    
    print(f"\nðŸŽ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡:")
    print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø²: {best_result.score:.4f}")
    print(f"   â€¢ Parameters:")
    for key, value in best_result.parameters.items():
        print(f"     - {key}: {value}")
    
    # Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
    summary = optimizer.get_optimization_summary()
    print(f"\nðŸ“Š Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:")
    print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§: {summary['total_evaluations']}")
    print(f"   â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {summary['best_score']:.4f}")
    print(f"   â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {summary['mean_score']:.4f}")
    print(f"   â€¢ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„ÛŒ: {summary['improvement']:.4f}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    saved_path = optimizer.save_optimization_results()
    print(f"\nðŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {saved_path}")
    
    print("\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯")