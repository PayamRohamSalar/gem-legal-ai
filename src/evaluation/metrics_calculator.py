"""
src/evaluation/metrics_calculator.py - Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ø² 4

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ù…Ø³Ø¦ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÛŒØ³ØªÙ… Ø§Ø³Øª:
- Retrieval Metrics: Precision@K, Recall@K, MRR
- Generation Metrics: BLEU, ROUGE, BERTScore  
- Legal Accuracy: ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ
- Citation Quality: Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
"""

import re
import math
import logging
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass
from collections import Counter
import statistics

# Import Ø¨Ø±Ø§ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ NLP
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    ROUGE_AVAILABLE = False
    print("âš ï¸  rouge-score Ù†ØµØ¨ Ù†ÛŒØ³Øª. pip install rouge-score")

try:
    import sacrebleu
    BLEU_AVAILABLE = True
except ImportError:
    BLEU_AVAILABLE = False
    print("âš ï¸  sacrebleu Ù†ØµØ¨ Ù†ÛŒØ³Øª. pip install sacrebleu")

logger = logging.getLogger(__name__)

@dataclass
class RetrievalMetrics:
    """Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
    precision_at_1: float
    precision_at_3: float
    precision_at_5: float
    recall_at_1: float
    recall_at_3: float
    recall_at_5: float
    mrr: float  # Mean Reciprocal Rank
    map_score: float  # Mean Average Precision
    ndcg_at_5: float  # Normalized Discounted Cumulative Gain

@dataclass 
class GenerationMetrics:
    """Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†"""
    bleu_1: float
    bleu_2: float
    bleu_4: float
    rouge_1_f: float
    rouge_2_f: float
    rouge_l_f: float
    meteor_score: float
    bert_score_f1: float

@dataclass
class LegalAccuracyMetrics:
    """Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¯Ù‚Øª Ø­Ù‚ÙˆÙ‚ÛŒ"""
    citation_accuracy: float      # Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
    legal_term_coverage: float    # Ù¾ÙˆØ´Ø´ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ
    factual_consistency: float    # Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
    law_compliance: float         # Ø§Ù†Ø·Ø¨Ø§Ù‚ Ø¨Ø§ Ù‚ÙˆØ§Ù†ÛŒÙ†
    completeness_score: float     # Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ù¾Ø§Ø³Ø®

@dataclass
class CitationMetrics:
    """Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø§Ø±Ø¬Ø§Ø¹"""
    citation_count: int
    valid_citations: int
    citation_precision: float
    citation_recall: float
    citation_format_score: float
    source_diversity: float

@dataclass
class OverallMetrics:
    """Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ"""
    overall_score: float
    retrieval_metrics: RetrievalMetrics
    generation_metrics: GenerationMetrics  
    legal_accuracy: LegalAccuracyMetrics
    citation_quality: CitationMetrics

class LegalMetricsCalculator:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒÚ¯Ø± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ"""
    
    def __init__(self):
        self.legal_terms = self._load_legal_terms()
        self.citation_patterns = self._load_citation_patterns()
        
        # ØªÙ†Ø¸ÛŒÙ… ROUGE scorer
        if ROUGE_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(
                ['rouge1', 'rouge2', 'rougeL'], use_stemmer=False
            )
        
        logger.info("LegalMetricsCalculator Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def _load_legal_terms(self) -> Set[str]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ"""
        return {
            # Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
            "Ù…Ø§Ø¯Ù‡", "Ø¨Ù†Ø¯", "ØªØ¨ØµØ±Ù‡", "Ù‚Ø§Ù†ÙˆÙ†", "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡", "Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„",
            "Ù…ØµÙˆØ¨", "Ù…Ø¬Ù„Ø³", "Ø´ÙˆØ±Ø§ÛŒ Ø¹Ø§Ù„ÛŒ", "ÙˆØ²Ø§Ø±Øª", "Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ",
            "Ù¾Ú˜ÙˆÙ‡Ø´", "ØªØ­Ù‚ÛŒÙ‚", "ÙÙ†Ø§ÙˆØ±ÛŒ", "Ø¯Ø§Ù†Ø´â€ŒØ¨Ù†ÛŒØ§Ù†", "Ø§Ù†ØªÙ‚Ø§Ù„ ÙÙ†Ø§ÙˆØ±ÛŒ",
            
            # Ø¹Ø¨Ø§Ø±Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ
            "Ø¨Ø± Ø§Ø³Ø§Ø³", "Ù…Ø·Ø§Ø¨Ù‚", "Ø·Ø¨Ù‚", "ÙˆÙÙ‚", "Ø¯Ø± Ø±Ø§Ø³ØªØ§ÛŒ",
            "Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª", "Ù…Ø´Ø±ÙˆØ· Ø¨Ø±", "Ø¯Ø± ØµÙˆØ±Øª", "Ø§Ù„Ø²Ø§Ù…", "Ù…Ù…Ù†ÙˆØ¹",
            "Ù…Ø¬Ø§Ø²", "Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„", "ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„", "Ø¶Ø±ÙˆØ±ÛŒ", "Ø§Ø®ØªÛŒØ§Ø±ÛŒ",
            
            # Ù†Ù‡Ø§Ø¯Ù‡Ø§
            "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡", "Ù…Ø¤Ø³Ø³Ù‡", "Ù…Ø±Ú©Ø² ØªØ­Ù‚ÛŒÙ‚Ø§Øª", "Ø´Ø±Ú©Øª", "Ø³Ø§Ø²Ù…Ø§Ù†",
            "Ù…Ø¹Ø§ÙˆÙ†Øª", "Ø¯ÙØªØ±", "Ø³ØªØ§Ø¯", "Ú©Ù…ÛŒØ³ÛŒÙˆÙ†", "Ù‡ÛŒØ¦Øª"
        }
    
    def _load_citation_patterns(self) -> List[str]:
        """Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø±Ø¬Ø§Ø¹ Ù…Ø¹ØªØ¨Ø±"""
        return [
            r"Ù…Ø§Ø¯Ù‡\s+\d+",                    # Ù…Ø§Ø¯Ù‡ Û³
            r"Ø¨Ù†Ø¯\s+[Ø¢-ÛŒ]+",                  # Ø¨Ù†Ø¯ Ø§Ù„Ù
            r"ØªØ¨ØµØ±Ù‡\s+\d+",                   # ØªØ¨ØµØ±Ù‡ Û±
            r"Ù‚Ø§Ù†ÙˆÙ†\s+.+Ù…ØµÙˆØ¨\s+\d{4}",       # Ù‚Ø§Ù†ÙˆÙ† ... Ù…ØµÙˆØ¨ Û±Û³Û¹Û¸
            r"Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡\s+.+Ù…ØµÙˆØ¨\s+\d{4}",    # Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ ... Ù…ØµÙˆØ¨ Û±Û³Û¹Û¸
            r"Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„\s+.+",              # Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ ...
            r"Ø¨Ø± Ø§Ø³Ø§Ø³\s+Ù…Ø§Ø¯Ù‡\s+\d+",         # Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø§Ø¯Ù‡ Û³
            r"Ù…Ø·Ø§Ø¨Ù‚\s+.+",                   # Ù…Ø·Ø§Ø¨Ù‚ Ù‚Ø§Ù†ÙˆÙ†
            r"Ø·Ø¨Ù‚\s+.+"                      # Ø·Ø¨Ù‚ Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡
        ]
    
    # === Retrieval Metrics ===
    
    def calculate_precision_at_k(
        self, 
        retrieved_docs: List[Dict], 
        relevant_docs: List[str], 
        k: int
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Precision@K"""
        
        if not retrieved_docs or k <= 0:
            return 0.0
        
        top_k = retrieved_docs[:k]
        relevant_count = 0
        
        for doc in top_k:
            doc_id = doc.get('document_id') or doc.get('source', '')
            if any(rel_id in doc_id for rel_id in relevant_docs):
                relevant_count += 1
        
        return relevant_count / min(len(top_k), k)
    
    def calculate_recall_at_k(
        self, 
        retrieved_docs: List[Dict], 
        relevant_docs: List[str], 
        k: int
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Recall@K"""
        
        if not relevant_docs:
            return 0.0
        
        top_k = retrieved_docs[:k]
        found_relevant = 0
        
        for doc in top_k:
            doc_id = doc.get('document_id') or doc.get('source', '')
            if any(rel_id in doc_id for rel_id in relevant_docs):
                found_relevant += 1
        
        return found_relevant / len(relevant_docs)
    
    def calculate_mrr(
        self, 
        queries_results: List[Tuple[List[Dict], List[str]]]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Mean Reciprocal Rank"""
        
        reciprocal_ranks = []
        
        for retrieved_docs, relevant_docs in queries_results:
            rr = 0.0
            
            for rank, doc in enumerate(retrieved_docs, 1):
                doc_id = doc.get('document_id') or doc.get('source', '')
                if any(rel_id in doc_id for rel_id in relevant_docs):
                    rr = 1.0 / rank
                    break
            
            reciprocal_ranks.append(rr)
        
        return statistics.mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    def calculate_ndcg_at_k(
        self, 
        retrieved_docs: List[Dict], 
        relevant_docs: Dict[str, float], 
        k: int
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ NDCG@K"""
        
        if not retrieved_docs or not relevant_docs:
            return 0.0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ DCG
        dcg = 0.0
        for i, doc in enumerate(retrieved_docs[:k]):
            doc_id = doc.get('document_id') or doc.get('source', '')
            relevance = 0.0
            
            for rel_id, score in relevant_docs.items():
                if rel_id in doc_id:
                    relevance = score
                    break
            
            if i == 0:
                dcg += relevance
            else:
                dcg += relevance / math.log2(i + 1)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ IDCG (DCG Ø¨Ù‡ÛŒÙ†Ù‡)
        sorted_relevances = sorted(relevant_docs.values(), reverse=True)[:k]
        idcg = 0.0
        for i, relevance in enumerate(sorted_relevances):
            if i == 0:
                idcg += relevance
            else:
                idcg += relevance / math.log2(i + 1)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def calculate_retrieval_metrics(
        self, 
        retrieved_docs: List[Dict], 
        relevant_docs: List[str],
        relevance_scores: Dict[str, float] = None
    ) -> RetrievalMetrics:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ"""
        
        if relevance_scores is None:
            relevance_scores = {doc: 1.0 for doc in relevant_docs}
        
        return RetrievalMetrics(
            precision_at_1=self.calculate_precision_at_k(retrieved_docs, relevant_docs, 1),
            precision_at_3=self.calculate_precision_at_k(retrieved_docs, relevant_docs, 3),
            precision_at_5=self.calculate_precision_at_k(retrieved_docs, relevant_docs, 5),
            recall_at_1=self.calculate_recall_at_k(retrieved_docs, relevant_docs, 1),
            recall_at_3=self.calculate_recall_at_k(retrieved_docs, relevant_docs, 3),
            recall_at_5=self.calculate_recall_at_k(retrieved_docs, relevant_docs, 5),
            mrr=self.calculate_mrr([(retrieved_docs, relevant_docs)]),
            map_score=self._calculate_average_precision(retrieved_docs, relevant_docs),
            ndcg_at_5=self.calculate_ndcg_at_k(retrieved_docs, relevance_scores, 5)
        )
    
    def _calculate_average_precision(
        self, 
        retrieved_docs: List[Dict], 
        relevant_docs: List[str]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Average Precision"""
        
        if not relevant_docs:
            return 0.0
        
        precision_sum = 0.0
        relevant_found = 0
        
        for i, doc in enumerate(retrieved_docs):
            doc_id = doc.get('document_id') or doc.get('source', '')
            
            if any(rel_id in doc_id for rel_id in relevant_docs):
                relevant_found += 1
                precision_at_i = relevant_found / (i + 1)
                precision_sum += precision_at_i
        
        return precision_sum / len(relevant_docs)
    
    # === Generation Metrics ===
    
    def calculate_bleu_scores(self, generated: str, reference: str) -> Dict[str, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª BLEU"""
        
        if not BLEU_AVAILABLE:
            return {"bleu_1": 0.0, "bleu_2": 0.0, "bleu_4": 0.0}
        
        try:
            # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§
            gen_clean = self._clean_text_for_bleu(generated)
            ref_clean = self._clean_text_for_bleu(reference)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ BLEU
            bleu_1 = sacrebleu.sentence_bleu(gen_clean, [ref_clean], smooth_method='exp').score
            bleu_2 = sacrebleu.sentence_bleu(gen_clean, [ref_clean], smooth_method='exp').score  
            bleu_4 = sacrebleu.sentence_bleu(gen_clean, [ref_clean]).score
            
            return {
                "bleu_1": bleu_1 / 100.0,  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³ 0-1
                "bleu_2": bleu_2 / 100.0,
                "bleu_4": bleu_4 / 100.0
            }
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ BLEU: {e}")
            return {"bleu_1": 0.0, "bleu_2": 0.0, "bleu_4": 0.0}
    
    def calculate_rouge_scores(self, generated: str, reference: str) -> Dict[str, float]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª ROUGE"""
        
        if not ROUGE_AVAILABLE:
            return {"rouge_1_f": 0.0, "rouge_2_f": 0.0, "rouge_l_f": 0.0}
        
        try:
            scores = self.rouge_scorer.score(reference, generated)
            
            return {
                "rouge_1_f": scores['rouge1'].fmeasure,
                "rouge_2_f": scores['rouge2'].fmeasure,
                "rouge_l_f": scores['rougeL'].fmeasure
            }
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ ROUGE: {e}")
            return {"rouge_1_f": 0.0, "rouge_2_f": 0.0, "rouge_l_f": 0.0}
    
    def calculate_meteor_score(self, generated: str, reference: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ METEOR (ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)"""
        
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ METEOR Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        gen_words = set(generated.split())
        ref_words = set(reference.split())
        
        if not ref_words:
            return 0.0
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ precision Ùˆ recall
        matches = len(gen_words.intersection(ref_words))
        precision = matches / len(gen_words) if gen_words else 0.0
        recall = matches / len(ref_words)
        
        # F-measure with recall bias
        if precision + recall == 0:
            return 0.0
        
        f_mean = (10 * precision * recall) / (9 * precision + recall)
        
        return f_mean
    
    def _clean_text_for_bleu(self, text: str) -> str:
        """ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ BLEU"""
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        text = re.sub(r'[^\w\s]', '', text)
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
        words = text.split()
        return ' '.join(words)
    
    def calculate_generation_metrics(
        self, 
        generated: str, 
        reference: str
    ) -> GenerationMetrics:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯"""
        
        bleu_scores = self.calculate_bleu_scores(generated, reference)
        rouge_scores = self.calculate_rouge_scores(generated, reference)
        meteor = self.calculate_meteor_score(generated, reference)
        
        return GenerationMetrics(
            bleu_1=bleu_scores["bleu_1"],
            bleu_2=bleu_scores["bleu_2"], 
            bleu_4=bleu_scores["bleu_4"],
            rouge_1_f=rouge_scores["rouge_1_f"],
            rouge_2_f=rouge_scores["rouge_2_f"],
            rouge_l_f=rouge_scores["rouge_l_f"],
            meteor_score=meteor,
            bert_score_f1=0.0  # Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
        )
    
    # === Legal Accuracy Metrics ===
    
    def calculate_citation_accuracy(
        self, 
        generated_text: str, 
        expected_citations: List[str]
    ) -> float:
        """Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ø§Ø² Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
        found_citations = self._extract_citations(generated_text)
        
        if not expected_citations:
            return 1.0 if not found_citations else 0.5
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ·Ø§Ø¨Ù‚
        correct_citations = 0
        for expected in expected_citations:
            if any(expected in found for found in found_citations):
                correct_citations += 1
        
        return correct_citations / len(expected_citations)
    
    def calculate_legal_term_coverage(self, generated_text: str) -> float:
        """Ù¾ÙˆØ´Ø´ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ"""
        
        text_words = set(generated_text.lower().split())
        found_terms = len(text_words.intersection(self.legal_terms))
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ Ù…ØªÙ†
        text_length_factor = min(len(text_words) / 50, 1.0)  # Ø­Ø¯Ø§Ú©Ø«Ø± 50 Ú©Ù„Ù…Ù‡
        
        return (found_terms / 10) * text_length_factor  # Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ø§ØµØ·Ù„Ø§Ø­
    
    def calculate_factual_consistency(
        self, 
        generated_text: str, 
        source_contexts: List[str]
    ) -> float:
        """Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹"""
        
        if not source_contexts:
            return 0.5
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¯Ø¹Ø§Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø² Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
        generated_claims = self._extract_claims(generated_text)
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹
        consistent_claims = 0
        for claim in generated_claims:
            if any(self._is_claim_supported(claim, context) for context in source_contexts):
                consistent_claims += 1
        
        return consistent_claims / len(generated_claims) if generated_claims else 0.5
    
    def calculate_law_compliance(self, generated_text: str) -> float:
        """Ø§Ù†Ø·Ø¨Ø§Ù‚ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ"""
        
        compliance_score = 0.0
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø§Ø±Ø¬Ø§Ø¹ Ø¨Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÛŒ
        if re.search(r"(Ù…Ø§Ø¯Ù‡|Ø¨Ù†Ø¯|ØªØ¨ØµØ±Ù‡|Ù‚Ø§Ù†ÙˆÙ†|Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡)", generated_text):
            compliance_score += 0.3
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ
        legal_phrases = ["Ø¨Ø± Ø§Ø³Ø§Ø³", "Ù…Ø·Ø§Ø¨Ù‚", "Ø·Ø¨Ù‚", "ÙˆÙÙ‚"]
        if any(phrase in generated_text for phrase in legal_phrases):
            compliance_score += 0.3
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù…Ù†Ø·Ù‚ÛŒ Ù¾Ø§Ø³Ø®
        if "ğŸ“‹" in generated_text or "Ù…Ø±Ø¬Ø¹:" in generated_text:
            compliance_score += 0.2
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶ÙˆØ­ Ùˆ Ù†Ø¸Ù…
        sentences = generated_text.split('.')
        if len(sentences) >= 3:  # Ø­Ø¯Ø§Ù‚Ù„ 3 Ø¬Ù…Ù„Ù‡
            compliance_score += 0.2
        
        return min(compliance_score, 1.0)
    
    def calculate_completeness_score(
        self, 
        generated_text: str, 
        expected_elements: List[str]
    ) -> float:
        """Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ù¾Ø§Ø³Ø®"""
        
        if not expected_elements:
            return 0.8  # Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡
        
        found_elements = 0
        for element in expected_elements:
            if element.lower() in generated_text.lower():
                found_elements += 1
        
        base_score = found_elements / len(expected_elements)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨
        length_bonus = min(len(generated_text) / 500, 0.2)  # Ø­Ø¯Ø§Ú©Ø«Ø± 0.2 Ø§Ù…ØªÛŒØ§Ø²
        
        return min(base_score + length_bonus, 1.0)
    
    def calculate_legal_accuracy_metrics(
        self,
        generated_text: str,
        expected_citations: List[str],
        source_contexts: List[str],
        expected_elements: List[str]
    ) -> LegalAccuracyMetrics:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¯Ù‚Øª Ø­Ù‚ÙˆÙ‚ÛŒ"""
        
        return LegalAccuracyMetrics(
            citation_accuracy=self.calculate_citation_accuracy(generated_text, expected_citations),
            legal_term_coverage=self.calculate_legal_term_coverage(generated_text),
            factual_consistency=self.calculate_factual_consistency(generated_text, source_contexts),
            law_compliance=self.calculate_law_compliance(generated_text),
            completeness_score=self.calculate_completeness_score(generated_text, expected_elements)
        )
    
    # === Citation Quality Metrics ===
    
    def _extract_citations(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ø§Ø² Ù…ØªÙ†"""
        
        citations = []
        for pattern in self.citation_patterns:
            matches = re.findall(pattern, text)
            citations.extend(matches)
        
        return citations
    
    def _extract_claims(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø¯Ø¹Ø§Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø² Ù…ØªÙ†"""
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
        sentences = re.split(r'[.!?]', text)
        
        # ÙÛŒÙ„ØªØ± Ø¬Ù…Ù„Ø§Øª Ù…Ù‡Ù…
        claims = []
        for sentence in sentences:
            sentence = sentence.strip()
            if (len(sentence) > 20 and 
                any(keyword in sentence for keyword in ["Ø§Ø³Øª", "Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯", "Ø´Ø§Ù…Ù„", "Ø¹Ø¨Ø§Ø±Øª"])):
                claims.append(sentence)
        
        return claims
    
    def _is_claim_supported(self, claim: str, context: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø¯Ø¹Ø§ ØªÙˆØ³Ø· Ù…Ù†Ø¨Ø¹"""
        
        claim_words = set(claim.lower().split())
        context_words = set(context.lower().split())
        
        # Ø­Ø¯Ø§Ù‚Ù„ 60% ØªØ·Ø§Ø¨Ù‚ Ú©Ù„Ù…Ø§Øª
        overlap = len(claim_words.intersection(context_words))
        return overlap / len(claim_words) >= 0.6
    
    def calculate_citation_metrics(
        self,
        generated_text: str,
        expected_citations: List[str],
        available_sources: List[str]
    ) -> CitationMetrics:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø§Ø±Ø¬Ø§Ø¹"""
        
        found_citations = self._extract_citations(generated_text)
        
        # ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
        citation_count = len(found_citations)
        
        # Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ù…Ø¹ØªØ¨Ø±
        valid_citations = len([c for c in found_citations if any(source in c for source in available_sources)])
        
        # Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
        citation_precision = valid_citations / citation_count if citation_count > 0 else 0.0
        
        # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
        expected_found = len([c for c in expected_citations if any(c in f for f in found_citations)])
        citation_recall = expected_found / len(expected_citations) if expected_citations else 0.0
        
        # Ú©ÛŒÙÛŒØª ÙØ±Ù…Øª
        format_score = self._calculate_citation_format_score(found_citations)
        
        # ØªÙ†ÙˆØ¹ Ù…Ù†Ø§Ø¨Ø¹
        unique_sources = len(set([c.split()[0] for c in found_citations if c.split()]))
        source_diversity = min(unique_sources / 3, 1.0)  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ù…Ù†Ø¨Ø¹ Ù…ØªÙ†ÙˆØ¹
        
        return CitationMetrics(
            citation_count=citation_count,
            valid_citations=valid_citations,
            citation_precision=citation_precision,
            citation_recall=citation_recall,
            citation_format_score=format_score,
            source_diversity=source_diversity
        )
    
    def _calculate_citation_format_score(self, citations: List[str]) -> float:
        """Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª ÙØ±Ù…Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª"""
        
        if not citations:
            return 0.0
        
        format_scores = []
        for citation in citations:
            score = 0.0
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø´Ù…Ø§Ø±Ù‡ Ù…Ø§Ø¯Ù‡
            if re.search(r"Ù…Ø§Ø¯Ù‡\s+\d+", citation):
                score += 0.3
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø§Ù… Ù‚Ø§Ù†ÙˆÙ†
            if any(word in citation for word in ["Ù‚Ø§Ù†ÙˆÙ†", "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡", "Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„"]):
                score += 0.3
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³Ø§Ù„ Ù…ØµÙˆØ¨Ù‡
            if re.search(r"\d{4}", citation):
                score += 0.2
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø§Ø±Ø¬Ø§Ø¹
            if any(word in citation for word in ["Ø¨Ø± Ø§Ø³Ø§Ø³", "Ù…Ø·Ø§Ø¨Ù‚", "Ø·Ø¨Ù‚"]):
                score += 0.2
            
            format_scores.append(score)
        
        return statistics.mean(format_scores)
    
    # === Overall Metrics ===
    
    def calculate_overall_metrics(
        self,
        generated_text: str,
        reference_text: str,
        retrieved_docs: List[Dict],
        relevant_docs: List[str],
        expected_citations: List[str],
        source_contexts: List[str],
        expected_elements: List[str]
    ) -> OverallMetrics:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§"""
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø± Ø¯Ø³ØªÙ‡ Ø§Ø² Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
        retrieval_metrics = self.calculate_retrieval_metrics(retrieved_docs, relevant_docs)
        generation_metrics = self.calculate_generation_metrics(generated_text, reference_text)
        legal_accuracy = self.calculate_legal_accuracy_metrics(
            generated_text, expected_citations, source_contexts, expected_elements
        )
        citation_quality = self.calculate_citation_metrics(
            generated_text, expected_citations, [doc.get('source', '') for doc in retrieved_docs]
        )
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ (ÙˆØ²Ù†â€ŒØ¯Ø§Ø±)
        overall_score = (
            0.25 * retrieval_metrics.precision_at_5 +  # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
            0.25 * generation_metrics.rouge_l_f +      # ØªÙˆÙ„ÛŒØ¯
            0.30 * statistics.mean([                   # Ø¯Ù‚Øª Ø­Ù‚ÙˆÙ‚ÛŒ
                legal_accuracy.citation_accuracy,
                legal_accuracy.legal_term_coverage,
                legal_accuracy.factual_consistency,
                legal_accuracy.law_compliance,
                legal_accuracy.completeness_score
            ]) +
            0.20 * citation_quality.citation_precision  # Ú©ÛŒÙÛŒØª Ø§Ø±Ø¬Ø§Ø¹
        )
        
        return OverallMetrics(
            overall_score=overall_score,
            retrieval_metrics=retrieval_metrics,
            generation_metrics=generation_metrics,
            legal_accuracy=legal_accuracy,
            citation_quality=citation_quality
        )

# ØªØ³Øª
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§® ØªØ³Øª Metrics Calculator")
    print("=" * 50)
    
    calculator = LegalMetricsCalculator()
    
    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
    generated = """Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø§Ø¯Ù‡ 3 Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ù…ØµÙˆØ¨ 1364:
    ÙˆØ¸Ø§ÛŒÙ Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ø´Ø§Ù…Ù„ Ù¾Ú˜ÙˆÙ‡Ø´ØŒ ØªØ¯Ø±ÛŒØ³ Ùˆ Ø®Ø¯Ù…Ø§Øª Ø§Ø³Øª.
    ğŸ“‹ Ù…Ø±Ø¬Ø¹: Ù…Ø§Ø¯Ù‡ 3 Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ"""
    
    reference = """Ø·Ø¨Ù‚ Ù…Ø§Ø¯Ù‡ 3 Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ:
    Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ù…Ú©Ù„Ù Ø¨Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù¾Ú˜ÙˆÙ‡Ø´ØŒ ØªØ¯Ø±ÛŒØ³ Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ø®Ø¯Ù…Ø§Øª Ù‡Ø³ØªÙ†Ø¯."""
    
    retrieved_docs = [
        {"source": "Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ", "content": "Ù…ØªÙ† Ù‚Ø§Ù†ÙˆÙ†"},
        {"source": "Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø§Ø±ØªÙ‚Ø§", "content": "Ù…ØªÙ† Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡"}
    ]
    
    relevant_docs = ["Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ"]
    expected_citations = ["Ù…Ø§Ø¯Ù‡ 3", "Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ"]
    
    # ØªØ³Øª ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
    overall = calculator.calculate_overall_metrics(
        generated_text=generated,
        reference_text=reference,
        retrieved_docs=retrieved_docs,
        relevant_docs=relevant_docs,
        expected_citations=expected_citations,
        source_contexts=[reference],
        expected_elements=["Ù¾Ú˜ÙˆÙ‡Ø´", "ØªØ¯Ø±ÛŒØ³", "Ø®Ø¯Ù…Ø§Øª"]
    )
    
    print(f"ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
    print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {overall.overall_score:.3f}")
    print(f"   â€¢ Precision@5: {overall.retrieval_metrics.precision_at_5:.3f}")
    print(f"   â€¢ ROUGE-L: {overall.generation_metrics.rouge_l_f:.3f}")
    print(f"   â€¢ Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª: {overall.legal_accuracy.citation_accuracy:.3f}")
    print(f"   â€¢ Ø§Ù†Ø·Ø¨Ø§Ù‚ Ø­Ù‚ÙˆÙ‚ÛŒ: {overall.legal_accuracy.law_compliance:.3f}")
    
    print("\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯")