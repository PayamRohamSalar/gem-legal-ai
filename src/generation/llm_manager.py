# src/generation/llm_manager.py

import ollama
from typing import Dict, Any, Optional
import logging
import time
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# ğŸ’¡ **Ø¨Ø®Ø´ Ø¬Ø¯ÛŒØ¯ Û±:** ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯
@dataclass
class GenerationMetrics:
    """
    Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® ØªÙˆØ³Ø· LLM.
    """
    input_tokens: int = 0
    output_tokens: int = 0
    generation_time: float = 0.0

@dataclass
class LLMConfig:
    """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ LLM"""
    model_name: str

class LLMManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ù†Ø¯Ù‡ Ø§ØµÙ„ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ LLM Ù…ØªØµÙ„ Ø¨Ù‡ Ollama (Ù†Ø³Ø®Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø± Ùˆ Ú©Ø§Ù…Ù„)"""

    def __init__(self, configs: Dict[str, LLMConfig]):
        self.configs = configs
        self.active_model_key: Optional[str] = None
        self.client = None

        try:
            self.client = ollama.Client(host='http://localhost:11434')
            self.client.list()
            logger.info("LLMManager Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø³Ø±ÙˆØ± Ollama Ù…ØªØµÙ„ Ø´Ø¯.")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ Ollama: {e}")
            logger.error("Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ø±ÙˆØ´Ù† Ø¨ÙˆØ¯Ù† Ùˆ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ø³Ø±ÙˆØ± Ollama Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")

    def set_active_model(self, model_key: str) -> bool:
        """ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®"""
        if model_key not in self.configs:
            logger.error(f"ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ {model_key} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return False
        self.active_model_key = model_key
        logger.info(f"Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯: {self.configs[self.active_model_key].model_name}")
        return True

    def generate_response(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§.
        """
        if not self.active_model_key:
            return {'success': False, 'error': 'Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ ÙØ¹Ø§Ù„ Ù†ÛŒØ³Øª', 'response': '', 'metrics': None}

        if not self.client:
            return {'success': False, 'error': 'Ú©Ù„Ø§ÛŒÙ†Øª Ollama Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.', 'response': '', 'metrics': None}

        config = self.configs[self.active_model_key]
        model_name = config.model_name
        
        start_time = time.time()
        try:
            response = self.client.chat(
                model=model_name,
                messages=[{'role': 'user', 'content': prompt}]
            )
            
            # ğŸ’¡ **Ø¨Ø®Ø´ Ø¬Ø¯ÛŒØ¯ Û²:** Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ùˆ ØªØºÛŒÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ
            generation_time = time.time() - start_time
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø§Ø² Ù¾Ø§Ø³Ø® Ollama
            metrics = GenerationMetrics(
                input_tokens=response.get('prompt_eval_count', 0),
                output_tokens=response.get('eval_count', 0),
                generation_time=round(generation_time, 2)
            )

            return {
                'success': True,
                'response': response['message']['content'],
                'model_used': model_name,
                'metrics': metrics  # <--- Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
            }
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ollama: {e}")
            error_msg = str(e)
            if "model" in str(e) and "not found" in str(e):
                error_msg = f"Ù…Ø¯Ù„ '{model_name}' Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± Ollama ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ Ø¯Ø³ØªÙˆØ± `ollama pull {model_name}` Ø¢Ù† Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯."
            
            return {'success': False, 'error': error_msg, 'response': '', 'metrics': None}

def create_model_configs() -> Dict[str, LLMConfig]:
    """Ø§ÛŒØ¬Ø§Ø¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ollama"""
    return {
        'qwen2.5:7b': LLMConfig(model_name="qwen2.5:7b"), # Ù†Ø§Ù… Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ú¯Ø°Ø§Ø±
        'deepseek': LLMConfig(model_name="deepseek-r1:7b"),
        'mistral': LLMConfig(model_name="mistral:latest")
    }