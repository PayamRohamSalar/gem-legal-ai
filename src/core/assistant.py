"""
src/generation/integrated_response_system.py - Ø³ÛŒØ³ØªÙ… ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§ÛŒ ÙØ§Ø² 3 Ø±Ø§ Ø§Ø¯ØºØ§Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ API Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import logging
import time
from datetime import datetime
import json

# Import Ú©Ø±Ø¯Ù† Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ ÙØ§Ø² 3
from src.generation.llm_manager import LLMManager, LLMConfig, GenerationMetrics, create_model_configs
from src.generation.prompt_engine import PromptEngine, QueryType, ContextInfo, ContextType
from src.generation.citation_engine import CitationEngine, Citation

from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

@dataclass
class ResponseRequest:
    """Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®"""
    question: str
    contexts: List[Dict[str, Any]]
    query_type: Optional[QueryType] = None
    model_preference: Optional[str] = None
    format_style: str = 'standard'
    include_citations: bool = True
    max_response_length: int = 2048
    temperature: float = 0.1
    additional_instructions: Optional[str] = None

@dataclass
class ResponseResult:
    """Ù†ØªÛŒØ¬Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®"""
    # Ù¾Ø§Ø³Ø® Ø§ØµÙ„ÛŒ
    response: str
    enhanced_response: str
    
    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´
    query_type: QueryType
    processing_time: float
    success: bool
    
    # Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ùˆ Ù…Ù†Ø§Ø¨Ø¹
    citations: List[Citation]
    references_list: str
    citation_validation: Dict[str, Any]
    
    # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯
    generation_metrics: Optional[GenerationMetrics]
    
    # Ú©ÛŒÙÛŒØª Ù¾Ø§Ø³Ø®
    quality_score: float
    confidence_score: float
    
    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ
    model_used: str
    prompt_used: str
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None

class LegalResponseSystem:
    """Ø³ÛŒØ³ØªÙ… Ø¬Ø§Ù…Ø¹ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø­Ù‚ÙˆÙ‚ÛŒ"""
    
    def __init__(
        self,
        llm_configs: Optional[Dict[str, LLMConfig]] = None,
        default_model: str = 'qwen2.5:7b', # ØªØºÛŒÛŒØ± Ø¨Ù‡ Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ù†ØµØ¨ Ø¯Ø§Ø±ÛŒ
        enable_caching: bool = True
    ):
        # Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ…
        self.llm_manager = LLMManager(llm_configs or create_model_configs())
        self.prompt_engine = PromptEngine()
        self.citation_engine = CitationEngine()
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        self.default_model = default_model
        self.enable_caching = enable_caching
        self.response_cache: Dict[str, ResponseResult] = {}
        
        # Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_response_time': 0.0,
            'cache_hits': 0
        }
        
        logger.info("Ø³ÛŒØ³ØªÙ… ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø­Ù‚ÙˆÙ‚ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")

    async def initialize(self) -> bool:
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM Manager ÙˆØ§Ù‚Ø¹ÛŒ"""
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM Manager ÙˆØ§Ù‚Ø¹ÛŒ
            if not self.llm_manager.client:
                logger.error("Ú©Ù„Ø§ÛŒÙ†Øª Ollama Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ø±ÙˆØ´Ù† Ø¨ÙˆØ¯Ù† Ø³Ø±ÙˆØ± Ollama Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
                return False

            self.llm_manager.set_active_model(self.default_model)
            logger.info(f"Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ '{self.default_model}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
            return True

        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
            return False

    def _generate_cache_key(self, request: ResponseRequest) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ú©Ø´"""
        key_data = {
            'question': request.question,
            'contexts_hash': hash(str(sorted([str(ctx) for ctx in request.contexts]))),
            'query_type': request.query_type.value if request.query_type else None,
            'model': request.model_preference or self.default_model,
            'temperature': request.temperature
        }
        return str(hash(str(key_data)))
    
    def _prepare_contexts(self, contexts: List[Dict[str, Any]]) -> List[ContextInfo]:
        """ØªØ¨Ø¯ÛŒÙ„ context Ù‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ ÙØ±Ù…Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯"""
        context_infos = []
        
        for ctx in contexts:
            try:
                # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³Ù†Ø¯
                doc_type_str = ctx.get('document_type', 'Ù‚Ø§Ù†ÙˆÙ†')
                if doc_type_str == 'Ù‚Ø§Ù†ÙˆÙ†':
                    doc_type = ContextType.LAW
                elif doc_type_str == 'Ø¢ÛŒÛŒÙ†_Ù†Ø§Ù…Ù‡':
                    doc_type = ContextType.REGULATION
                elif doc_type_str == 'Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„':
                    doc_type = ContextType.GUIDELINE
                else:
                    doc_type = ContextType.LAW  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
                
                context_info = ContextInfo(
                    content=ctx.get('content', ''),
                    source=ctx.get('source', ''),
                    document_type=doc_type,
                    article_number=ctx.get('article_number'),
                    relevance_score=ctx.get('relevance_score', 0.8)
                )
                context_infos.append(context_info)
                
            except Exception as e:
                logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ context: {e}")
                continue
        
        return context_infos
    
    def _calculate_quality_score(
        self, 
        response: str, 
        citations: List[Citation],
        generation_metrics: Optional[GenerationMetrics]
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª Ù¾Ø§Ø³Ø®"""
        
        score = 0.0
        max_score = 100.0
        
        # Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù¾Ø§Ø³Ø® (20 Ø§Ù…ØªÛŒØ§Ø²)
        response_length = len(response)
        if 200 <= response_length <= 2000:
            score += 20
        elif 100 <= response_length < 200 or 2000 < response_length <= 3000:
            score += 10
        
        # ÙˆØ¬ÙˆØ¯ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª (25 Ø§Ù…ØªÛŒØ§Ø²)
        if citations:
            score += 20
            # Ú©ÛŒÙÛŒØª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª (5 Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¶Ø§ÙÛŒ)
            avg_confidence = sum(c.confidence_score for c in citations) / len(citations)
            score += 5 * avg_confidence
        
        # Ø³Ø§Ø®ØªØ§Ø± Ù…Ù†Ø§Ø³Ø¨ Ù¾Ø§Ø³Ø® (20 Ø§Ù…ØªÛŒØ§Ø²)
        if any(keyword in response for keyword in ['Ø¨Ø± Ø§Ø³Ø§Ø³', 'Ù…Ø·Ø§Ø¨Ù‚', 'Ø·Ø¨Ù‚']):
            score += 10
        if any(keyword in response for keyword in ['Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡']):
            score += 10
        
        # ÙˆØ¶ÙˆØ­ Ùˆ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ (15 Ø§Ù…ØªÛŒØ§Ø²)
        sentences = response.split('.')
        if len(sentences) >= 3:  # Ø­Ø¯Ø§Ù‚Ù„ 3 Ø¬Ù…Ù„Ù‡
            score += 7
        if len(response.split()) >= 50:  # Ø­Ø¯Ø§Ù‚Ù„ 50 Ú©Ù„Ù…Ù‡
            score += 8
        
        # Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÙ„ÛŒØ¯ (20 Ø§Ù…ØªÛŒØ§Ø²)
        if generation_metrics:
            # Ø²Ù…Ø§Ù† Ù…Ù†Ø§Ø³Ø¨ (10 Ø§Ù…ØªÛŒØ§Ø²)
            if generation_metrics.generation_time < 10:
                score += 10
            elif generation_metrics.generation_time < 20:
                score += 5
            
            # ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø³Ø¨ ØªÙˆÚ©Ù† (10 Ø§Ù…ØªÛŒØ§Ø²)
            if generation_metrics.output_tokens >= 50:
                score += 10
            elif generation_metrics.output_tokens >= 20:
                score += 5
        
        return min(score, max_score)
    
    def _calculate_confidence_score(
        self, 
        response: str, 
        citations: List[Citation],
        context_count: int
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØµØ­Øª Ù¾Ø§Ø³Ø®"""
        
        confidence = 0.0
        
        # ÙˆØ¬ÙˆØ¯ Ù…Ù†Ø§Ø¨Ø¹ Ú©Ø§ÙÛŒ
        if context_count >= 3:
            confidence += 0.3
        elif context_count >= 1:
            confidence += 0.2
        
        # Ú©ÛŒÙÛŒØª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
        if citations:
            avg_citation_confidence = sum(c.confidence_score for c in citations) / len(citations)
            confidence += 0.4 * avg_citation_confidence
        
        # ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        legal_patterns = ['Ø¨Ø± Ø§Ø³Ø§Ø³', 'Ù…Ø·Ø§Ø¨Ù‚', 'Ø·Ø¨Ù‚', 'ÙˆÙÙ‚', 'Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'Ù‚Ø§Ù†ÙˆÙ†']
        pattern_matches = sum(1 for pattern in legal_patterns if pattern in response)
        confidence += 0.3 * min(pattern_matches / len(legal_patterns), 1.0)
        
        return min(confidence, 1.0)
    
    async def generate_response(self, request: ResponseRequest) -> ResponseResult:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ Ø¨Ø§ ØªÙ…Ø§Ù… Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§"""
        
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´
            if self.enable_caching:
                cache_key = self._generate_cache_key(request)
                if cache_key in self.response_cache:
                    self.stats['cache_hits'] += 1
                    logger.info("Ù¾Ø§Ø³Ø® Ø§Ø² Ú©Ø´ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯")
                    return self.response_cache[cache_key]
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ context Ù‡Ø§
            context_infos = self._prepare_contexts(request.contexts)
            
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³ÙˆØ§Ù„ Ø§Ú¯Ø± Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡
            query_type = request.query_type or self.prompt_engine.detect_query_type(request.question)
            
            # Ø³Ø§Ø®Øª prompt
            additional_fields = {}
            if request.additional_instructions:
                additional_fields['additional_instructions'] = request.additional_instructions
            
            prompt, detected_query_type = self.prompt_engine.build_prompt(
                request.question,
                context_infos,
                query_type,
                **additional_fields
            )
            
            # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
            generation_result = self.llm_manager.generate_response(
                prompt,
                temperature=request.temperature,
                max_new_tokens=request.max_response_length
            )
            
            if not generation_result['success']:
                raise Exception(generation_result.get('error', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯'))
            
            response_text = generation_result['response']
            generation_metrics = generation_result['metrics']
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª
            citations = []
            enhanced_response = response_text
            references_list = ""
            citation_validation = {}
            
            if request.include_citations:
                citation_result = self.citation_engine.enhance_response_with_citations(
                    response_text, 
                    request.contexts
                )
                
                enhanced_response = citation_result['enhanced_response']
                citations = citation_result['citations']
                references_list = citation_result['references_list']
                citation_validation = citation_result['validation']
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ú©ÛŒÙÛŒØª
            quality_score = self._calculate_quality_score(
                response_text, 
                citations, 
                generation_metrics
            )
            
            confidence_score = self._calculate_confidence_score(
                response_text, 
                citations, 
                len(request.contexts)
            )
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
            result = ResponseResult(
                response=response_text,
                enhanced_response=enhanced_response,
                query_type=detected_query_type,
                processing_time=time.time() - start_time,
                success=True,
                citations=citations,
                references_list=references_list,
                citation_validation=citation_validation,
                generation_metrics=generation_metrics,
                quality_score=quality_score,
                confidence_score=confidence_score,
                model_used=request.model_preference or self.default_model,
                prompt_used=prompt,
                metadata={
                    'context_count': len(request.contexts),
                    'prompt_length': len(prompt),
                    'timestamp': datetime.now().isoformat()
                }
            )
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
            if self.enable_caching:
                self.response_cache[cache_key] = result
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
            self.stats['successful_requests'] += 1
            self._update_stats(result.processing_time)
            
            logger.info(f"Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ - Ø²Ù…Ø§Ù†: {result.processing_time:.2f}s")
            return result
            
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {e}")
            
            return ResponseResult(
                response="",
                enhanced_response="",
                query_type=QueryType.GENERAL_INQUIRY,
                processing_time=time.time() - start_time,
                success=False,
                citations=[],
                references_list="",
                citation_validation={},
                generation_metrics=None,
                quality_score=0.0,
                confidence_score=0.0,
                model_used=self.default_model,
                prompt_used="",
                error_message=str(e)
            )
    
    def _update_stats(self, processing_time: float) -> None:
        """Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        total_successful = self.stats['successful_requests']
        if total_successful > 0:
            current_avg = self.stats['average_response_time']
            self.stats['average_response_time'] = (
                (current_avg * (total_successful - 1) + processing_time) / total_successful
            )
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…"""
        return {
            **self.stats,
            'llm_info': self.llm_manager.get_model_info(),
            'cache_size': len(self.response_cache),
            'prompt_templates': len(self.prompt_engine.templates),
            'uptime': datetime.now().isoformat()
        }
    
    def clear_cache(self) -> None:
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø´"""
        self.response_cache.clear()
        logger.info("Ú©Ø´ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ú© Ø´Ø¯")
    
    def shutdown(self) -> None:
        """Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ø³ÛŒØ³ØªÙ…"""
        logger.info("Ø´Ø±ÙˆØ¹ Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ø³ÛŒØ³ØªÙ…...")
        
        # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…Ù†Ø§Ø¨Ø¹
        self.llm_manager.cleanup()
        self.clear_cache()
        
        logger.info("Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®Ø§Ù…ÙˆØ´ Ø´Ø¯")

class SimpleLegalAssistant:
    """Ø±Ø§Ø¨Ø· Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ÛŒØ³ØªÙ…"""
    
    def __init__(self):
        # Ø³Ø§Ø®Øª ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„ÛŒ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        # Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø±Ø§ Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ØµØ¨ Ø´Ø¯Ù‡ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        self.system = LegalResponseSystem(default_model='qwen2.5:7b')
        self.initialized = False
    
    async def setup(self) -> bool:
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø§ØµÙ„ÛŒ."""
        self.initialized = await self.system.initialize()
        return self.initialized

    async def ask(self, request: ResponseRequest) -> ResponseResult:
        """
        Ø§Ø±Ø³Ø§Ù„ ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ù†ØªÛŒØ¬Ù‡ Ú©Ø§Ù…Ù„.
        Ø§ÛŒÙ† Ù…ØªØ¯ Ø¯ÛŒÚ¯Ø± ÙÙ‚Ø· ÛŒÚ© Ø±Ø´ØªÙ‡ Ø¨Ø±Ù†Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        """
        if not self.initialized:
            raise RuntimeError("Ø³ÛŒØ³ØªÙ… Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª")

        return await self.system.generate_response(request)
    
    def get_stats(self) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…."""
        return self.system.get_system_stats()

    def clear_cache(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø´."""
        self.system.clear_cache()

    def shutdown(self) -> None:
        """Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ø³ÛŒØ³ØªÙ…."""
        self.system.shutdown()

# ØªØ³Øª
if __name__ == "__main__":
    import asyncio
    logging.basicConfig(level=logging.INFO)
    
    async def test_system():
        """ØªØ³Øª Ø³ÛŒØ³ØªÙ… ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡"""
        
        print("ğŸš€ ØªØ³Øª Ø³ÛŒØ³ØªÙ… ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®")
        print("=" * 50)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø³ØªÛŒØ§Ø± Ø³Ø§Ø¯Ù‡
        assistant = SimpleLegalAssistant()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
        success = await assistant.setup()
        if not success:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ")
            return
        
        print("âœ… Ø³ÛŒØ³ØªÙ… Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        
        # Ù†Ù…ÙˆÙ†Ù‡ context Ù‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        contexts = [
            {
                'content': 'Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ù…ÙˆØ¸Ù Ø¨Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ ØªØ­Ù‚ÛŒÙ‚ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ ØªØ®ØµØµÛŒ Ø®ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯',
                'source': 'Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù†ØªØ¸Ø§Ù…ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ - Ù…ØµÙˆØ¨ 1364',
                'document_type': 'Ù‚Ø§Ù†ÙˆÙ†',
                'article_number': '3',
                'relevance_score': 0.95
            },
            {
                'content': 'Ù¾Ú˜ÙˆÙ‡Ø´ Ø¨Ø§ÛŒØ¯ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ù†Ø´ Ù†ÙˆÛŒÙ† Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ø§Ø´Ø¯',
                'source': 'Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø§Ø±ØªÙ‚Ø§ÛŒ Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ - Ù…ØµÙˆØ¨ 1398',
                'document_type': 'Ø¢ÛŒÛŒÙ†_Ù†Ø§Ù…Ù‡',
                'article_number': '7',
                'relevance_score': 0.88
            }
        ]
        
        # ØªØ³Øª Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø®ØªÙ„Ù
        test_questions = [
            "ÙˆØ¸Ø§ÛŒÙ Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´ Ú†ÛŒØ³ØªØŸ",
            "Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ú˜ÙˆÙ‡Ø´ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ",
            "Ù„Ø·ÙØ§Ù‹ Ø§ÛŒÙ† Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ”¸ ØªØ³Øª {i}: {question}")
            
            start_time = time.time()
            response = await assistant.ask(question, contexts)
            end_time = time.time()
            
            print(f"ğŸ“ Ù¾Ø§Ø³Ø®: {response[:200]}...")
            print(f"â±ï¸  Ø²Ù…Ø§Ù†: {end_time - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
            print("-" * 40)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
        stats = assistant.get_stats()
        print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ø³ÛŒØ³ØªÙ…:")
        print(f"  - Ú©Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {stats['total_requests']}")
        print(f"  - Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚: {stats['successful_requests']}")
        print(f"  - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®: {stats['average_response_time']:.2f}s")
        print(f"  - Cache hits: {stats['cache_hits']}")
        print(f"  - Ù…Ø¯Ù„ ÙØ¹Ø§Ù„: {stats['llm_info']['active_model']}")
        
        # Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù†
        assistant.shutdown()
        print("\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯")
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª
    asyncio.run(test_system())