# src/data_processing/document_extractor_fixed.py

import os
import json
import re
from datetime import datetime
from typing import Dict, List, Tuple
from docx import Document
from docx.shared import Inches
import pandas as pd

class LegalDocumentExtractor:
    """
    Ú©Ù„Ø§Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ ÙˆØ±Ø¯ - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
    """
    
    def __init__(self, input_dir: str = "data/raw_documents", output_dir: str = "data/processed_chunks"):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.metadata_dir = "data/metadata"
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
        
        # Ù¾ØªØ±Ù†â€ŒÙ‡Ø§ÛŒ regex Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ
        self.patterns = {
            'article': re.compile(r'Ù…Ø§Ø¯Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]+', re.UNICODE),
            'clause': re.compile(r'Ø¨Ù†Ø¯\s*([Ø§Ù„Ù-ÛŒ\d]+)', re.UNICODE),
            'note': re.compile(r'ØªØ¨ØµØ±Ù‡\s*(\d*)', re.UNICODE),
            'chapter': re.compile(r'ÙØµÙ„\s*(\d+|Ø§ÙˆÙ„|Ø¯ÙˆÙ…|Ø³ÙˆÙ…|Ú†Ù‡Ø§Ø±Ù…|Ù¾Ù†Ø¬Ù…)', re.UNICODE),
            'law_title': re.compile(r'Ù‚Ø§Ù†ÙˆÙ†\s+(.+?)(?:\n|$)', re.UNICODE),
            'approval_date': re.compile(r'Ù…ØµÙˆØ¨\s*(\d{1,2}/\d{1,2}/\d{4})', re.UNICODE),
            'authority': re.compile(r'(Ù…Ø¬Ù„Ø³ Ø´ÙˆØ±Ø§ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ|Ù‡ÛŒØ¦Øª ÙˆØ²ÛŒØ±Ø§Ù†|Ø´ÙˆØ±Ø§ÛŒ Ø¹Ø§Ù„ÛŒ)', re.UNICODE)
        }
    
    def extract_text_from_docx(self, file_path: str) -> Tuple[str, Dict]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ùˆ metadata Ø§Ø² ÙØ§ÛŒÙ„ ÙˆØ±Ø¯ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        """
        try:
            doc = Document(file_path)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² paragraphs Ø¨Ø§ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø±
            paragraphs = []
            text_lines = []
            
            print(f"ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² {len(doc.paragraphs)} Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù...")
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ paragraphs
            for i, para in enumerate(doc.paragraphs):
                para_text = para.text.strip()
                if para_text:
                    # Ø­ÙØ¸ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ§ÛŒÙ„
                    para_info = {
                        'text': para_text,
                        'style': para.style.name if para.style else 'Normal',
                        'is_heading': self._is_heading_style(para.style.name if para.style else ''),
                        'line_number': i
                    }
                    paragraphs.append(para_info)
                    
                    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øª Ù…ØªÙ† Ø¨Ø§ line break
                    text_lines.append(para_text)
                    
                    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø· Ø®Ø§Ù„ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ø¹Ù†Ø§ÙˆÛŒÙ† Ù…Ù‡Ù…
                    if self._is_important_heading(para_text):
                        text_lines.append("")  # Ø®Ø· Ø®Ø§Ù„ÛŒ
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø¯Ø§ÙˆÙ„
            tables_text = []
            for table in doc.tables:
                table_text = self._extract_table_text(table)
                if table_text:
                    tables_text.append(table_text)
                    text_lines.extend(table_text.split('\n'))
            
            # ØªØ±Ú©ÛŒØ¨ Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø­ÙØ¸ line break Ù‡Ø§
            full_text = '\n'.join(text_lines)
            
            print(f"ğŸ“ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(text_lines)} Ø®Ø·ØŒ {len(full_text.split())} Ú©Ù„Ù…Ù‡")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ metadata Ø§ÙˆÙ„ÛŒÙ‡
            metadata = self._extract_metadata(full_text, file_path)
            
            return full_text, {
                'metadata': metadata,
                'paragraphs': paragraphs,
                'tables': tables_text,
                'total_lines': len(text_lines),
                'extraction_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„ {file_path}: {str(e)}")
            return "", {}
    
    def _is_important_heading(self, text: str) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø¹Ù†Ø§ÙˆÛŒÙ† Ù…Ù‡Ù… Ú©Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø®Ø· Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯"""
        important_patterns = [
            r'ÙØµÙ„\s*[\d\u06F0-\u06F9]+',
            r'Ø¨Ø®Ø´\s*[\d\u06F0-\u06F9]+',
            r'Ù…Ø§Ø¯Ù‡\s*[\d\u06F0-\u06F9]+',
            r'Ù‚Ø§Ù†ÙˆÙ†\s+',
            r'Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡\s+',
            r'Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„\s+'
        ]
        
        for pattern in important_patterns:
            if re.search(pattern, text, re.UNICODE):
                return True
        return False
    
    def _extract_table_text(self, table) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² Ø¬Ø¯Ø§ÙˆÙ„ Ø¨Ø§ ÙØ±Ù…Øª Ø¨Ù‡ØªØ±"""
        table_lines = []
        for row in table.rows:
            row_cells = []
            for cell in row.cells:
                cell_text = cell.text.strip()
                if cell_text:
                    row_cells.append(cell_text)
            if row_cells:
                table_lines.append(' | '.join(row_cells))
        return '\n'.join(table_lines)
    
    def _is_heading_style(self, style_name: str) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø³Ø¨Ú© Ø¹Ù†ÙˆØ§Ù†"""
        heading_styles = ['Heading', 'Title', 'Subtitle', 'Ø¹Ù†ÙˆØ§Ù†', 'heading']
        return any(style.lower() in style_name.lower() for style in heading_styles)
    
    def _extract_metadata(self, text: str, file_path: str) -> Dict:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ metadata Ø§Ø² Ù…ØªÙ† Ø³Ù†Ø¯ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        """
        metadata = {
            'file_name': os.path.basename(file_path),
            'file_path': file_path,
            'document_type': self._detect_document_type(text),
            'title': self._extract_title(text),
            'approval_date': self._extract_approval_date(text),
            'authority': self._extract_authority(text),
            'articles_count': len(self.patterns['article'].findall(text)),
            'chapters_count': len(self.patterns['chapter'].findall(text)),
            'notes_count': len(self.patterns['note'].findall(text)),
            'word_count': len(text.split()),
            'char_count': len(text),
            'line_count': len(text.split('\n')),
            'language': 'persian'
        }
        
        print(f"ğŸ“Š Metadata: {metadata['articles_count']} Ù…Ø§Ø¯Ù‡ØŒ {metadata['line_count']} Ø®Ø·")
        
        return metadata
    
    def _detect_document_type(self, text: str) -> str:
        """ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø³Ù†Ø¯"""
        text_start = text[:1000].lower()  # Ø¨Ø±Ø±Ø³ÛŒ 1000 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„
        
        if 'Ù‚Ø§Ù†ÙˆÙ†' in text_start:
            return 'Ù‚Ø§Ù†ÙˆÙ†'
        elif 'Ø¢ÛŒÛŒÙ† Ù†Ø§Ù…Ù‡' in text_start or 'Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡' in text_start:
            return 'Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡'
        elif 'Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„' in text_start:
            return 'Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„'
        elif 'Ù…ØµÙˆØ¨Ù‡' in text_start:
            return 'Ù…ØµÙˆØ¨Ù‡'
        elif 'Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡' in text_start:
            return 'Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡'
        else:
            return 'Ø³Ù†Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ'
    
    def _extract_title(self, text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø³Ù†Ø¯"""
        lines = text.split('\n')[:15]  # Ø¨Ø±Ø±Ø³ÛŒ 15 Ø®Ø· Ø§ÙˆÙ„
        
        for line in lines:
            line = line.strip()
            if len(line) > 15 and len(line) < 300:  # Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù†
                # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
                title = re.sub(r'[#*\-=\[\]]+', '', line).strip()
                if title and not title.isdigit() and 'Ø¨Ø®Ø´' not in title[:10]:
                    return title
        
        # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø³Ø¹ÛŒ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ù¾ØªØ±Ù† Ù‚Ø§Ù†ÙˆÙ†
        law_match = self.patterns['law_title'].search(text)
        if law_match:
            return law_match.group(1).strip()
        
        return "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
    
    def _extract_approval_date(self, text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® ØªØµÙˆÛŒØ¨"""
        date_match = self.patterns['approval_date'].search(text)
        return date_match.group(1) if date_match else ""
    
    def _extract_authority(self, text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø±Ø¬Ø¹ ØªØµÙˆÛŒØ¨â€ŒÚ©Ù†Ù†Ø¯Ù‡"""
        authority_match = self.patterns['authority'].search(text)
        return authority_match.group(1) if authority_match else ""
    
    def clean_text(self, text: str) -> str:
        """
        Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ø§Ø² Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        """
        # Ø­ÙØ¸ line break Ù‡Ø§ÛŒ Ù…Ù‡Ù…
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø± Ø®Ø·
            cleaned_line = line.strip()
            
            if cleaned_line:  # ÙÙ‚Ø· Ø®Ø·ÙˆØ· ØºÛŒØ±Ø®Ø§Ù„ÛŒ
                # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¯Ø± Ø¯Ø§Ø®Ù„ Ø®Ø·
                cleaned_line = re.sub(r'\s+', ' ', cleaned_line)
                
                # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ú©Ù†ØªØ±Ù„ÛŒ
                cleaned_line = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', cleaned_line)
                
                # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø±Ù‚Ø§Ù… Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                arabic_to_persian = str.maketrans('Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©', 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹')
                cleaned_line = cleaned_line.translate(arabic_to_persian)
                
                cleaned_lines.append(cleaned_line)
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø±
        return '\n'.join(cleaned_lines)
    
    def process_single_document(self, file_path: str) -> Dict:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø³Ù†Ø¯ Ù…Ù†ÙØ±Ø¯ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        """
        print(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {os.path.basename(file_path)}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†
        raw_text, doc_info = self.extract_text_from_docx(file_path)
        
        if not raw_text:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² {file_path}")
            return {}
        
        # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
        cleaned_text = self.clean_text(raw_text)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ
        lines_before = len(raw_text.split('\n'))
        lines_after = len(cleaned_text.split('\n'))
        
        print(f"ğŸ§¹ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ: {lines_before} â†’ {lines_after} Ø®Ø·")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
        output_file = os.path.join(
            self.output_dir, 
            f"{os.path.splitext(os.path.basename(file_path))[0]}_cleaned.txt"
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(cleaned_text)
        
        # Ø°Ø®ÛŒØ±Ù‡ metadata
        metadata_file = os.path.join(
            self.metadata_dir,
            f"{os.path.splitext(os.path.basename(file_path))[0]}_metadata.json"
        )
        
        doc_info['cleaned_text_path'] = output_file
        doc_info['metadata']['cleaned_word_count'] = len(cleaned_text.split())
        doc_info['metadata']['cleaned_char_count'] = len(cleaned_text)
        doc_info['metadata']['cleaned_line_count'] = len(cleaned_text.split('\n'))
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(doc_info, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯:")
        print(f"   ğŸ“„ Ù…ØªÙ†: {output_file}")
        print(f"   ğŸ“Š Metadata: {metadata_file}")
        print(f"   ğŸ“ˆ Ø¢Ù…Ø§Ø±: {doc_info['metadata']['word_count']} Ú©Ù„Ù…Ù‡ØŒ {doc_info['metadata']['articles_count']} Ù…Ø§Ø¯Ù‡")
        
        return doc_info
    
    def process_all_documents(self) -> List[Dict]:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙ…Ø§Ù… Ø§Ø³Ù†Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± ÙˆØ±ÙˆØ¯ÛŒ
        """
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø²: {self.input_dir}")
        
        processed_docs = []
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ±Ø¯
        word_files = []
        for file_name in os.listdir(self.input_dir):
            if file_name.endswith(('.docx', '.doc')):
                word_files.append(os.path.join(self.input_dir, file_name))
        
        if not word_files:
            print("âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ ÙˆØ±Ø¯ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± ÙˆØ±ÙˆØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return []
        
        print(f"ğŸ“š ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(word_files)}")
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± ÙØ§ÛŒÙ„
        for file_path in word_files:
            try:
                doc_info = self.process_single_document(file_path)
                if doc_info:
                    processed_docs.append(doc_info)
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {file_path}: {str(e)}")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒ
        summary = {
            'total_documents': len(processed_docs),
            'total_words': sum(doc['metadata']['word_count'] for doc in processed_docs),
            'total_articles': sum(doc['metadata']['articles_count'] for doc in processed_docs),
            'total_lines': sum(doc['metadata']['line_count'] for doc in processed_docs),
            'document_types': list(set(doc['metadata']['document_type'] for doc in processed_docs)),
            'processing_date': datetime.now().isoformat()
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡
        summary_file = os.path.join(self.metadata_dir, 'processing_summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´:")
        print(f"   ğŸ“„ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù†Ø§Ø¯: {summary['total_documents']}")
        print(f"   ğŸ“ Ù…Ø¬Ù…ÙˆØ¹ Ú©Ù„Ù…Ø§Øª: {summary['total_words']}")
        print(f"   ğŸ“‹ Ù…Ø¬Ù…ÙˆØ¹ Ù…ÙˆØ§Ø¯: {summary['total_articles']}")
        print(f"   ğŸ“ Ù…Ø¬Ù…ÙˆØ¹ Ø®Ø·ÙˆØ·: {summary['total_lines']}")
        print(f"   ğŸ—‚ï¸ Ø§Ù†ÙˆØ§Ø¹ Ø§Ø³Ù†Ø§Ø¯: {', '.join(summary['document_types'])}")
        
        return processed_docs


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    # Ø§ÛŒØ¬Ø§Ø¯ extractor
    extractor = LegalDocumentExtractor()
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙ…Ø§Ù… Ø§Ø³Ù†Ø§Ø¯
    results = extractor.process_all_documents()
    
    if results:
        print(f"\nğŸ‰ Ù¾Ø±Ø¯Ø§Ø²Ø´ {len(results)} Ø³Ù†Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
    else:
        print("âŒ Ù‡ÛŒÚ† Ø³Ù†Ø¯ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯!")


if __name__ == "__main__":
    main()