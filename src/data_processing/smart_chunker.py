# src/data_processing/smart_chunker_improved.py

import os
import json
import re
from typing import List, Dict, Tuple
from datetime import datetime
import hashlib

class LegalTextChunker:
    """
    Ú©Ù„Ø§Ø³ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…ØªÙˆÙ† Ø­Ù‚ÙˆÙ‚ÛŒ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
    """
    
    def __init__(self, 
                 chunk_size: int = 400, 
                 chunk_overlap: int = 50,
                 min_chunk_size: int = 100):
        """
        Args: 
            chunk_size: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù‡Ø± chunk
            chunk_overlap: ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª overlap Ø¨ÛŒÙ† chunk Ù‡Ø§
            min_chunk_size: Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù‡Ø± chunk
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        
        # Ù¾ØªØ±Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø§Ø®ØªØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ
        self.structure_patterns = {
            'article': [
                re.compile(r'^[\s]*[â€Œ]?Ù…Ø§Ø¯Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]+[\.\-\s]', re.UNICODE),
                re.compile(r'^[\s]*Ù…Ø§Ø¯Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]+', re.UNICODE),
                re.compile(r'[â€Œ]Ù…Ø§Ø¯Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]+', re.UNICODE)
            ],
            'clause': [
                re.compile(r'^[\s]*([Ø§Ù„Ù-ÛŒ]|[Û±-Û¹]|\d+)\s*[\.\-\:\)]', re.UNICODE),
                re.compile(r'^[\s]*[Ø§Ù„Ù-ÛŒ]\s*[\.\-]', re.UNICODE),
                re.compile(r'^[\s]*\d+\s*[\.\-]', re.UNICODE)
            ],
            'note': [
                re.compile(r'^[\s]*[â€Œ]?ØªØ¨ØµØ±Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]*', re.UNICODE),
                re.compile(r'ØªØ¨ØµØ±Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]*', re.UNICODE)
            ],
            'chapter': [
                re.compile(r'^[\s]*ÙØµÙ„\s*[\u06F0-\u06F9\u0660-\u0669\d]+(.*)', re.UNICODE),
                re.compile(r'ÙØµÙ„\s*(Ø§ÙˆÙ„|Ø¯ÙˆÙ…|Ø³ÙˆÙ…|Ú†Ù‡Ø§Ø±Ù…|Ù¾Ù†Ø¬Ù…|Ø´Ø´Ù…)', re.UNICODE)
            ],
            'section': [
                re.compile(r'^[\s]*Ø¨Ø®Ø´\s*[\u06F0-\u06F9\u0660-\u0669\d]+(.*)', re.UNICODE),
                re.compile(r'Ø¨Ø®Ø´\s*(Ø§ÙˆÙ„|Ø¯ÙˆÙ…|Ø³ÙˆÙ…)', re.UNICODE)
            ],
            'law_title': [
                re.compile(r'Ù‚Ø§Ù†ÙˆÙ†\s+(.{10,})', re.UNICODE)
            ]
        }
        
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø±Ø§ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù boundaries
        self.boundary_weights = {
            'law_title': 12,
            'chapter': 10,
            'section': 9,
            'article': 8,
            'note': 7,
            'clause': 6,
            'paragraph': 5,
            'sentence': 3
        }
    
    def identify_structure_boundaries(self, text: str) -> List[Dict]:
        """
        Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ boundaries Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ø¯Ø± Ù…ØªÙ† - Ù†Ø³Ø®Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
        """
        boundaries = []
        lines = text.split('\n')
        
        print(f"ğŸ” ØªØ­Ù„ÛŒÙ„ {len(lines)} Ø®Ø· Ù…ØªÙ†...")
        
        current_position = 0
        
        for i, line in enumerate(lines):
            original_line = line
            line = line.strip()
            
            # Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ ÛŒØ§ Ú©ÙˆØªØ§Ù‡
            if not line or len(line) < 3:
                current_position += len(original_line) + 1  # +1 Ø¨Ø±Ø§ÛŒ \n
                continue
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù boundaries
            boundary_found = False
            
            for boundary_type, patterns in self.structure_patterns.items():
                if boundary_found:
                    break
                    
                for pattern in patterns:
                    match = pattern.search(line)
                    if match:
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ø§Ø² match
                        number = ""
                        if match.groups():
                            number = match.group(1)
                        else:
                            # Ø³Ø¹ÛŒ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ø§Ø² Ø®ÙˆØ¯ Ø®Ø·
                            number_match = re.search(r'[\u06F0-\u06F9\u0660-\u0669\d]+', line)
                            if number_match:
                                number = number_match.group()
                        
                        boundaries.append({
                            'type': boundary_type,
                            'position': current_position,
                            'line_number': i,
                            'text': line[:80] + "..." if len(line) > 80 else line,
                            'number': number,
                            'weight': self.boundary_weights.get(boundary_type, 1),
                            'full_line': line,
                            'line_length': len(line)
                        })
                        boundary_found = True
                        #print(f"  ğŸ“ {boundary_type}: {line[:60]}...")
                        break
            
            # Ø§Ú¯Ø± boundary Ù†Ø¨ÙˆØ¯ØŒ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†
            if not boundary_found and len(line) > 150:
                boundaries.append({
                    'type': 'paragraph',
                    'position': current_position,
                    'line_number': i,
                    'text': line[:80] + "..." if len(line) > 80 else line,
                    'number': '',
                    'weight': self.boundary_weights['paragraph'],
                    'full_line': line,
                    'line_length': len(line)
                })
            
            current_position += len(original_line) + 1  # +1 Ø¨Ø±Ø§ÛŒ \n
        
        print(f"ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ boundaries ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(boundaries)}")
        
        # Ø§Ú¯Ø± boundaries Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ…
        if len(boundaries) < 10:
            print("âš ï¸ boundaries Ú©Ø§ÙÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† paragraph boundaries...")
            boundaries.extend(self._add_paragraph_boundaries(text, lines, len(boundaries)))
        
        return sorted(boundaries, key=lambda x: x['position'])
    
    def _add_paragraph_boundaries(self, text: str, lines: List[str], existing_count: int) -> List[Dict]:
        """
        Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† boundaries Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ³Ø·
        """
        paragraph_boundaries = []
        current_position = 0
        
        for i, line in enumerate(lines):
            original_line = line
            line = line.strip()
            
            # Ø´Ø±Ø§ÛŒØ· ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ paragraph boundary
            if (len(line) > 50 and  # Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„
                not any(char in line[:20] for char in ['Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡', 'ÙØµÙ„']) and  # Ù†Ø¨Ø§Ø´Ø¯ boundary Ø§ØµÙ„ÛŒ
                line.count('.') >= 1):  # Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø¬Ù…Ù„Ù‡
                
                paragraph_boundaries.append({
                    'type': 'paragraph',
                    'position': current_position,
                    'line_number': i,
                    'text': line[:80] + "..." if len(line) > 80 else line,
                    'number': str(len(paragraph_boundaries) + 1),
                    'weight': self.boundary_weights['paragraph'],
                    'full_line': line,
                    'line_length': len(line)
                })
            
            current_position += len(original_line) + 1
        
        print(f"ğŸ“ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ {len(paragraph_boundaries)} paragraph boundary")
        return paragraph_boundaries
    
    def create_semantic_chunks(self, text: str, boundaries: List[Dict]) -> List[Dict]:
        """
        Ø§ÛŒØ¬Ø§Ø¯ chunk Ù‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ boundaries - Ù†Ø³Ø®Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
        """
        if not boundaries:
            return self._create_simple_chunks(text)
        
        chunks = []
        current_chunk_lines = []
        current_word_count = 0
        current_metadata = {
            'structures': [],
            'start_position': 0,
            'boundaries_included': []
        }
        
        print(f"ğŸ”„ Ø§ÛŒØ¬Ø§Ø¯ chunks Ø§Ø² {len(boundaries)} boundary...")
        
        # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø®Ø·ÙˆØ·
        lines = text.split('\n')
        processed_lines = 0
        
        for boundary in boundaries:
            try:
                # Ø®Ø·ÙˆØ· Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÛŒÙ† boundary
                boundary_line_num = boundary['line_number']
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø·ÙˆØ· Ù‚Ø¨Ù„ Ø§Ø² Ø§ÛŒÙ† boundary (Ø§Ú¯Ø± Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯)
                while processed_lines < boundary_line_num:
                    if processed_lines < len(lines):
                        line = lines[processed_lines].strip()
                        if line:
                            current_chunk_lines.append(line)
                            current_word_count += len(line.split())
                    processed_lines += 1
                
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø· boundary
                boundary_line = boundary['full_line']
                current_chunk_lines.append(boundary_line)
                current_word_count += len(boundary_line.split())
                processed_lines += 1
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø¨Ø§ÛŒØ¯ chunk Ø¬Ø¯ÛŒØ¯ Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒÙ…
                should_break = (
                    current_word_count >= self.chunk_size or
                    (current_word_count >= self.min_chunk_size and 
                     boundary['weight'] >= self.boundary_weights['article'])
                )
                
                if should_break and current_chunk_lines:
                    # Ø§ÛŒØ¬Ø§Ø¯ chunk ÙØ¹Ù„ÛŒ
                    chunk_text = '\n'.join(current_chunk_lines)
                    chunk_data = self._create_chunk_data(
                        chunk_text, 
                        current_metadata, 
                        len(chunks)
                    )
                    chunks.append(chunk_data)
                    
                    # Ø´Ø±ÙˆØ¹ chunk Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ overlap
                    overlap_lines = self._create_overlap_lines(current_chunk_lines)
                    current_chunk_lines = overlap_lines
                    current_word_count = sum(len(line.split()) for line in overlap_lines)
                    current_metadata = {
                        'structures': [boundary['type']],
                        'start_position': boundary['position'],
                        'boundaries_included': [boundary]
                    }
                else:
                    # Ø§Ø¯Ø§Ù…Ù‡ chunk ÙØ¹Ù„ÛŒ
                    current_metadata['structures'].append(boundary['type'])
                    current_metadata['boundaries_included'].append(boundary)
                    
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ boundary {boundary.get('line_number', '?')}: {str(e)}")
                continue
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø§Ù‚ÛŒ Ø®Ø·ÙˆØ·
        while processed_lines < len(lines):
            line = lines[processed_lines].strip()
            if line:
                current_chunk_lines.append(line)
                current_word_count += len(line.split())
            processed_lines += 1
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† chunk
        if current_chunk_lines:
            chunk_text = '\n'.join(current_chunk_lines)
            if len(chunk_text.split()) >= self.min_chunk_size:
                chunk_data = self._create_chunk_data(
                    chunk_text, 
                    current_metadata, 
                    len(chunks)
                )
                chunks.append(chunk_data)
        
        print(f"âœ… ØªÙˆÙ„ÛŒØ¯ {len(chunks)} chunk")
        return chunks
    
    def _create_overlap_lines(self, lines: List[str]) -> List[str]:
        """
        Ø§ÛŒØ¬Ø§Ø¯ overlap Ø¨ÛŒÙ† chunks Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø·ÙˆØ·
        """
        if not lines:
            return []
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ· overlap Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª
        overlap_words = 0
        overlap_lines = []
        
        for line in reversed(lines):
            line_words = len(line.split())
            if overlap_words + line_words <= self.chunk_overlap:
                overlap_lines.insert(0, line)
                overlap_words += line_words
            else:
                break
        
        return overlap_lines
    
    def _create_simple_chunks(self, text: str) -> List[Dict]:
        """
        Ø§ÛŒØ¬Ø§Ø¯ chunks Ø³Ø§Ø¯Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ boundaries Ú©Ø§ÙÛŒ
        """
        chunks = []
        lines = text.split('\n')
        
        current_lines = []
        current_word_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            line_words = len(line.split())
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø¨Ø§ÛŒØ¯ chunk Ø¬Ø¯ÛŒØ¯ Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒÙ…
            if current_word_count + line_words > self.chunk_size and current_lines:
                # Ø§ÛŒØ¬Ø§Ø¯ chunk ÙØ¹Ù„ÛŒ
                chunk_text = '\n'.join(current_lines)
                if len(chunk_text.split()) >= self.min_chunk_size:
                    chunk_data = {
                        'chunk_id': self._generate_chunk_id(chunk_text, len(chunks)),
                        'chunk_index': len(chunks),
                        'text': chunk_text,
                        'word_count': len(chunk_text.split()),
                        'char_count': len(chunk_text),
                        'sentence_count': len([s for s in chunk_text.split('.') if s.strip()]),
                        'structures': ['simple_paragraph'],
                        'start_position': 0,
                        'boundaries_info': [],
                        'quality_score': self._calculate_simple_quality_score(chunk_text),
                        'keywords': self._extract_keywords(chunk_text),
                        'legal_entities': self._extract_legal_entities(chunk_text)
                    }
                    chunks.append(chunk_data)
                
                # Ø´Ø±ÙˆØ¹ chunk Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ overlap
                overlap_lines = current_lines[-2:] if len(current_lines) > 2 else []
                current_lines = overlap_lines + [line]
                current_word_count = sum(len(l.split()) for l in current_lines)
            else:
                current_lines.append(line)
                current_word_count += line_words
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† chunk
        if current_lines:
            chunk_text = '\n'.join(current_lines)
            if len(chunk_text.split()) >= self.min_chunk_size:
                chunk_data = {
                    'chunk_id': self._generate_chunk_id(chunk_text, len(chunks)),
                    'chunk_index': len(chunks),
                    'text': chunk_text,
                    'word_count': len(chunk_text.split()),
                    'char_count': len(chunk_text),
                    'sentence_count': len([s for s in chunk_text.split('.') if s.strip()]),
                    'structures': ['simple_paragraph'],
                    'start_position': 0,
                    'boundaries_info': [],
                    'quality_score': self._calculate_simple_quality_score(chunk_text),
                    'keywords': self._extract_keywords(chunk_text),
                    'legal_entities': self._extract_legal_entities(chunk_text)
                }
                chunks.append(chunk_data)
        
        return chunks
    
    def _calculate_simple_quality_score(self, text: str) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª Ø³Ø§Ø¯Ù‡"""
        score = 5.0  # Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„
        words = text.split()
        if self.min_chunk_size <= len(words) <= self.chunk_size:
            score += 2.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        legal_indicators = ['Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ù‚Ø±Ø±']
        legal_count = sum(1 for indicator in legal_indicators if indicator in text)
        score += min(legal_count * 0.5, 3.0)
        
        return min(score, 10.0)
    
    def _create_chunk_data(self, text: str, metadata: Dict, chunk_index: int) -> Dict:
        """Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ÛŒÚ© chunk"""
        
        # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† chunk
        clean_text = self._clean_chunk_text(text)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ID ÛŒÚ©ØªØ§
        chunk_id = self._generate_chunk_id(clean_text, chunk_index)
        
        # Ø¢Ù…Ø§Ø±Ú¯ÛŒØ±ÛŒ
        words = clean_text.split()
        sentences = re.split(r'[.!?ØŸÛ”]', clean_text)
        
        return {
            'chunk_id': chunk_id,
            'chunk_index': chunk_index,
            'text': clean_text,
            'word_count': len(words),
            'char_count': len(clean_text),
            'sentence_count': len([s for s in sentences if s.strip()]),
            'structures': list(set(metadata.get('structures', []))),
            'start_position': metadata.get('start_position', 0),
            'boundaries_info': metadata.get('boundaries_included', []),
            'quality_score': self._calculate_quality_score(clean_text, metadata),
            'keywords': self._extract_keywords(clean_text),
            'legal_entities': self._extract_legal_entities(clean_text)
        }
    
    def _clean_chunk_text(self, text: str) -> str:
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† chunk"""
        # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø¬Ø¯Ø¯
        text = '\n'.join(lines)
        
        # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¯Ø± Ù‡Ø± Ø®Ø·
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def _generate_chunk_id(self, text: str, index: int) -> str:
        """ØªÙˆÙ„ÛŒØ¯ ID ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ chunk"""
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
        return f"chunk_{index:04d}_{text_hash}"
    
    def _calculate_quality_score(self, text: str, metadata: Dict) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª chunk"""
        score = 0.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„
        words = text.split()
        if self.min_chunk_size <= len(words) <= self.chunk_size:
            score += 3.0
        elif len(words) < self.min_chunk_size:
            score += 1.0
        else:
            score += 2.0
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø±
        structures = metadata.get('structures', [])
        structure_bonus = sum(self.boundary_weights.get(s, 0) for s in structures)
        score += min(structure_bonus / 10, 3.0)
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ
        legal_indicators = ['Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ù‚Ø±Ø±', 'Ù…ÙˆØ¶ÙˆØ¹']
        legal_count = sum(1 for indicator in legal_indicators if indicator in text)
        score += min(legal_count * 0.5, 2.0)
        
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙ†ÙˆØ¹ Ø¬Ù…Ù„Ø§Øª
        sentences = text.split('.')
        if len(sentences) > 2:
            score += 1.0
        
        return min(score, 10.0)  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø§Ù…ØªÛŒØ§Ø² 10
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ø§Ø² chunk"""
        legal_keywords = [
            'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø§Ø¯Ù‡', 'Ø¨Ù†Ø¯', 'ØªØ¨ØµØ±Ù‡', 'ÙØµÙ„', 'Ù…Ù‚Ø±Ø±', 'Ù…ÙˆØ¶ÙˆØ¹',
            'Ù…Ø¬Ù„Ø³', 'Ù‡ÛŒØ¦Øª ÙˆØ²ÛŒØ±Ø§Ù†', 'Ø´ÙˆØ±Ø§ÛŒ Ø¹Ø§Ù„ÛŒ', 'ÙˆØ²Ø§Ø±Øª', 'Ù…Ø¤Ø³Ø³Ù‡',
            'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡', 'Ù¾Ú˜ÙˆÙ‡Ø´', 'ÙÙ†Ø§ÙˆØ±ÛŒ', 'ØªØ­Ù‚ÛŒÙ‚Ø§Øª', 'Ø¹Ù„ÙˆÙ…',
            'Ø§Ù†ØªØ¸Ø§Ù…ÛŒ', 'ØªØ®Ù„Ù', 'Ù…Ø¬Ø§Ø²Ø§Øª', 'ØªØ¹Ù‡Ø¯', 'Ù…Ø³Ø¦ÙˆÙ„ÛŒØª'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in legal_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _extract_legal_entities(self, text: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¬ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø­Ù‚ÙˆÙ‚ÛŒ"""
        entities = []
        
        # ØªØ´Ø®ÛŒØµ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª Ø¨Ù‡ Ù…ÙˆØ§Ø¯
        article_refs = re.findall(r'Ù…Ø§Ø¯Ù‡\s*[\u06F0-\u06F9\u0660-\u0669\d]+', text)
        for ref in article_refs:
            entities.append({
                'type': 'article_reference',
                'value': ref
            })
        
        # ØªØ´Ø®ÛŒØµ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§
        dates = re.findall(r'(\d{1,2}/\d{1,2}/\d{4})', text)
        for date in dates:
            entities.append({
                'type': 'date',
                'value': date
            })
        
        return entities
    
    def chunk_document(self, text: str, document_metadata: Dict) -> List[Dict]:
        """
        ØªÙ‚Ø³ÛŒÙ… ÛŒÚ© Ø³Ù†Ø¯ Ú©Ø§Ù…Ù„ Ø¨Ù‡ chunk Ù‡Ø§ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
        """
        print(f"ğŸ”„ Ø´Ø±ÙˆØ¹ chunking Ø³Ù†Ø¯: {document_metadata.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù…ØªÙ†
        total_words = len(text.split())
        total_lines = len(text.split('\n'))
        
        print(f"ğŸ“„ ÙˆØ±ÙˆØ¯ÛŒ: {total_words} Ú©Ù„Ù…Ù‡ØŒ {total_lines} Ø®Ø·")
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ boundaries
        boundaries = self.identify_structure_boundaries(text)
        print(f"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ boundaries ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(boundaries)}")
        
        # Ø§ÛŒØ¬Ø§Ø¯ chunk Ù‡Ø§
        chunks = self.create_semantic_chunks(text, boundaries)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† metadata Ø³Ù†Ø¯ Ø¨Ù‡ Ù‡Ø± chunk
        for chunk in chunks:
            chunk['document_metadata'] = document_metadata
            chunk['document_title'] = document_metadata.get('title', '')
            chunk['document_type'] = document_metadata.get('document_type', '')
            chunk['authority'] = document_metadata.get('authority', '')
            chunk['approval_date'] = document_metadata.get('approval_date', '')
        
        if chunks:
            avg_chunk_size = sum(c['word_count'] for c in chunks) / len(chunks)
            avg_quality = sum(c['quality_score'] for c in chunks) / len(chunks)
            print(f"âœ… ØªÙˆÙ„ÛŒØ¯ {len(chunks)} chunk Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¯")
            print(f"ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ chunk: {avg_chunk_size:.1f} Ú©Ù„Ù…Ù‡")
            print(f"â­ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©ÛŒÙÛŒØª: {avg_quality:.1f}/10")
        else:
            print("âŒ Ù‡ÛŒÚ† chunk ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯!")
        
        return chunks
    
    def save_chunks(self, chunks: List[Dict], output_dir: str, document_name: str):
        """
        Ø°Ø®ÛŒØ±Ù‡ chunk Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
        """
        if not chunks:
            print("âš ï¸ Ù‡ÛŒÚ† chunk Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
            return
            
        os.makedirs(output_dir, exist_ok=True)
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªÚ© ØªÚ© chunk Ù‡Ø§
        chunks_dir = os.path.join(output_dir, f"{document_name}_chunks")
        os.makedirs(chunks_dir, exist_ok=True)
        
        for chunk in chunks:
            chunk_file = os.path.join(chunks_dir, f"{chunk['chunk_id']}.json")
            with open(chunk_file, 'w', encoding='utf-8') as f:
                json.dump(chunk, f, ensure_ascii=False, indent=2)
        
        # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø®Ù„Ø§ØµÙ‡
        summary = {
            'document_name': document_name,
            'total_chunks': len(chunks),
            'total_words': sum(c['word_count'] for c in chunks),
            'average_chunk_size': sum(c['word_count'] for c in chunks) / len(chunks),
            'min_chunk_size': min(c['word_count'] for c in chunks),
            'max_chunk_size': max(c['word_count'] for c in chunks),
            'quality_scores': [c['quality_score'] for c in chunks],
            'average_quality': sum(c['quality_score'] for c in chunks) / len(chunks),
            'structures_found': list(set(
                structure for chunk in chunks 
                for structure in chunk['structures']
            )),
            'chunking_date': datetime.now().isoformat(),
            'chunking_parameters': {
                'chunk_size': self.chunk_size,
                'chunk_overlap': self.chunk_overlap,
                'min_chunk_size': self.min_chunk_size
            }
        }
        
        summary_file = os.path.join(output_dir, f"{document_name}_chunks_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ chunk Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯ Ø¯Ø±: {chunks_dir}")
        print(f"ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {summary_file}")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª chunker"""
    # Ø®ÙˆØ§Ù†Ø¯Ù† ÛŒÚ© ÙØ§ÛŒÙ„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    processed_dir = "data/processed_chunks"
    metadata_dir = "data/metadata"
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
    text_files = [f for f in os.listdir(processed_dir) if f.endswith('_cleaned.txt')]
    
    if not text_files:
        print("âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        print("Ø§Ø¨ØªØ¯Ø§ document_extractor.py Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
        return
    
    # Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
    test_file = text_files[0]
    document_name = test_file.replace('_cleaned.txt', '')
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…ØªÙ†
    with open(os.path.join(processed_dir, test_file), 'r', encoding='utf-8') as f:
        text = f.read()
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† metadata
    metadata_file = os.path.join(metadata_dir, f"{document_name}_metadata.json")
    with open(metadata_file, 'r', encoding='utf-8') as f:
        doc_metadata = json.load(f)
    
    # Ø§ÛŒØ¬Ø§Ø¯ chunker
    chunker = LegalTextChunker(chunk_size=400, chunk_overlap=50)
    
    # chunking
    chunks = chunker.chunk_document(text, doc_metadata['metadata'])
    
    # Ø°Ø®ÛŒØ±Ù‡
    output_dir = "data/chunks"
    chunker.save_chunks(chunks, output_dir, document_name)
    
    print(f"\nğŸ‰ Chunking Ø³Ù†Ø¯ {document_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ chunk Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡: {len(chunks)}")


if __name__ == "__main__":
    main()