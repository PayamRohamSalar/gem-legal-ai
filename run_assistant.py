# run_assistant.py
import asyncio
import sys
import os

# Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ sys.path
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©Ø±Ø¯Ù† Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
from src.retrieval.vector_database import LegalVectorDatabase
from src.retrieval.embedding_manager import EmbeddingModelManager
from src.retrieval.hybrid_search import HybridSearchEngine
from src.generation.integrated_response_system import LegalResponseSystem, ResponseRequest, QueryType

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
DB_PATH = "data/vector_db"
COLLECTION_NAME = "legal_hybrid_v1"
ACTIVE_LLM_KEY = "qwen"  # <-- Ø§ØµÙ„Ø§Ø­ Ø´Ø¯: Ø§Ø² Ú©Ù„ÛŒØ¯ Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

async def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ"""
    print("ğŸš€=============== INITIALIZING LEGAL AI ASSISTANT ===============ğŸš€")
    
    # === Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ ===
    vector_db = LegalVectorDatabase(db_path=DB_PATH, collection_name=COLLECTION_NAME)
    embedding_manager = EmbeddingModelManager()
    embedding_manager.load_model(embedding_manager.get_recommended_model())
    search_engine = HybridSearchEngine(vector_db, embedding_manager)
    print("âœ… Search Engine is ready.")
    
    # === Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ ===
    response_system = LegalResponseSystem(default_model=ACTIVE_LLM_KEY)
    from src.generation.llm_manager import create_model_configs
    response_system.llm_manager.configs = create_model_configs()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ ÙØ¹Ø§Ù„
    if not response_system.llm_manager.set_active_model(ACTIVE_LLM_KEY):
        print(f"âŒ ERROR: Could not set active model to '{ACTIVE_LLM_KEY}'. Please check configs.")
        return
    print("âœ… Response System is ready.")

    # === Ø·Ø±Ø­ Ø³ÙˆØ§Ù„ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø®Ø· Ù„ÙˆÙ„Ù‡ ===
    user_question = "Ø§Ù‡Ø¯Ø§Ù Ø³Ù†Ø¯ Ú¯Ø³ØªØ±Ø´ Ú©Ø§Ø±Ø¨Ø±Ø¯ ÙÙ†Ø§ÙˆØ±ÛŒ Ù†Ø§Ù†Ùˆ Ø¯Ø± Ø§ÙÙ‚ Ù¡Ù¤Ù Ù¤ Ú†ÛŒØ³ØªØŸ"
    print(f"\n--- Processing question: '{user_question}' ---")

    # Û±. Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† context
    contexts = search_engine.hybrid_search(user_question, top_k=7)
    if not contexts:
        print("âŒ No relevant documents found.")
        return

    # Û². Ù†Ù…Ø§ÛŒØ´ Ù…ØªÙ† Ú©Ø§Ù…Ù„ chunkÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
    print("\n--- Inspecting FULL Retrieved Context ---")
    print("==============================")
    for i, ctx in enumerate(contexts, 1):
        print(f"--- CHUNK #{i} | Score: {ctx.get('final_score', 0):.3f} | Source: {ctx['metadata'].get('document_title', 'N/A')} ---")
        print(ctx['text'])
        print("-" * 25)
    print("==============================")
    
    # Û³. Ø³Ø§Ø®Øª Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ
    request = ResponseRequest(
        question=user_question,
        contexts=[{'content': ctx['text'], 'source': ctx['metadata'].get('document_title', '')} for ctx in contexts]
    )

    # Û´. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ
    result = await response_system.generate_response(request)

    # Ûµ. Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡
    if result.success:
        print("\n\nâœ…âœ…âœ… FINAL ENHANCED RESPONSE âœ…âœ…âœ…")
        print("=" * 50)
        print("âœï¸ **Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ:**\n")
        print(result.enhanced_response)
        print("\n" + "=" * 50)
        print(result.references_list)
        print("=" * 50)
        print(f"â­ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª Ù¾Ø§Ø³Ø®: {result.quality_score:.1f}/100")
        print(f"ğŸ¯ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØµØ­Øª Ù¾Ø§Ø³Ø®: {result.confidence_score*100:.1f}%")
        print(f"â±ï¸ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´: {result.processing_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    else:
        print(f"\nâŒ ERROR: {result.error_message}")

    print("\nğŸš€=============== ASSISTANT RUN COMPLETE ===============ğŸš€")

if __name__ == "__main__":
    asyncio.run(main())