# run_assistant.py
import sys
import os
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©Ø±Ø¯Ù† Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
from src.retrieval.hybrid_search import HybridSearchEngine
from src.retrieval.vector_database import LegalVectorDatabase
from src.retrieval.embedding_manager import EmbeddingModelManager
from src.generation.llm_manager import LLMManager, create_model_configs
from src.generation.prompt_engine import PromptEngine, ContextInfo, ContextType # <-- Ø§ØµÙ„Ø§Ø­ Ø´Ø¯

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ ---
DB_PATH = "data/vector_db"
COLLECTION_NAME = "legal_hybrid_v1"
ACTIVE_LLM_KEY = "qwen"

def main():
    print("ðŸš€=============== INITIALIZING LEGAL AI ASSISTANT (Stable Version) ===============ðŸš€")
    
    # === Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ ===
    print("\n--- Initializing Search Engine ---")
    vector_db = LegalVectorDatabase(db_path=DB_PATH, collection_name=COLLECTION_NAME)
    embedding_manager = EmbeddingModelManager()
    embedding_manager.load_model(embedding_manager.get_recommended_model())
    search_engine = HybridSearchEngine(vector_db, embedding_manager)
    print("âœ… Search Engine is ready.")

    # === Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯ÛŒØ± LLM ===
    print("\n--- Initializing LLM Manager ---")
    llm_configs = create_model_configs()
    llm_manager = LLMManager(llm_configs)
    if not llm_manager.set_active_model(ACTIVE_LLM_KEY):
        return
    print("âœ… LLM Manager is ready.")

    # === Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ù¾Ø±Ø§Ù…Ù¾Øª ===
    prompt_engine = PromptEngine() # <-- Ø§ØµÙ„Ø§Ø­ Ø´Ø¯
    print("âœ… Prompt Engine is ready.")

    # === Ø·Ø±Ø­ Ø³ÙˆØ§Ù„ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø®Ø· Ù„ÙˆÙ„Ù‡ ===
    user_question = "Ø§Ù‡Ø¯Ø§Ù Ø³Ù†Ø¯ Ú¯Ø³ØªØ±Ø´ Ú©Ø§Ø±Ø¨Ø±Ø¯ ÙÙ†Ø§ÙˆØ±ÛŒ Ù†Ø§Ù†Ùˆ Ø¯Ø± Ø§ÙÙ‚ Ù¡Ù¤Ù Ù¤ Ú†ÛŒØ³ØªØŸ"
    print(f"\n--- Processing question: '{user_question}' ---")

    # Û±. Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† context
    contexts_raw = search_engine.hybrid_search(user_question, top_k=5)
    if not contexts_raw:
        print("âŒ No relevant documents found.")
        return

    # Û². Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª
    # ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø®Ø§Ù… Ø¨Ù‡ ÙØ±Ù…Øª ContextInfo
    contexts_info = [ContextInfo(
        content=res['text'],
        source=res['metadata'].get('document_title', 'Ø³Ù†Ø¯ Ù†Ø§Ù…Ø´Ø®Øµ'),
        document_type=ContextType.LAW, # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ ÙØ¹Ù„Ø§Ù‹ Ù‡Ù…Ù‡ Ø±Ø§ Ù‚Ø§Ù†ÙˆÙ† Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
        relevance_score=res.get('final_score', 0)
    ) for res in contexts_raw]

    final_prompt, query_type = prompt_engine.build_prompt( # <-- Ø§ØµÙ„Ø§Ø­ Ø´Ø¯
        question=user_question,
        contexts=contexts_info
    )
    print(f"âœ… Prompt built for query type: {query_type.value}")
    
    # Û³. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ
    print(f"\n--- Generating response with '{ACTIVE_LLM_KEY}' model ---")
    result = llm_manager.generate_response(final_prompt)
    
    # Û´. Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡
    if result['success']:
        print("\n\nâœ…âœ…âœ… FINAL RESPONSE âœ…âœ…âœ…")
        print("=" * 50)
        print("âœï¸ **Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ:**\n")
        print(result['response'])
        print("\n" + "=" * 50)
    else:
        print(f"\nâŒ ERROR: {result['error']}")

    print("\nðŸš€=============== ASSISTANT RUN COMPLETE ===============ðŸš€")

if __name__ == "__main__":
    main()