# scripts/run_evaluation.py
import asyncio
import logging
import sys
from pathlib import Path

# Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø³ÛŒØ± Ø±ÛŒØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ…Ù¾ÙˆØ±Øª ØµØ­ÛŒØ­ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

from src.evaluation.evaluation_system import Phase4EvaluationSystem, EvaluationConfig

async def main():
    """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ø² 4"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸš€=============== START: PHASE 4 - FULL SYSTEM EVALUATION ===============ğŸš€")
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    config = EvaluationConfig(
        dataset_size=30,      # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ÙˆØ§Ù„Ø§Øª Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯
        test_subset_size=10,  # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙÙ‚Ø· Ø±ÙˆÛŒ 10 Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª
        use_ollama=True,
        preferred_model='mistral_7b', # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡
        timeout_per_question=45,
        save_detailed_results=True
    )
    
    evaluation_system = Phase4EvaluationSystem()
    
    try:
        # Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        result, report = await evaluation_system.run_full_evaluation(config)
        
        if result:
            print(f"\nğŸ“Š=============== EVALUATION SUMMARY ===============ğŸ“Š")
            print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…: {result.overall_score:.3f}")
            print(f"   â€¢ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {result.successful_evaluations}/{result.total_questions} ({result.successful_evaluations/result.total_questions*100:.1f}%)")
            print(f"   â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®: {result.detailed_metrics.get('avg_processing_time', 0):.2f} Ø«Ø§Ù†ÛŒÙ‡")
            
            print(f"\nğŸ“ˆ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§:")
            print(f"   â€¢ Ø¯Ù‚Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (Precision@5): {result.detailed_metrics.get('avg_precision_at_5', 0):.3f}")
            print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø² ØªÙˆÙ„ÛŒØ¯ (ROUGE-L): {result.detailed_metrics.get('avg_rouge_l', 0):.3f}")
            print(f"   â€¢ Ø¯Ù‚Øª Ø§Ø±Ø¬Ø§Ø¹Ø§Øª: {result.detailed_metrics.get('avg_citation_accuracy', 0):.3f}")
            
            print(f"\nğŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:")
            for rec in report['recommendations']:
                print(f"   â€¢ {rec}")
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ: {e}", exc_info=True)

    print("\nğŸš€=============== END: PHASE 4 EVALUATION ===============ğŸš€")

if __name__ == "__main__":
    asyncio.run(main())