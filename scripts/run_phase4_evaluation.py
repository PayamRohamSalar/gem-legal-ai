#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_phase4_evaluation.py - Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ø² 4

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ù…Ø³Ø¦ÙˆÙ„ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ùˆ Ù‡Ù…Ø§Ù‡Ù†Ú¯ ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§ÛŒ ÙØ§Ø² 4 Ø§Ø³Øª:
- ØªØ³Øª Ø§ØªØµØ§Ù„ Ollama
- ØªÙˆÙ„ÛŒØ¯ Dataset
- Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
- ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ

Ø§Ø³ØªÙØ§Ø¯Ù‡:
    python run_phase4_evaluation.py --mode full
    python run_phase4_evaluation.py --mode quick
    python run_phase4_evaluation.py --mode test-only
"""

import sys
import os
import argparse
import asyncio
import logging
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ§Ø±Ø³ÛŒ
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src
current_dir = Path(__file__).parent
src_path = current_dir / 'src'
if src_path.exists():
    sys.path.insert(0, str(src_path))

# Import Ø§Ø¬Ø²Ø§ÛŒ Ø³ÛŒØ³ØªÙ…
try:
    from generation.llm_manager import LLMManager, create_model_configs
    from evaluation.dataset_generator import LegalDatasetGenerator, QuestionDifficulty, QuestionCategory
    from evaluation.phase4_evaluation_system import Phase4EvaluationSystem, EvaluationConfig
    from evaluation.optimization_manager import LegalSystemOptimizer, OptimizationConfig
    from evaluation.metrics_calculator import LegalMetricsCalculator
    SYSTEM_AVAILABLE = True
except ImportError as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± import Ø³ÛŒØ³ØªÙ…: {e}")
    SYSTEM_AVAILABLE = False

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
def setup_logging(log_level: str = "INFO") -> None:
    """ØªÙ†Ø¸ÛŒÙ… Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ"""
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ logs
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ Ø¨Ø§ timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = logs_dir / f"phase4_evaluation_{timestamp}.log"
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    print(f"ğŸ“‹ Log file: {log_file}")

class Phase4Runner:
    """Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² 4"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.start_time = datetime.now()
        self.results = {}
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
        self.setup_directories()
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("Phase4Runner Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def setup_directories(self) -> None:
        """Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²"""
        
        directories = [
            "data/test_dataset",
            "data/phase4_evaluation", 
            "data/optimization_results",
            "logs",
            "reports"
        ]
        
        for dir_path in directories:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    async def test_ollama_connection(self) -> bool:
        """ØªØ³Øª Ø§ØªØµØ§Ù„ Ollama"""
        
        print("\nğŸ” ØªØ³Øª Ø§ØªØµØ§Ù„ Ollama...")
        print("=" * 50)
        
        try:
            configs = create_model_configs()
            manager = LLMManager(configs)
            
            # Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
            available = manager.list_available_models()
            print(f"ğŸ“‹ ÙˆØ¶Ø¹ÛŒØª Ollama: {available['ollama_status']}")
            print(f"ğŸ“¦ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {available['ollama_models']}")
            
            # ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
            preferred_model = self.config.get('preferred_model', 'qwen_7b')
            print(f"\nğŸ”„ ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {preferred_model}")
            
            success = manager.load_model(preferred_model, prefer_ollama=True)
            
            if success:
                print("âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
                
                # ØªØ³Øª ØªÙˆÙ„ÛŒØ¯
                test_prompts = [
                    "Ø³Ù„Ø§Ù…",
                    "ÙˆØ¸Ø§ÛŒÙ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ú†ÛŒØ³ØªØŸ"
                ]
                
                for prompt in test_prompts:
                    print(f"\nğŸ§ª ØªØ³Øª: {prompt}")
                    
                    start = time.time()
                    result = manager.generate_response(prompt)
                    duration = time.time() - start
                    
                    if result['success']:
                        response = result['response'][:100] + "..." if len(result['response']) > 100 else result['response']
                        print(f"âœ… Ù¾Ø§Ø³Ø® ({duration:.2f}s): {response}")
                        
                        # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
                        metrics = result.get('metrics')
                        if metrics:
                            print(f"ğŸ“Š ØªÙˆÚ©Ù†â€ŒÙ‡Ø§: {metrics.input_tokens}â†’{metrics.output_tokens}")
                    else:
                        print(f"âŒ Ø®Ø·Ø§: {result['error']}")
                        return False
                
                self.results['ollama_test'] = {
                    'status': 'success',
                    'model': preferred_model,
                    'available_models': available['ollama_models']
                }
                
                return True
            else:
                print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„")
                self.results['ollama_test'] = {'status': 'failed', 'reason': 'model_load_failed'}
                return False
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ³Øª Ollama: {e}")
            self.results['ollama_test'] = {'status': 'failed', 'reason': str(e)}
            return False
    
    def generate_test_dataset(self) -> Optional[List]:
        """ØªÙˆÙ„ÛŒØ¯ Dataset ØªØ³Øª"""
        
        print("\nğŸ“Š ØªÙˆÙ„ÛŒØ¯ Dataset ØªØ³Øª...")
        print("=" * 50)
        
        try:
            generator = LegalDatasetGenerator()
            
            dataset_size = self.config.get('dataset_size', 50)
            print(f"ğŸ¯ ØªÙˆÙ„ÛŒØ¯ {dataset_size} Ø³ÙˆØ§Ù„...")
            
            # ØªÙˆØ²ÛŒØ¹ Ù…ØªØ¹Ø§Ø¯Ù„
            difficulty_distribution = {
                QuestionDifficulty.BASIC: 0.4,
                QuestionDifficulty.INTERMEDIATE: 0.4,
                QuestionDifficulty.ADVANCED: 0.2
            }
            
            category_distribution = {
                QuestionCategory.FACULTY_DUTIES: 0.4,
                QuestionCategory.KNOWLEDGE_BASED_COMPANIES: 0.3,
                QuestionCategory.TECHNOLOGY_TRANSFER: 0.2,
                QuestionCategory.RESEARCH_CONTRACTS: 0.1
            }
            
            questions = generator.generate_dataset(
                total_questions=dataset_size,
                difficulty_distribution=difficulty_distribution,
                category_distribution=category_distribution
            )
            
            # Ø°Ø®ÛŒØ±Ù‡ dataset
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = generator.save_dataset(
                questions, 
                f"phase4_dataset_{timestamp}.json"
            )
            
            # Ø¢Ù…Ø§Ø± dataset
            stats = generator.get_dataset_stats(questions)
            
            print(f"âœ… Dataset ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {len(questions)} Ø³ÙˆØ§Ù„")
            print(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø±: {filepath}")
            print(f"ğŸ“Š Ø¢Ù…Ø§Ø±:")
            for key, value in stats.items():
                if isinstance(value, dict):
                    print(f"   â€¢ {key}:")
                    for k, v in value.items():
                        print(f"     - {k}: {v}")
                else:
                    print(f"   â€¢ {key}: {value}")
            
            self.results['dataset_generation'] = {
                'status': 'success',
                'questions_count': len(questions),
                'filepath': str(filepath),
                'stats': stats
            }
            
            return questions
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Dataset: {e}")
            self.results['dataset_generation'] = {'status': 'failed', 'reason': str(e)}
            return None
    
    async def run_system_evaluation(self, questions: List = None) -> Optional[Dict]:
        """Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…"""
        
        print("\nğŸ”¬ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…...")
        print("=" * 50)
        
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            eval_config = EvaluationConfig(
                dataset_size=self.config.get('dataset_size', 50),
                test_subset_size=self.config.get('test_subset_size', 10),
                use_ollama=self.config.get('use_ollama', True),
                preferred_model=self.config.get('preferred_model', 'qwen_7b'),
                timeout_per_question=self.config.get('timeout_per_question', 30),
                save_detailed_results=True
            )
            
            print(f"âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
            print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª ØªØ³Øª: {eval_config.test_subset_size}")
            print(f"   â€¢ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ollama: {eval_config.use_ollama}")
            print(f"   â€¢ Ù…Ø¯Ù„ ØªØ±Ø¬ÛŒØ­ÛŒ: {eval_config.preferred_model}")
            print(f"   â€¢ Timeout: {eval_config.timeout_per_question}s")
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            evaluation_system = Phase4EvaluationSystem()
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
            print(f"\nğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ...")
            evaluation_result = await evaluation_system.run_full_evaluation(
                eval_config, questions
            )
            
            # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
            print(f"\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:")
            print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª: {evaluation_result.total_questions}")
            print(f"   â€¢ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒÙ‡Ø§: {evaluation_result.successful_evaluations}")
            print(f"   â€¢ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª: {evaluation_result.successful_evaluations/evaluation_result.total_questions*100:.1f}%")
            print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„ÛŒ: {evaluation_result.overall_score:.3f}")
            print(f"   â€¢ Ø²Ù…Ø§Ù† Ú©Ù„: {evaluation_result.evaluation_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
            
            if evaluation_result.detailed_metrics:
                print(f"\nğŸ“Š Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙØµÛŒÙ„ÛŒ:")
                for metric, value in evaluation_result.detailed_metrics.items():
                    print(f"   â€¢ {metric}: {value:.3f}")
            
            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙˆØ¶Ø¹ÛŒØª
            status = "Ø¹Ø§Ù„ÛŒ" if evaluation_result.overall_score > 0.8 else \
                     "Ø®ÙˆØ¨" if evaluation_result.overall_score > 0.6 else "Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯"
            print(f"\nğŸ¯ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…: {status}")
            
            self.results['system_evaluation'] = {
                'status': 'success',
                'overall_score': evaluation_result.overall_score,
                'success_rate': evaluation_result.successful_evaluations / evaluation_result.total_questions,
                'total_time': evaluation_result.evaluation_time,
                'detailed_metrics': evaluation_result.detailed_metrics,
                'system_status': status
            }
            
            return evaluation_result
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÛŒØ³ØªÙ…: {e}")
            self.results['system_evaluation'] = {'status': 'failed', 'reason': str(e)}
            return None
    
    def run_optimization(self) -> Optional[Dict]:
        """Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)"""
        
        if not self.config.get('run_optimization', False):
            print("\nâ­ï¸ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ø¯ Ø´Ø¯ (ØªÙ†Ø¸ÛŒÙ…Ø§Øª)")
            return None
        
        print("\nâš™ï¸ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ...")
        print("=" * 50)
        
        try:
            # ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Mock Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„
            def evaluation_function(parameters: Dict[str, Any]) -> Dict[str, float]:
                import random
                import time
                
                # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
                time.sleep(0.5)
                
                base_score = 0.7
                
                # ØªØ£Ø«ÛŒØ± Ø¨Ø±Ø®ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
                if parameters.get('chunk_size', 500) in [400, 500, 600]:
                    base_score += 0.1
                
                if parameters.get('temperature', 0.1) <= 0.2:
                    base_score += 0.1
                
                # Ù†ÙˆÛŒØ² ØªØµØ§Ø¯ÙÛŒ
                noise = random.uniform(-0.1, 0.1)
                
                return {
                    'retrieval_precision': min(1.0, base_score + noise),
                    'retrieval_recall': min(1.0, base_score + noise * 0.8),
                    'generation_rouge': min(1.0, base_score + noise * 1.2),
                    'legal_accuracy': min(1.0, base_score + noise * 0.9),
                    'citation_quality': min(1.0, base_score + noise * 1.1),
                    'response_time': random.uniform(3, 12)
                }
            
            # Ø§ÛŒØ¬Ø§Ø¯ optimizer
            optimizer = LegalSystemOptimizer(evaluation_function)
            
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            opt_config = OptimizationConfig(
                max_iterations=self.config.get('optimization_iterations', 20),
                early_stopping_patience=5,
                n_random_starts=3
            )
            
            print(f"ğŸ”§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:")
            print(f"   â€¢ Ø­Ø¯Ø§Ú©Ø«Ø± ØªÚ©Ø±Ø§Ø±: {opt_config.max_iterations}")
            print(f"   â€¢ Ø±ÙˆØ´: Random Search")
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            print(f"\nğŸš€ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ...")
            best_result = optimizer.random_search(opt_config)
            
            print(f"\nğŸ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡:")
            print(f"   â€¢ Ø§Ù…ØªÛŒØ§Ø²: {best_result.score:.4f}")
            print(f"   â€¢ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡:")
            for key, value in best_result.parameters.items():
                print(f"     - {key}: {value}")
            
            # Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
            summary = optimizer.get_optimization_summary()
            print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:")
            print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§: {summary['total_evaluations']}")
            print(f"   â€¢ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„ÛŒ: {summary['improvement']:.4f}")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
            optimizer.save_optimization_results()
            
            self.results['optimization'] = {
                'status': 'success',
                'best_score': best_result.score,
                'best_parameters': best_result.parameters,
                'total_evaluations': summary['total_evaluations'],
                'improvement': summary['improvement']
            }
            
            return best_result
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
            self.results['optimization'] = {'status': 'failed', 'reason': str(e)}
            return None
    
    def generate_final_report(self) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ ÙØ§Ø² 4"""
        
        print("\nğŸ“‹ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ...")
        print("=" * 50)
        
        total_duration = datetime.now() - self.start_time
        
        report = {
            "phase4_final_report": {
                "execution_date": self.start_time.isoformat(),
                "total_duration": str(total_duration),
                "config": self.config,
                "results": self.results
            },
            
            "summary": {
                "ollama_status": self.results.get('ollama_test', {}).get('status', 'unknown'),
                "dataset_generated": self.results.get('dataset_generation', {}).get('status', 'unknown') == 'success',
                "evaluation_completed": self.results.get('system_evaluation', {}).get('status', 'unknown') == 'success',
                "optimization_run": self.results.get('optimization', {}).get('status', 'unknown') == 'success',
                "overall_success": all(
                    result.get('status') == 'success' 
                    for key, result in self.results.items() 
                    if key != 'optimization'  # optimization Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ø³Øª
                )
            },
            
            "recommendations": self._generate_recommendations()
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = Path("reports") / f"phase4_final_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
        summary = report['summary']
        print(f"âœ… Ollama: {'ÙØ¹Ø§Ù„' if summary['ollama_status'] == 'success' else 'ØºÛŒØ±ÙØ¹Ø§Ù„'}")
        print(f"âœ… Dataset: {'ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯' if summary['dataset_generated'] else 'Ø®Ø·Ø§'}")
        print(f"âœ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ: {'Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯' if summary['evaluation_completed'] else 'Ø®Ø·Ø§'}")
        print(f"âœ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {'Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯' if summary['optimization_run'] else 'Ø±Ø¯ Ø´Ø¯'}")
        
        overall_status = "Ù…ÙˆÙÙ‚" if summary['overall_success'] else "Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¨Ø±Ø±Ø³ÛŒ"
        print(f"\nğŸ¯ ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ ÙØ§Ø² 4: {overall_status}")
        
        if report['recommendations']:
            print(f"\nğŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:")
            for rec in report['recommendations']:
                print(f"   â€¢ {rec}")
        
        print(f"\nğŸ“„ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„: {report_file}")
        
        return str(report_file)
    
    def _generate_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬"""
        
        recommendations = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ollama
        if self.results.get('ollama_test', {}).get('status') != 'success':
            recommendations.append("Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ollama Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
        eval_result = self.results.get('system_evaluation', {})
        if eval_result.get('status') == 'success':
            score = eval_result.get('overall_score', 0)
            if score < 0.6:
                recommendations.append("Ø¨Ù‡Ø¨ÙˆØ¯ prompt templates Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÛŒØ³ØªÙ…")
            elif score < 0.8:
                recommendations.append("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ hyperparameterÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        if not self.results.get('optimization'):
            recommendations.append("Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø³ÛŒØ³ØªÙ…")
        
        if not recommendations:
            recommendations.append("Ø³ÛŒØ³ØªÙ… Ø¢Ù…Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ ÙØ§Ø² 5 Ø§Ø³Øª")
        
        return recommendations
    
    async def run_full_phase4(self) -> bool:
        """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ§Ø² 4"""
        
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ§Ø² 4")
        print("=" * 60)
        print(f"â° Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"âš™ï¸ Ø­Ø§Ù„Øª Ø§Ø¬Ø±Ø§: {self.config.get('mode', 'full')}")
        
        success = True
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ³Øª Ollama
        if not await self.test_ollama_connection():
            print("âš ï¸ Ollama ÙØ¹Ø§Ù„ Ù†ÛŒØ³ØªØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Mock")
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: ØªÙˆÙ„ÛŒØ¯ Dataset
        questions = self.generate_test_dataset()
        if not questions:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Dataset")
            success = False
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÛŒØ³ØªÙ…
        evaluation_result = await self.run_system_evaluation(questions)
        if not evaluation_result:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³ÛŒØ³ØªÙ…")
            success = False
        
        # Ù…Ø±Ø­Ù„Ù‡ 4: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        self.run_optimization()
        
        # Ù…Ø±Ø­Ù„Ù‡ 5: Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        report_file = self.generate_final_report()
        
        total_duration = datetime.now() - self.start_time
        
        print(f"\nğŸ Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² 4")
        print("=" * 60)
        print(f"â±ï¸ Ù…Ø¯Øª Ú©Ù„: {total_duration}")
        print(f"ğŸ“Š ÙˆØ¶Ø¹ÛŒØª: {'âœ… Ù…ÙˆÙÙ‚' if success else 'âŒ Ø¨Ø§ Ø®Ø·Ø§'}")
        print(f"ğŸ“„ Ú¯Ø²Ø§Ø±Ø´: {report_file}")
        
        return success

def parse_arguments():
    """ØªØ¬Ø²ÛŒÙ‡ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†"""
    
    parser = argparse.ArgumentParser(description="Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² 4 Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯")
    
    parser.add_argument(
        '--mode',
        choices=['full', 'quick', 'test-only'],
        default='full',
        help='Ø­Ø§Ù„Øª Ø§Ø¬Ø±Ø§: full (Ú©Ø§Ù…Ù„), quick (Ø³Ø±ÛŒØ¹), test-only (ÙÙ‚Ø· ØªØ³Øª)'
    )
    
    parser.add_argument('--dataset-size', type=int, default=50, help='ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª dataset')
    parser.add_argument('--test-subset', type=int, default=10, help='ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª ØªØ³Øª')
    parser.add_argument('--model', default='qwen_7b', help='Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡')
    parser.add_argument('--timeout', type=int, default=30, help='timeout Ù‡Ø± Ø³ÙˆØ§Ù„ (Ø«Ø§Ù†ÛŒÙ‡)')
    parser.add_argument('--optimize', action='store_true', help='Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ')
    parser.add_argument('--log-level', default='INFO', help='Ø³Ø·Ø­ Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ')
    
    return parser.parse_args()

async def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    
    if not SYSTEM_AVAILABLE:
        print("âŒ Ø³ÛŒØ³ØªÙ… Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ù…Ø³ÛŒØ± src Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
        return False
    
    # ØªØ¬Ø²ÛŒÙ‡ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§
    args = parse_arguments()
    
    # ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
    setup_logging(args.log_level)
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø§Ù„Øª
    config = {
        'mode': args.mode,
        'dataset_size': 20 if args.mode == 'quick' else args.dataset_size,
        'test_subset_size': 5 if args.mode == 'quick' else args.test_subset,
        'preferred_model': args.model,
        'use_ollama': True,
        'timeout_per_question': args.timeout,
        'run_optimization': args.optimize,
        'optimization_iterations': 10 if args.mode == 'quick' else 20
    }
    
    if args.mode == 'test-only':
        config.update({
            'dataset_size': 10,
            'test_subset_size': 5,
            'run_optimization': False
        })
    
    # Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ø² 4
    runner = Phase4Runner(config)
    success = await runner.run_full_phase4()
    
    return success

if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸ Ø§Ø¬Ø±Ø§ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        sys.exit(1)