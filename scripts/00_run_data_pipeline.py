# scripts/00_run_data_pipeline.py
import sys
import os
import json

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data_processing.document_extractor import LegalDocumentExtractor
from src.data_processing.smart_chunker import LegalTextChunker
from src.retrieval.vector_database import LegalVectorDatabase

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
RAW_DOCS_PATH = "data/raw_documents"
PROCESSED_TEXT_PATH = "data/processed_chunks"
CHUNKS_OUTPUT_PATH = "data/chunks"
DB_PATH = "data/vector_db"
COLLECTION_NAME = "legal_hybrid_v1" # Ù†Ø§Ù… Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‡ÛŒØ¨Ø±ÛŒØ¯

def main():
    print("ğŸš€=============== START: HYBRID DATA PIPELINE ===============ğŸš€")

    # === Ø¨Ø®Ø´ Û±: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ===
    print("\n--- STAGE 1: Extracting and Cleaning Documents ---")
    extractor = LegalDocumentExtractor(input_dir=RAW_DOCS_PATH, output_dir=PROCESSED_TEXT_PATH)
    processed_docs_info = extractor.process_all_documents()

    if not processed_docs_info:
        print("âŒ Pipeline stopped. No documents processed.")
        return

    # === Ø¨Ø®Ø´ Û²: ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ ===
    print("\n--- STAGE 2: Performing Smart Chunking ---")
    chunker = LegalTextChunker(chunk_size=400, chunk_overlap=80, min_chunk_size=100)
    
    # Ø§ÛŒÙ† Ø¯Ø³ØªÙˆØ± ØªÙ…Ø§Ù… chunk Ù‡Ø§ Ø±Ø§ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù…Ø±Ø­Ù„Ù‡ Ù‚Ø¨Ù„ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    chunker.save_chunks_from_processed_docs(PROCESSED_TEXT_PATH, CHUNKS_OUTPUT_PATH)
    
    # === Ø¨Ø®Ø´ Û³: Ø³Ø§Ø®Øª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ ===
    print("\n--- STAGE 3: Building Vector Database ---")
    db = LegalVectorDatabase(db_path=DB_PATH, collection_name=COLLECTION_NAME)
    db.reset_database() # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù‚Ø¨Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… chunk Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    success = db.add_chunks_from_files(chunks_dir=CHUNKS_OUTPUT_PATH)

    if success:
        print("\nâœ… Vector Database built successfully!")
        stats = db.get_statistics()
        print(f"ğŸ“Š Final Stats: {stats.get('total_documents', 0)} chunks indexed.")
    else:
        print("\nâŒ Failed to build Vector Database.")
    
    print("\nğŸš€=============== END: HYBRID DATA PIPELINE ===============ğŸš€")

# ÛŒÚ© ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ù‡ Ú©Ù„Ø§Ø³ Ú†Ø§Ù†Ú©Ø± Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§Ø­Øªâ€ŒØªØ± Ø¨Ø§Ø´Ø¯
def save_chunks_from_processed_docs(self, processed_dir, output_dir):
    text_files = [f for f in os.listdir(processed_dir) if f.endswith('_cleaned.txt')]
    for test_file in text_files:
        document_name = test_file.replace('_cleaned.txt', '')
        with open(os.path.join(processed_dir, test_file), 'r', encoding='utf-8') as f:
            text = f.read()
        metadata_file = os.path.join("data/metadata", f"{document_name}_metadata.json")
        with open(metadata_file, 'r', encoding='utf-8') as f:
            doc_metadata = json.load(f)
        chunks = self.chunk_document(text, doc_metadata['metadata'])
        self.save_chunks(chunks, output_dir, document_name)

# Ø§ÛŒÙ† Ù…ØªØ¯ Ø±Ø§ Ø¨Ù‡ Ú©Ù„Ø§Ø³ LegalTextChunker Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
LegalTextChunker.save_chunks_from_processed_docs = save_chunks_from_processed_docs

if __name__ == "__main__":
    main()