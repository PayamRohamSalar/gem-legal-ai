# scripts/02_build_vector_store.py
import sys
import os
import json
from tqdm import tqdm

# Ø§ÙØ²ÙˆØ¯Ù† Ù…Ø³ÛŒØ± Ø±ÛŒØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.retrieval.vector_db_handler import VectorDBHandler

# ØªØ¹Ø±ÛŒÙ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
INPUT_JSONL_PATH = os.path.join(ROOT_DIR, 'data', 'processed_data.jsonl')
VECTOR_STORE_PATH = os.path.join(ROOT_DIR, 'data', 'vector_store')
COLLECTION_NAME = "legal_documents_v1" # ÛŒÚ© Ù†Ø§Ù… Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø®ÙˆØ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯

def main():
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ§Ø² Û²:
    1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ jsonl.
    2. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ.
    3. Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø§ÙØ²ÙˆØ¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù†.
    """
    print("ğŸš€ Starting Phase 2: Building Vector Store...")

    # 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡
    print(f"Reading processed data from {INPUT_JSONL_PATH}...")
    try:
        with open(INPUT_JSONL_PATH, 'r', encoding='utf-8') as f:
            all_docs = [json.loads(line) for line in f]
    except FileNotFoundError:
        print(f"Error: Input file not found at {INPUT_JSONL_PATH}")
        print("Please make sure you have successfully completed Phase 1.")
        return

    # 2. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ChromaDB
    # Ù…Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ù…ØªÙˆÙ†ØŒ Ù…ØªØ§Ø¯ÛŒØªØ§ Ùˆ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…
    all_chunks_texts = []
    all_chunks_metadatas = []
    all_chunks_ids = []

    for doc in all_docs:
        for chunk in doc['chunks']:
            all_chunks_texts.append(chunk['text'])
            all_chunks_ids.append(chunk['chunk_id'])
            # Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ù‡Ø± chunk Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            all_chunks_metadatas.append({
                "document_id": doc['document_id'],
                "document_title": doc['title'],
                "article_number": chunk.get('number', 'N/A'),
                "chunk_type": chunk.get('type', 'N/A')
            })
            
    print(f"Prepared a total of {len(all_chunks_ids)} chunks for indexing.")

    # 3. Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ù¾Ø± Ú©Ø±Ø¯Ù† Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
    db_handler = VectorDBHandler(db_path=VECTOR_STORE_PATH)
    collection = db_handler.get_or_create_collection(name=COLLECTION_NAME)

    # Ø§ÙØ²ÙˆØ¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ (batch) Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±
    batch_size = 100
    print(f"Adding chunks to vector store in batches of {batch_size}...")
    for i in tqdm(range(0, len(all_chunks_ids), batch_size)):
        batch_ids = all_chunks_ids[i:i+batch_size]
        batch_texts = all_chunks_texts[i:i+batch_size]
        batch_metadatas = all_chunks_metadatas[i:i+batch_size]
        
        db_handler.add_batch(
            ids=batch_ids,
            documents=batch_texts,
            metadatas=batch_metadatas
        )

    print("\nVector store has been built and populated successfully! âœ…")
    print(f"Total items in collection: {collection.count()}")

if __name__ == "__main__":
    main()