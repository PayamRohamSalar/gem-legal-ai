"""
src/generation/llm_manager.py - Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ LLM (Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ollama)

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ù…Ø³Ø¦ÙˆÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒØŒ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù† Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª.
ÙˆÛŒÚ˜Ú¯ÛŒ Ø¬Ø¯ÛŒØ¯: Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
"""

import torch
import psutil
import time
import gc
import logging
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# Import Ollama
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸  Ollama Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ø§Ø² Ù…Ø¯Ù„ Mock Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒâ€ŒØ´Ø¯Ù‡"""
    QWEN_7B = "qwen2.5:7b"
    LLAMA_8B = "llama3.1:8b"
    MISTRAL_7B = "mistral:7b"
    MOCK = "mock_model"

@dataclass
class GenerationMetrics:
    """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®"""
    generation_time: float
    input_tokens: int
    output_tokens: int
    total_tokens: int
    memory_usage_mb: float
    model_name: str
    temperature: float
    success: bool = True
    error_message: Optional[str] = None

@dataclass
class LLMConfig:
    """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ LLM"""
    model_name: str
    model_type: ModelType
    device: str = "auto"
    max_new_tokens: int = 2048
    temperature: float = 0.1
    top_p: float = 0.9
    top_k: int = 40
    repeat_penalty: float = 1.1
    context_length: int = 8192

# Ø¯Ø± ÙØ§ÛŒÙ„ src/generation/llm_manager.py

class OllamaManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama (Ù†Ø³Ø®Ù‡ Ù‚ÙˆÛŒâ€ŒØªØ±)"""

    def __init__(self, host: str = 'http://localhost:11434'):
        self.host = host
        self.client = None
        self.available_models = []
        self.connection_status = False

        if OLLAMA_AVAILABLE:
            self.client = ollama.Client(host=self.host)
            self._check_connection()
        else:
            logger.warning("Ollama package Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")

    def _check_connection(self) -> bool:
        if not self.client:
            return False

        try:
            logger.info(f"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø³Ø±ÙˆØ± Ollama Ø¯Ø± {self.host} ...")
            response = self.client.list()
            self.available_models = [model.get('name') for model in response.get('models', []) if model.get('name')]
            self.connection_status = True
            logger.info(f"Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯. Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {self.available_models}")
            if not self.available_models:
                logger.warning("Ollama Ù…ØªØµÙ„ Ø§Ø³Øª Ø§Ù…Ø§ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø±Ø§ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø±ÙˆØ± Ollama Ø±Ø§ Ø±ÛŒâ€ŒØ§Ø³ØªØ§Ø±Øª Ú©Ù†ÛŒØ¯.")
            return True
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama: {e}. Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ø±ÙˆØ´Ù† Ø¨ÙˆØ¯Ù† Ø³Ø±ÙˆØ± Ollama Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯.")
            self.connection_status = False
            return False

    
    def generate(self, model_name: str, prompt: str, **kwargs) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ollama"""
        if not self.connection_status:
            raise Exception("Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†ÛŒØ³Øª")
        
        try:
            start_time = time.time()
            
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            options = {
                'temperature': kwargs.get('temperature', 0.1),
                'top_p': kwargs.get('top_p', 0.9),
                'top_k': kwargs.get('top_k', 40),
                'repeat_penalty': kwargs.get('repeat_penalty', 1.1),
                'num_predict': kwargs.get('max_new_tokens', 2048)
            }
            
            # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
            response = ollama.generate(
                model=model_name,
                prompt=prompt,
                options=options
            )
            
            generation_time = time.time() - start_time
            
            return {
                'success': True,
                'response': response['response'],
                'generation_time': generation_time,
                'model': model_name,
                'total_duration': response.get('total_duration', 0),
                'load_duration': response.get('load_duration', 0),
                'prompt_eval_count': response.get('prompt_eval_count', 0),
                'eval_count': response.get('eval_count', 0)
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Ollama: {e}")
            return {
                'success': False,
                'error': str(e),
                'response': '',
                'generation_time': time.time() - start_time
            }

class LLMManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ù†Ø¯Ù‡ Ø§ØµÙ„ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ LLM"""
    
    def __init__(self, configs: Dict[str, LLMConfig]):
        self.configs = configs
        self.models: Dict[str, Any] = {}
        self.tokenizers: Dict[str, Any] = {}
        self.active_model: Optional[str] = None
        self.generation_history: List[GenerationMetrics] = []
        
        # Ù…Ø¯ÛŒØ±ÛŒØª Ollama
        self.ollama_manager = OllamaManager()
        
        logger.info("LLMManager Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
    
    def _get_memory_usage(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ²Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / (1024 * 1024)
    
    def _estimate_tokens(self, text: str) -> int:
        """ØªØ®Ù…ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ (ØªÙ‚Ø±ÛŒØ¨ÛŒ)"""
        # ØªØ®Ù…ÛŒÙ† Ø³Ø§Ø¯Ù‡: ÙØ§Ø±Ø³ÛŒ Ø­Ø¯ÙˆØ¯ 4 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± ØªÙˆÚ©Ù†
        return len(text) // 4
    
    def list_available_models(self) -> Dict[str, List[str]]:
        """Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        return {
            'ollama_models': self.ollama_manager.available_models,
            'loaded_models': list(self.models.keys()),
            'configured_models': list(self.configs.keys()),
            'ollama_status': self.ollama_manager.connection_status
        }
    
    def load_model_ollama(self, model_key: str) -> bool:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ø² Ollama"""
        if model_key not in self.configs:
            logger.error(f"ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ {model_key} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return False
        
        config = self.configs[model_key]
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ollama
        if not self.ollama_manager.connection_status:
            logger.error("Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†ÛŒØ³Øª")
            return False
        
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„
            if config.model_name not in self.ollama_manager.available_models:
                logger.info(f"Ù…Ø¯Ù„ {config.model_name} Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯...")
                if not self.ollama_manager.pull_model(config.model_name):
                    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ {config.model_name}")
                    return False
            
            # ØªØ³Øª Ù…Ø¯Ù„
            test_result = self.ollama_manager.generate(
                config.model_name, 
                "Ø³Ù„Ø§Ù…",
                temperature=0.1,
                max_new_tokens=10
            )
            
            if test_result['success']:
                self.models[model_key] = {'type': 'ollama', 'name': config.model_name}
                self.active_model = model_key
                logger.info(f"Ù…Ø¯Ù„ Ollama {model_key} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
                return True
            else:
                logger.error(f"ØªØ³Øª Ù…Ø¯Ù„ {config.model_name} Ù†Ø§Ù…ÙˆÙÙ‚: {test_result.get('error')}")
                return False
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ollama {model_key}: {e}")
            return False
    
    def load_model_mock(self, model_key: str) -> bool:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ mock Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ³Øª (fallback)"""
        if model_key not in self.configs:
            logger.error(f"ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ {model_key} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return False
        
        try:
            logger.warning(f"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ mock Ù…Ø¯Ù„: {model_key} (Ollama Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª)")
            
            # Mock objects Ø¨Ø±Ø§ÛŒ ØªØ³Øª
            mock_model = type('MockModel', (), {
                'generate': lambda *args, **kwargs: type('MockOutput', (), {
                    'sequences': [torch.tensor([[1, 2, 3, 4, 5]])],
                    'scores': None
                })(),
                'device': 'cpu'
            })()
            
            mock_tokenizer = type('MockTokenizer', (), {
                'encode': lambda self, text: list(range(min(len(text)//4, 100))),
                'decode': lambda self, tokens, **kwargs: "Ø§ÛŒÙ† ÛŒÚ© Ù¾Ø§Ø³Ø® ØªØ³Øª Mock Ø§Ø³Øª. Ø¯Ø± Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒØŒ Ollama Ù¾Ø§Ø³Ø® ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø¯.",
                'eos_token': '</s>',
                'eos_token_id': 2,
                'pad_token': '</s>',
                '__call__': lambda self, text, **kwargs: {
                    'input_ids': torch.tensor([[1, 2, 3]]),
                    'attention_mask': torch.tensor([[1, 1, 1]])
                }
            })()
            
            self.models[model_key] = {'type': 'mock', 'model': mock_model}
            self.tokenizers[model_key] = mock_tokenizer
            self.active_model = model_key
            
            logger.info(f"Mock Ù…Ø¯Ù„ {model_key} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            return True
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ mock Ù…Ø¯Ù„ {model_key}: {e}")
            return False
    
    def load_model(self, model_key: str, prefer_ollama: bool = True) -> bool:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ (Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ollama)"""
        if prefer_ollama and OLLAMA_AVAILABLE:
            success = self.load_model_ollama(model_key)
            if success:
                return True
            logger.warning("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ollama Ù†Ø§Ù…ÙˆÙÙ‚. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Mock...")
        
        return self.load_model_mock(model_key)
    
    def generate_response(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ø¯Ù„ ÙØ¹Ø§Ù„"""
        if not self.active_model:
            return {
                'success': False,
                'error': 'Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ ÙØ¹Ø§Ù„ Ù†ÛŒØ³Øª',
                'response': '',
                'metrics': None
            }
        
        start_time = time.time()
        start_memory = self._get_memory_usage()
        
        try:
            config = self.configs[self.active_model]
            model_info = self.models[self.active_model]
            
            # ØªØ®Ù…ÛŒÙ† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
            input_tokens = self._estimate_tokens(prompt)
            
            # ØªÙˆÙ„ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù…Ø¯Ù„
            if model_info['type'] == 'ollama':
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ollama
                result = self.ollama_manager.generate(
                    model_info['name'],
                    prompt,
                    temperature=kwargs.get('temperature', config.temperature),
                    top_p=kwargs.get('top_p', config.top_p),
                    top_k=kwargs.get('top_k', config.top_k),
                    max_new_tokens=kwargs.get('max_new_tokens', config.max_new_tokens),
                    repeat_penalty=kwargs.get('repeat_penalty', config.repeat_penalty)
                )
                
                if result['success']:
                    response = result['response']
                    output_tokens = self._estimate_tokens(response)
                    
                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù…Ø§Ø± Ø¯Ù‚ÛŒÙ‚ Ollama Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
                    if 'eval_count' in result:
                        output_tokens = result['eval_count']
                    if 'prompt_eval_count' in result:
                        input_tokens = result['prompt_eval_count']
                else:
                    raise Exception(result['error'])
                    
            else:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Mock
                tokenizer = self.tokenizers[self.active_model]
                time.sleep(0.3)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´
                response = tokenizer.decode([], skip_special_tokens=True)
                output_tokens = self._estimate_tokens(response)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
            generation_time = time.time() - start_time
            memory_usage = self._get_memory_usage() - start_memory
            
            metrics = GenerationMetrics(
                generation_time=generation_time,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                total_tokens=input_tokens + output_tokens,
                memory_usage_mb=memory_usage,
                model_name=self.active_model,
                temperature=kwargs.get('temperature', config.temperature),
                success=True
            )
            
            self.generation_history.append(metrics)
            
            return {
                'success': True,
                'response': response,
                'metrics': metrics,
                'model_used': self.active_model,
                'model_type': model_info['type']
            }
            
        except Exception as e:
            generation_time = time.time() - start_time
            error_metrics = GenerationMetrics(
                generation_time=generation_time,
                input_tokens=input_tokens if 'input_tokens' in locals() else 0,
                output_tokens=0,
                total_tokens=input_tokens if 'input_tokens' in locals() else 0,
                memory_usage_mb=self._get_memory_usage() - start_memory,
                model_name=self.active_model,
                temperature=kwargs.get('temperature', 0.1),
                success=False,
                error_message=str(e)
            )
            
            self.generation_history.append(error_metrics)
            
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {e}")
            return {
                'success': False,
                'error': str(e),
                'response': '',
                'metrics': error_metrics
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        info = {
            'loaded_models': list(self.models.keys()),
            'active_model': self.active_model,
            'total_generations': len(self.generation_history),
            'memory_usage_mb': self._get_memory_usage(),
            'ollama_status': self.ollama_manager.connection_status,
            'ollama_models': self.ollama_manager.available_models
        }
        
        # Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯
        if self.generation_history:
            successful_gens = [m for m in self.generation_history if m.success]
            if successful_gens:
                info['performance'] = {
                    'success_rate': len(successful_gens) / len(self.generation_history),
                    'avg_generation_time': sum(m.generation_time for m in successful_gens) / len(successful_gens),
                    'avg_output_tokens': sum(m.output_tokens for m in successful_gens) / len(successful_gens)
                }
        
        return info
    
    def get_generation_stats(self) -> Dict[str, Any]:
        """Ø¢Ù…Ø§Ø± ØªÙØµÛŒÙ„ÛŒ ØªÙˆÙ„ÛŒØ¯Ø§Øª"""
        if not self.generation_history:
            return {'message': 'Ù‡ÛŒÚ† ØªÙˆÙ„ÛŒØ¯ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯Ù‡'}
        
        successful = [m for m in self.generation_history if m.success]
        failed = [m for m in self.generation_history if not m.success]
        
        stats = {
            'total_generations': len(self.generation_history),
            'successful': len(successful),
            'failed': len(failed),
            'success_rate': len(successful) / len(self.generation_history) * 100,
            'timestamp': datetime.now().isoformat()
        }
        
        if successful:
            stats['performance'] = {
                'avg_time': sum(m.generation_time for m in successful) / len(successful),
                'min_time': min(m.generation_time for m in successful),
                'max_time': max(m.generation_time for m in successful),
                'avg_input_tokens': sum(m.input_tokens for m in successful) / len(successful),
                'avg_output_tokens': sum(m.output_tokens for m in successful) / len(successful),
                'total_tokens_processed': sum(m.total_tokens for m in successful)
            }
        
        return stats
    
    def cleanup(self) -> None:
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù…Ù†Ø§Ø¨Ø¹"""
        for model_key in list(self.models.keys()):
            del self.models[model_key]
        
        if hasattr(self, 'tokenizers'):
            for tokenizer_key in list(self.tokenizers.keys()):
                del self.tokenizers[tokenizer_key]
            self.tokenizers.clear()
        
        self.models.clear()
        self.active_model = None
        self.generation_history.clear()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        logger.info("Ù…Ù†Ø§Ø¨Ø¹ LLM Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù†Ø¯")

def create_model_configs() -> Dict[str, LLMConfig]:
    """Ø§ÛŒØ¬Ø§Ø¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    return {
        'qwen_7b': LLMConfig(
            model_name="qwen2.5:7b",
            model_type=ModelType.QWEN_7B,
            temperature=0.1,
            max_new_tokens=2048,
            top_p=0.9,
            top_k=40
        ),
        'llama_8b': LLMConfig(
            model_name="llama3.1:8b",
            model_type=ModelType.LLAMA_8B,
            temperature=0.1,
            max_new_tokens=2048,
            top_p=0.9,
            top_k=40
        ),
        'mistral_7b': LLMConfig(
            model_name="mistral:7b",
            model_type=ModelType.MISTRAL_7B,
            temperature=0.1,
            max_new_tokens=2048,
            top_p=0.9,
            top_k=40
        )
    }

# ØªØ³Øª Ø¬Ø§Ù…Ø¹
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸ§ª ØªØ³Øª Ø¬Ø§Ù…Ø¹ LLMManager Ø¨Ø§ Ollama")
    print("=" * 60)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª
    configs = create_model_configs()
    manager = LLMManager(configs)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    available = manager.list_available_models()
    print(f"ğŸ“‹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:")
    print(f"   â€¢ Ollama: {available['ollama_models']}")
    print(f"   â€¢ ÙˆØ¶Ø¹ÛŒØª Ollama: {available['ollama_status']}")
    
    # ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
    print(f"\nğŸ”„ ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„...")
    model_to_test = 'qwen_7b'
    
    success = manager.load_model(model_to_test, prefer_ollama=True)
    print(f"âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {'Ù…ÙˆÙÙ‚' if success else 'Ù†Ø§Ù…ÙˆÙÙ‚'}")
    
    if success:
        # ØªØ³Øª ØªÙˆÙ„ÛŒØ¯
        print(f"\nğŸ¤– ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...")
        test_prompts = [
            "Ø³Ù„Ø§Ù…ØŒ Ú†Ø·ÙˆØ±ÛŒØŸ",
            "ÙˆØ¸Ø§ÛŒÙ Ø§Ø¹Ø¶Ø§ÛŒ Ù‡ÛŒØ¦Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù¾Ú˜ÙˆÙ‡Ø´ Ú†ÛŒØ³ØªØŸ",
            "ØªØ¹Ø±ÛŒÙ Ø´Ø±Ú©Øª Ø¯Ø§Ù†Ø´â€ŒØ¨Ù†ÛŒØ§Ù† Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯."
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ“ ØªØ³Øª {i}: {prompt}")
            
            result = manager.generate_response(prompt, temperature=0.1)
            
            if result['success']:
                response = result['response']
                metrics = result['metrics']
                
                print(f"   âœ… Ù…ÙˆÙÙ‚ - Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®: {len(response)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                print(f"   â±ï¸  Ø²Ù…Ø§Ù†: {metrics.generation_time:.2f}s")
                print(f"   ğŸ”¢ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§: {metrics.input_tokens}â†’{metrics.output_tokens}")
                print(f"   ğŸ’§ Ù†ÙˆØ¹ Ù…Ø¯Ù„: {result['model_type']}")
                print(f"   ğŸ“„ Ù¾Ø§Ø³Ø®: {response[:100]}...")
            else:
                print(f"   âŒ Ù†Ø§Ù…ÙˆÙÙ‚: {result['error']}")
    
    # Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
    print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:")
    stats = manager.get_generation_stats()
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"   â€¢ {key}:")
            for k, v in value.items():
                print(f"     - {k}: {v}")
        else:
            print(f"   â€¢ {key}: {value}")
    
    # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ
    manager.cleanup()
    print("\nâœ… ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯")