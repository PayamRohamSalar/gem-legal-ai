# scripts/03_test_rag_pipeline.py
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¬Ø¯ÛŒØ¯
from src.retrieval.vector_database import LegalVectorDatabase
from src.retrieval.embedding_manager import EmbeddingModelManager
from src.retrieval.hybrid_search import HybridSearchEngine
from src.generation.llm_manager import LLMManager, create_model_configs
from src.generation.prompt_engine import PromptEngine, ContextInfo
from src.generation.citation_engine import CitationEngine

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
DB_PATH = "data/vector_db"
COLLECTION_NAME = "legal_hybrid_v1"
ACTIVE_LLM = "qwen"  # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ 'mistral' ÛŒØ§ 'llama' ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯

def main():
    print("ğŸš€=============== START: PROFESSIONAL RAG PIPELINE ===============ğŸš€")
    
    # === ÙØ§Ø² Û±: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ØªÙ…Ø§Ù… Ù…ÙˆØªÙˆØ±Ù‡Ø§ ===
    print("\n--- STAGE 1: Initializing All Engines ---")
    vector_db = LegalVectorDatabase(db_path=DB_PATH, collection_name=COLLECTION_NAME)
    embedding_manager = EmbeddingModelManager()
    embedding_manager.load_model(embedding_manager.get_recommended_model())
    
    search_engine = HybridSearchEngine(vector_db, embedding_manager)
    prompt_engine = PromptEngine()
    citation_engine = CitationEngine()
    
    llm_configs = create_model_configs()
    llm_manager = LLMManager(llm_configs)
    llm_manager.set_active_model(ACTIVE_LLM)
    print("âœ… All engines are ready.")

    # === ÙØ§Ø² Û²: Ø·Ø±Ø­ Ø³ÙˆØ§Ù„ Ùˆ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÛŒØ¨Ø±ÛŒØ¯ ===
    user_question = "Ø§Ù‡Ø¯Ø§Ù Ø³Ù†Ø¯ Ú¯Ø³ØªØ±Ø´ Ú©Ø§Ø±Ø¨Ø±Ø¯ ÙÙ†Ø§ÙˆØ±ÛŒ Ù†Ø§Ù†Ùˆ Ø¯Ø± Ø§ÙÙ‚ Ù¡Ù¤Ù Ù¤ Ú†ÛŒØ³ØªØŸ"
    print(f"\n--- STAGE 2: Hybrid Search for question: '{user_question}' ---")
    contexts_raw = search_engine.hybrid_search(user_question, top_k=7)
    if not contexts_raw:
        print("âŒ No relevant context found. Aborting.")
        return

    # ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø®Ø§Ù… Ø¨Ù‡ ContextInfo Ø¨Ø±Ø§ÛŒ Ù…ÙˆØªÙˆØ± Ù¾Ø±Ø§Ù…Ù¾Øª
    contexts_info = [prompt_engine.ContextInfo(
        content=res['text'],
        source=res['metadata'].get('document_title', 'Ø³Ù†Ø¯ Ù†Ø§Ù…Ø´Ø®Øµ'),
        document_type=prompt_engine.ContextType.LAW, # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ ÙØ¹Ù„Ø§Ù‹ Ù‡Ù…Ù‡ Ø±Ø§ Ù‚Ø§Ù†ÙˆÙ† Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
        relevance_score=res.get('final_score', 0)
    ) for res in contexts_raw]
        
    print(f"âœ… Retrieved {len(contexts_info)} relevant contexts.")

    # === ÙØ§Ø² Û³: Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ ===
    print("\n--- STAGE 3: Building Smart Prompt ---")
    final_prompt, query_type = prompt_engine.build_prompt(user_question, contexts_info)
    print(f"âœ… Prompt built successfully for query type: {query_type.value}")
    
    # === ÙØ§Ø² Û´: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ LLM ===
    print(f"\n--- STAGE 4: Generating Response with '{ACTIVE_LLM}' ---")
    llm_result = llm_manager.generate_response(final_prompt)
    if not llm_result['success']:
        print(f"âŒ LLM Generation failed: {llm_result['error']}")
        return
    raw_response = llm_result['response']
    print("âœ… Raw response generated by LLM.")
    
    # === ÙØ§Ø² Ûµ: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª (Citation) ===
    print("\n--- STAGE 5: Processing Citations ---")
    enhanced_result = citation_engine.enhance_response_with_citations(raw_response, contexts_raw)
    
    # === ÙØ§Ø² Û¶: Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ ===
    print("\n\nâœ…âœ…âœ… FINAL ENHANCED RESPONSE âœ…âœ…âœ…")
    print("=" * 50)
    print("âœï¸ **Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø³ØªÛŒØ§Ø± Ø­Ù‚ÙˆÙ‚ÛŒ:**\n")
    print(enhanced_result['enhanced_response'])
    print("\n" + "=" * 50)
    print(enhanced_result['references_list'])
    print("=" * 50)
    validation = enhanced_result['validation']
    print(f"ğŸ”¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ø±Ø¬Ø§Ø¹Ø§Øª: {validation['valid_citations']}/{validation['total_citations']} Ù…Ø¹ØªØ¨Ø± ({validation['accuracy_percentage']:.1f}%)")
    
    print("\nğŸš€=============== END: PROFESSIONAL RAG PIPELINE ===============ğŸš€")

if __name__ == "__main__":
    main()