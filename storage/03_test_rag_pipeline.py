# scripts/03_test_rag_pipeline.py
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ایمپورت کردن تمام ماژول‌های حرفه‌ای جدید
from src.retrieval.vector_database import LegalVectorDatabase
from src.retrieval.embedding_manager import EmbeddingModelManager
from src.retrieval.hybrid_search import HybridSearchEngine
from src.generation.llm_manager import LLMManager, create_model_configs
from src.generation.prompt_engine import PromptEngine, ContextInfo
from src.generation.citation_engine import CitationEngine

# --- تنظیمات ---
DB_PATH = "data/vector_db"
COLLECTION_NAME = "legal_hybrid_v1"
ACTIVE_LLM = "qwen"  # می‌توانید به 'mistral' یا 'llama' تغییر دهید

def main():
    print("🚀=============== START: PROFESSIONAL RAG PIPELINE ===============🚀")
    
    # === فاز ۱: راه‌اندازی تمام موتورها ===
    print("\n--- STAGE 1: Initializing All Engines ---")
    vector_db = LegalVectorDatabase(db_path=DB_PATH, collection_name=COLLECTION_NAME)
    embedding_manager = EmbeddingModelManager()
    embedding_manager.load_model(embedding_manager.get_recommended_model())
    
    search_engine = HybridSearchEngine(vector_db, embedding_manager)
    prompt_engine = PromptEngine()
    citation_engine = CitationEngine()
    
    llm_configs = create_model_configs()
    llm_manager = LLMManager(llm_configs)
    llm_manager.set_active_model(ACTIVE_LLM)
    print("✅ All engines are ready.")

    # === فاز ۲: طرح سوال و جستجوی هیبرید ===
    user_question = "اهداف سند گسترش کاربرد فناوری نانو در افق ١٤٠٤ چیست؟"
    print(f"\n--- STAGE 2: Hybrid Search for question: '{user_question}' ---")
    contexts_raw = search_engine.hybrid_search(user_question, top_k=7)
    if not contexts_raw:
        print("❌ No relevant context found. Aborting.")
        return

    # تبدیل نتایج خام به ContextInfo برای موتور پرامپت
    contexts_info = [prompt_engine.ContextInfo(
        content=res['text'],
        source=res['metadata'].get('document_title', 'سند نامشخص'),
        document_type=prompt_engine.ContextType.LAW, # برای سادگی، فعلاً همه را قانون در نظر می‌گیریم
        relevance_score=res.get('final_score', 0)
    ) for res in contexts_raw]
        
    print(f"✅ Retrieved {len(contexts_info)} relevant contexts.")

    # === فاز ۳: ساخت پرامپت هوشمند ===
    print("\n--- STAGE 3: Building Smart Prompt ---")
    final_prompt, query_type = prompt_engine.build_prompt(user_question, contexts_info)
    print(f"✅ Prompt built successfully for query type: {query_type.value}")
    
    # === فاز ۴: تولید پاسخ با LLM ===
    print(f"\n--- STAGE 4: Generating Response with '{ACTIVE_LLM}' ---")
    llm_result = llm_manager.generate_response(final_prompt)
    if not llm_result['success']:
        print(f"❌ LLM Generation failed: {llm_result['error']}")
        return
    raw_response = llm_result['response']
    print("✅ Raw response generated by LLM.")
    
    # === فاز ۵: استخراج و اعتبارسنجی ارجاعات (Citation) ===
    print("\n--- STAGE 5: Processing Citations ---")
    enhanced_result = citation_engine.enhance_response_with_citations(raw_response, contexts_raw)
    
    # === فاز ۶: نمایش نتیجه نهایی ===
    print("\n\n✅✅✅ FINAL ENHANCED RESPONSE ✅✅✅")
    print("=" * 50)
    print("✍️ **پاسخ نهایی دستیار حقوقی:**\n")
    print(enhanced_result['enhanced_response'])
    print("\n" + "=" * 50)
    print(enhanced_result['references_list'])
    print("=" * 50)
    validation = enhanced_result['validation']
    print(f"🔬 اعتبارسنجی ارجاعات: {validation['valid_citations']}/{validation['total_citations']} معتبر ({validation['accuracy_percentage']:.1f}%)")
    
    print("\n🚀=============== END: PROFESSIONAL RAG PIPELINE ===============🚀")

if __name__ == "__main__":
    main()